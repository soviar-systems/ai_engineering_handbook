{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5435388e-fa58-45f8-adc5-ddf878cc22a8",
   "metadata": {},
   "source": [
    "---\n",
    "abbreviations:\n",
    "  GEMM: GEneral Matrix-Matrix multiplication\n",
    "  GEMV: General Matrix-Vector multiply, a special case of GEMM\n",
    "  WMMA: Warp-level matrix operations\n",
    "---\n",
    "# PHASE 1: FOUNDATIONAL NEURONS & BACKPROP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f011f9a0-81e2-408a-b861-44801afbb0c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T18:22:45.000597Z",
     "iopub.status.busy": "2025-12-23T18:22:45.000402Z",
     "iopub.status.idle": "2025-12-23T18:22:45.004033Z",
     "shell.execute_reply": "2025-12-23T18:22:45.003688Z",
     "shell.execute_reply.started": "2025-12-23T18:22:45.000579Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.14.0 free-threading build (main, Oct 28 2025, 12:10:48) [Clang 20.1.4 ]'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7789b6d8-03a5-4986-8043-bb04e0170a04",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Environment preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b567dece-e85d-40b0-9569-19f325633139",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-12-23T18:21:59.021698Z",
     "iopub.status.busy": "2025-12-23T18:21:59.021527Z",
     "iopub.status.idle": "2025-12-23T18:22:08.833566Z",
     "shell.execute_reply": "2025-12-23T18:22:08.833261Z",
     "shell.execute_reply.started": "2025-12-23T18:21:59.021688Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPython 3.14.0+freethreaded interpreter at: \u001b[36m/home/commi/.local/bin/python3.14t\u001b[39m\n",
      "Creating virtual environment at: \u001b[36m/home/commi/venv/slm_from_scratch\u001b[39m\n",
      "Activate with: \u001b[32msource /home/commi/venv/slm_from_scratch/bin/activate\u001b[39m\n",
      "\u001b[2mUsing Python 3.14.0 environment at: /home/commi/venv/slm_from_scratch\u001b[0m\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m29 packages\u001b[0m \u001b[2min 728ms\u001b[0m\u001b[0m                                        \u001b[0m\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m3 packages\u001b[0m \u001b[2min 2.60s\u001b[0m\u001b[0m                                             \n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m29 packages\u001b[0m \u001b[2min 55ms\u001b[0m\u001b[0m                               \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1masttokens\u001b[0m\u001b[2m==3.0.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcomm\u001b[0m\u001b[2m==0.2.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdebugpy\u001b[0m\u001b[2m==1.8.19\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdecorator\u001b[0m\u001b[2m==5.2.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mexecuting\u001b[0m\u001b[2m==2.2.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mipykernel\u001b[0m\u001b[2m==7.1.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mipython\u001b[0m\u001b[2m==9.8.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mipython-pygments-lexers\u001b[0m\u001b[2m==1.1.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mjedi\u001b[0m\u001b[2m==0.19.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mjupyter-client\u001b[0m\u001b[2m==8.7.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mjupyter-core\u001b[0m\u001b[2m==5.9.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmatplotlib-inline\u001b[0m\u001b[2m==0.2.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnest-asyncio\u001b[0m\u001b[2m==1.6.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpackaging\u001b[0m\u001b[2m==25.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mparso\u001b[0m\u001b[2m==0.8.5\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpexpect\u001b[0m\u001b[2m==4.9.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mplatformdirs\u001b[0m\u001b[2m==4.5.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mprompt-toolkit\u001b[0m\u001b[2m==3.0.52\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpsutil\u001b[0m\u001b[2m==7.1.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mptyprocess\u001b[0m\u001b[2m==0.7.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpure-eval\u001b[0m\u001b[2m==0.2.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpygments\u001b[0m\u001b[2m==2.19.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpython-dateutil\u001b[0m\u001b[2m==2.9.0.post0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpyzmq\u001b[0m\u001b[2m==27.1.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msix\u001b[0m\u001b[2m==1.17.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mstack-data\u001b[0m\u001b[2m==0.6.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtornado\u001b[0m\u001b[2m==6.5.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtraitlets\u001b[0m\u001b[2m==5.14.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mwcwidth\u001b[0m\u001b[2m==0.2.14\u001b[0m\n",
      "Installed kernelspec slm_from_scratch in /home/commi/.local/share/jupyter/kernels/slm_from_scratch\n",
      "\u001b[2mUsing Python 3.14.0 environment at: /home/commi/venv/slm_from_scratch\u001b[0m\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m15 packages\u001b[0m \u001b[2min 540ms\u001b[0m\u001b[0m                                        \u001b[0m\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m2 packages\u001b[0m \u001b[2min 3.86s\u001b[0m\u001b[0m                                             \n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m12 packages\u001b[0m \u001b[2min 25ms\u001b[0m\u001b[0m                               \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcontourpy\u001b[0m\u001b[2m==1.3.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcycler\u001b[0m\u001b[2m==0.12.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfonttools\u001b[0m\u001b[2m==4.61.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mkiwisolver\u001b[0m\u001b[2m==1.4.9\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmatplotlib\u001b[0m\u001b[2m==3.10.8\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==2.4.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpandas\u001b[0m\u001b[2m==2.3.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpillow\u001b[0m\u001b[2m==12.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpyparsing\u001b[0m\u001b[2m==3.3.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpytz\u001b[0m\u001b[2m==2025.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mseaborn\u001b[0m\u001b[2m==0.13.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtzdata\u001b[0m\u001b[2m==2025.3\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# bash\n",
    "venv_name=\"slm_from_scratch\"\n",
    "venv_path=\"${HOME}/venv/${venv_name}\"\n",
    "\n",
    "create_jupyter_venv -p 3.14t -n \"${venv_name}\"\n",
    "\n",
    "uv pip install -p \"${venv_path}\" \\\n",
    "    matplotlib \\\n",
    "    numpy \\\n",
    "    seaborn\n",
    "\n",
    "# remove_jupyter_venv \"${venv_name}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d493d59-d203-4cfd-b934-7b37a75cc2f8",
   "metadata": {},
   "source": [
    "**The Core Idea**: A neural network is just a mathematical function that can be represented as a computational graph. The \"learning\" happens by adjusting the parameters of this function to minimize some error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03721216-616b-4ec5-b957-7e8b23d1595c",
   "metadata": {},
   "source": [
    "# **1. Forward Pass**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435f338d-af06-4b6b-9a28-7761c69d1875",
   "metadata": {},
   "source": [
    "We begin with the absolute building block of all deep learning: **the single neuron**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8cffa5-0d16-44fa-b39d-64f2d5cda9ea",
   "metadata": {},
   "source": [
    "# 1.1 The Mathematical Neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fe8759-4c80-44e4-b3cc-c45b59b47a2a",
   "metadata": {},
   "source": [
    "A neuron computes:\n",
    "\n",
    "```{math}\n",
    "z = \\mathbf{w}^\\top \\mathbf{x} + b\n",
    "```\n",
    "\n",
    "```{math}\n",
    ":label: eq:activation_general\n",
    "a = \\sigma(z)\n",
    "```\n",
    "\n",
    "Where:\n",
    "- $\\mathbf{x} \\in \\mathbb{R}^d$ is the input vector,\n",
    "- $\\mathbf{w} \\in \\mathbb{R}^d$ is the weight vector,\n",
    "- $b \\in \\mathbb{R}$ is the bias,\n",
    "- $\\sigma$ is a nonlinear activation function (e.g., ReLU, tanh),\n",
    "- $a$ is the output (activation).\n",
    "\n",
    "This is **not** a biological metaphor‚Äîit is a differentiable function that enables composition and gradient flow.\n",
    "\n",
    "\n",
    "```{mermaid}\n",
    "graph LR\n",
    "    X --> mul\n",
    "    W.T --> mul\n",
    "    b --> add\n",
    "    mul --> add\n",
    "    add --> z\n",
    "```\n",
    "\n",
    "Think about processing one input vector vs many inputs:\n",
    "\n",
    "**Single input**: $[x_1, x_2, x_3]$ with weights $[\\theta_1, \\theta_2, \\theta_3]$  \n",
    "$\\text {output} = x_1\\theta_1 + x_2\\theta_2 + x_3\\theta_3 + b$\n",
    "\n",
    "**Multiple inputs (as matrix)**: \n",
    "```\n",
    "Inputs: [ [x‚ÇÅ‚ÇÅ, x‚ÇÅ‚ÇÇ, x‚ÇÅ‚ÇÉ],   Weights: [ùúÉ‚ÇÅ, ùúÉ‚ÇÇ, ùúÉ‚ÇÉ]·µÄ\n",
    "          [x‚ÇÇ‚ÇÅ, x‚ÇÇ‚ÇÇ, x‚ÇÇ‚ÇÉ],\n",
    "          [x‚ÇÉ‚ÇÅ, x‚ÇÉ‚ÇÇ, x‚ÇÉ‚ÇÉ] ]\n",
    "```\n",
    "\n",
    "What linear algebra operation would efficiently compute all outputs at once? Matrix multiplication."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f4fc10-1cc1-4b09-88d3-3b2ee5da1a08",
   "metadata": {},
   "source": [
    "# 1.2 What is GEMM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe0fa18-a11e-45c5-a629-25b674cfc21d",
   "metadata": {},
   "source": [
    "GEMM stands for **GE**neral **M**atrix-**M**atrix multiplication. It is a foundational linear algebra operation defined as:\n",
    "\n",
    "```{math}\n",
    "C = \\alpha \\cdot A \\cdot B + \\beta \\cdot C\n",
    "```\n",
    "\n",
    "where:\n",
    "- {math}`A \\in \\mathbb{R}^{m \\times k}`, {math}`B \\in \\mathbb{R}^{k \\times n}`, {math}`C \\in \\mathbb{R}^{m \\times n}`\n",
    "- {math}`\\alpha, \\beta` are scalar coefficients (often {math}`\\alpha = 1, \\beta = 0` in deep learning)\n",
    "- The operation computes all pairwise dot products between rows of {math}`A` and columns of {math}`B`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11aaf55c-3d04-460d-aabf-81e27d7899eb",
   "metadata": {},
   "source": [
    "In linear algebra and deep learning, a **dot product** is an operation between two vectors of the same dimension, resulting in a single scalar:\n",
    "\n",
    "$$a \\cdot b = \\sum_{i=1}^{n} a_i b_i$$\n",
    "\n",
    "When we speak of **matrix multiplication** (2$C = AB$), we are indeed performing \"pairwise dot products\" between the **rows** of $A$ and the **columns** of $B$. However, it is critical to distinguish this from an **element-wise (Hadamard) product**, which is also a \"pairwise\" operation but occurs between corresponding elements of two matrices of the identical shape ($A \\odot B$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbecbc46-f3c5-42e3-ad9e-fe12a3ed1cc1",
   "metadata": {},
   "source": [
    "## Pairwise Dot Products"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "99688e9e-5c55-40d5-bf31-ae645ac8a452",
   "metadata": {},
   "source": [
    "Essentially, **pairwise dot products** is a way of describing the \"all-vs-all\" calculation that happens during matrix multiplication.\n",
    "\n",
    "While a single dot product tells you the relationship between two specific vectors, pairwise dot products tell you the relationship between **every possible pair of vectors** from two different sets.\n",
    "\n",
    "In the context of the GEMM formula {math}`C = A \\cdot B`, think of matrix $A$ as a stack of horizontal rows and matrix $B$ as a collection of vertical columns.\n",
    "\n",
    "1. **The Inputs:** Matrix $A$ has $m$ rows, and Matrix $B$ has $n$ columns.\n",
    "2. **The Matching:** To find a specific value in the resulting matrix $C$, you take one row from $A$ and \"dot\" it with one column from $B$.\n",
    "3. **The \"Pairwise\" Result:** Because you repeat this for every row combined with every column, you end up with $m \\times n$ individual dot products.\n",
    "\n",
    "If Matrix $A$ contains vectors $\\{a_1, a_2\\}$ and Matrix $B$ contains vectors $\\{b_1, b_2\\}$, the **pairwise dot products** are:\n",
    "\n",
    "```{math}\n",
    "C = \n",
    "\\begin{pmatrix}\n",
    "a_1 \\cdot b_1 & a_1 \\cdot b_2 \\\\\n",
    "a_2 \\cdot b_1 & a_2 \\cdot b_2\n",
    "\\end{pmatrix}\n",
    "```\n",
    "\n",
    "You‚Äôll often hear this term when discussing **Attention Mechanisms** (like in Transformers). \n",
    "\n",
    "> When a model calculates \"Self-Attention,\" it is performing pairwise dot products between every word (vector) in a sentence and every other word in that same sentence to see how much they relate to one another.\n",
    "\n",
    "**Summary Table**\n",
    "\n",
    "| Concept | Operation | Definition | Result |\n",
    "| --- | --- | --- | --- |\n",
    "| **Dot Product** | $a \\cdot b = \\sum_{i=1}^{n} a_i b_i$| Operation between 2 vectors. | A single number (scalar). |\n",
    "| **Pairwise Dot Products** | $AB$ | Operation between 2 **sets** of vectors. | A matrix of numbers. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3412ad42-05e8-4407-a6fb-b005fc76de6e",
   "metadata": {},
   "source": [
    "## Terminology Mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc6e6ad-b1ce-418f-aeaf-4b4e519072ce",
   "metadata": {},
   "source": [
    "These terms are related but operate at different levels of abstraction:\n",
    "\n",
    "| Term | Domain | Definition | Complexity |\n",
    "| --- | --- | --- | --- |\n",
    "| **Dot Product** | Linear Algebra | The sum of products of corresponding entries of two sequences of numbers. | $O(N)$ |\n",
    "| **GEMV** | Level 2 BLAS/Hardware | Optimized Matrix-Vector multiplication. | $O(N^2)$ |\n",
    "| **GEMM** | Level 3 BLAS/Hardware | **GE**neral **M**atrix **M**ultiply. A specific routine ($$C = \\alpha AB + \\beta C$$) optimized for hardware (CUDA/CPUs). Level 3 operations are preferred because they have much higher arithmetic intensity, meaning they do more work per byte of data moved. | $O(N^3)$ |\n",
    "| **Matrix-Vector Mult** | Linear Algebra | A special case of GEMM where one matrix has a dimension of 1. |  |\n",
    "| **Pairwise Dot Product** | Descriptive | A conceptual way to describe how the rows/columns interact during GEMM. |  |\n",
    "| **Matrix Multiplication** | NumPy `np.dot` / `np.matmul` | Software implementations that call underlying GEMM libraries (like OpenBLAS or MKL). They behave differently for 3D+ tensors: `np.matmul` (and the `@` operator) is designed specifically for \"batch\" matrix multiplication, whereas `np.dot` performs a more traditional sum-product over the last axes. |  |\n",
    "| **Element-Wise (Hadamard) product** | NumPy/Math `np.multiply()` | a multiplication operation between corresponding elements of two matrices of the identical shape ($A \\odot B = A_{ij} \\times B_{ij}$) | $O(N^2)$ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08910064-a899-4fc0-8492-7bb23de9abad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T20:46:46.164474Z",
     "iopub.status.busy": "2025-12-23T20:46:46.163776Z",
     "iopub.status.idle": "2025-12-23T20:46:46.166506Z",
     "shell.execute_reply": "2025-12-23T20:46:46.166161Z",
     "shell.execute_reply.started": "2025-12-23T20:46:46.164453Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "A = np.array([[1, 2], \n",
    "              [3, 4]])\n",
    "\n",
    "B = np.array([[5, 6], \n",
    "              [7, 8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4485accf-8571-4b82-aae7-7e27d7c11dbb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T20:47:33.546777Z",
     "iopub.status.busy": "2025-12-23T20:47:33.546533Z",
     "iopub.status.idle": "2025-12-23T20:47:33.549941Z",
     "shell.execute_reply": "2025-12-23T20:47:33.549626Z",
     "shell.execute_reply.started": "2025-12-23T20:47:33.546755Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5, 12],\n",
       "       [21, 32]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hadamard Product (Element-wise)\n",
    "# Result: [[1*5, 2*6], [3*7, 4*8]]\n",
    "A * B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e326a42b-6b4f-471c-a548-30335feafebb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T20:47:39.609366Z",
     "iopub.status.busy": "2025-12-23T20:47:39.609175Z",
     "iopub.status.idle": "2025-12-23T20:47:39.611681Z",
     "shell.execute_reply": "2025-12-23T20:47:39.611363Z",
     "shell.execute_reply.started": "2025-12-23T20:47:39.609347Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[19, 22],\n",
       "       [43, 50]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Matrix Multiplication (Dot Product)\n",
    "# Result: [[(1*5 + 2*7), (1*6 + 2*8)], [(3*5 + 4*7), (3*6 + 4*8)]]\n",
    "A @ B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ff3f69-b092-475e-b638-a013c8296a67",
   "metadata": {},
   "source": [
    "## Why GEMM Matters in Deep Learning & CUDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd25ccca-51bd-41f0-833b-f4540fa23e05",
   "metadata": {},
   "source": [
    "1. **Core Computational Unit**:  \n",
    "   Every dense layer in a neural network is a GEMM:\n",
    "   - Forward pass: {math}`\\text{output} = XW^T` ‚Üí GEMM with {math}`A = X`, {math}`B = W^T`\n",
    "   - Backward pass: {math}`\\frac{\\partial L}{\\partial W} = \\frac{\\partial L}{\\partial Z}^T X` ‚Üí another GEMM\n",
    "\n",
    "2. **Hardware Optimization Target**:  \n",
    "   - Modern GPUs (like your RTX 4090 Ti) dedicate >80% of their die area to **tensor cores** and **GEMM-optimized CUDA cores**\n",
    "   - Libraries like cuBLAS and CUTLASS provide highly tuned GEMM kernels that exploit:\n",
    "     - Memory coalescing\n",
    "     - Shared memory tiling\n",
    "     - Warp-level matrix operations (WMMA)\n",
    "\n",
    "3. **Performance Bottleneck**:  \n",
    "   A 100M-parameter LLM spends ~60‚Äì80% of its training time in GEMM calls. Inefficient GEMM = wasted VRAM bandwidth and compute.\n",
    "\n",
    "**Concrete Example in Your Context**\n",
    "\n",
    "In your scalar neuron implementation:\n",
    "- You computed {math}`z = w \\cdot x + b` ‚Äî a *vector-scalar* operation\n",
    "- When you vectorize to a **batched neuron** (next step), you'll compute:\n",
    "  ```{math}\n",
    "  Z = XW^T + b\n",
    "  ```\n",
    "  where {math}`X \\in \\mathbb{R}^{B \\times d}`, {math}`W \\in \\mathbb{R}^{1 \\times d}`, {math}`Z \\in \\mathbb{R}^{B \\times 1}`\n",
    "- This is a **GEMM** (specifically, a **GEMV**‚ÄîGeneral Matrix-Vector multiply, a special case of GEMM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf94b351-7ab0-4f0e-8b24-67451e41745d",
   "metadata": {},
   "source": [
    "## Key Insight for AI Architects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce42f5bf-f845-4dff-8625-0d211aae687c",
   "metadata": {},
   "source": [
    "Understanding GEMM lets you:\n",
    "- Predict VRAM bandwidth requirements (bytes moved = {math}`m \\cdot k + k \\cdot n + m \\cdot n`)\n",
    "- Diagnose compute-bound vs memory-bound kernels\n",
    "- Justify architectural choices (e.g., why grouped-query attention reduces GEMM overhead)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b6fb12-9459-4696-bfae-46a294165250",
   "metadata": {},
   "source": [
    "# 1.3 Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4046533d-d153-4695-934a-72e2b666a2e3",
   "metadata": {},
   "source": [
    "Now, consider this:  \n",
    "**Why do we require $\\sigma$ in {eq}`eq:activation_general` to be nonlinear? What happens to the representational capacity of a multi-layer network if $\\sigma$ is linear?**\n",
    "\n",
    "Correct. If all activations are linear, the composition of layers collapses to a single linear transformation:  \n",
    "```{math}\n",
    "f(\\mathbf{x}) = \\mathbf{W}_L \\cdots \\mathbf{W}_2 \\mathbf{W}_1 \\mathbf{x} + \\mathbf{b}_{\\text{total}} = \\mathbf{W}_{\\text{eff}} \\mathbf{x} + \\mathbf{b}_{\\text{eff}}\n",
    "```\n",
    "\n",
    "This cannot model nonlinear decision boundaries‚Äîhence the necessity of nonlinear $\\sigma$.\n",
    "\n",
    "```{hint} What is ‚Äúeff‚Äù in $\\mathbf{W}_{\\text{eff}}$?*\n",
    ":class: dropdown\n",
    ":open: true\n",
    "\n",
    "‚Äúeff‚Äù stands for **effective**. It denotes that the product of multiple weight matrices collapses to a **single equivalent linear transformation** when all activations are linear. So $\\mathbf{W}_{\\text{eff}} = \\mathbf{W}_L \\cdots \\mathbf{W}_1$ is the *effective weight matrix* of the entire network.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54981e3-cb62-420d-897c-8ab44be212ab",
   "metadata": {},
   "source": [
    "## Activation Function Options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110e94be-f145-4040-be85-8371c11c5567",
   "metadata": {},
   "source": [
    "| Function | Formula | Range | Key Properties |\n",
    "|----------|---------|-------|----------------|\n",
    "| **Sigmoid** | 1/(1+e‚ÅªÀ£) | (0,1) | Smooth, bounded, but can saturate (vanishing gradients) |\n",
    "| **Tanh** | (eÀ£-e‚ÅªÀ£)/(eÀ£+e‚ÅªÀ£) | (-1,1) | Zero-centered, but still can saturate |\n",
    "| **ReLU** | max(0,x) | [0,‚àû) | Simple, avoids saturation, but \"dying ReLU\" problem |\n",
    "| **Leaky ReLU** | max(0.01x,x) | (-‚àû,‚àû) | Fixes dying ReLU, small gradient for negatives |\n",
    "\n",
    "**Historical Context & Modern Practice**\n",
    "\n",
    "- **1980s-2000s**: Sigmoid/tanh were dominant (biological plausibility)\n",
    "- **2010s**: ReLU became standard for hidden layers (training speed)\n",
    "- **Today**: Variants like Leaky ReLU, GELU are common"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460195b9-6b51-4e96-afbf-0a21dcd1159d",
   "metadata": {},
   "source": [
    "### tanh()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1e5c1c-3b99-4762-aa05-8f39afdcd6fc",
   "metadata": {},
   "source": [
    "The hyperbolic tangent function is defined as:\n",
    "\n",
    "```{math}\n",
    "\\tanh(z) = \\frac{e^{z} - e^{-z}}{e^{z} + e^{-z}}\n",
    "```\n",
    "\n",
    "This is the **complete closed-form formula**. It maps $z \\in \\mathbb{R}$ to $a \\in (-1, 1)$.\n",
    "\n",
    "For computation by hand, you can evaluate it numerically using known values or a calculator.\n",
    "\n",
    "This is how the function looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "770dea50-07e9-464e-8d2b-db4929282100",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-22T18:54:22.338201Z",
     "iopub.status.busy": "2025-12-22T18:54:22.337977Z",
     "iopub.status.idle": "2025-12-22T18:54:23.285434Z",
     "shell.execute_reply": "2025-12-22T18:54:23.285110Z",
     "shell.execute_reply.started": "2025-12-22T18:54:22.338184Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_tanh():\n",
    "    # Generate x values from -10 to 10\n",
    "    x = np.linspace(-10, 10, 400)\n",
    "    \n",
    "    # Compute tanh for each x value\n",
    "    y = np.tanh(x)\n",
    "    \n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(x, y, label='tanh(x)')\n",
    "    \n",
    "    # Add title and labels\n",
    "    plt.title('Hyperbolic Tangent Function')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('tanh(x)')\n",
    "    \n",
    "    # Add a legend\n",
    "    plt.legend()\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.grid(True)\n",
    "    plt.axhline(0, color='black',linewidth=0.5)\n",
    "    plt.axvline(0, color='black',linewidth=0.5)\n",
    "    plt.show()\n",
    "    \n",
    "# Call the function to display the plot\n",
    "plot_tanh()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9723ae0e-6c13-4b50-b7a5-5ac0b22abdf3",
   "metadata": {},
   "source": [
    "# 1.4 What the Single Neuron really is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0ab855-e0aa-48d6-bec6-35ec701486d1",
   "metadata": {},
   "source": [
    "You already have **prior experience** (CNNs, NumPy backprop), so we are not teaching you deep learning from absolute zero. Instead, we are **recalibrating your foundation at the level required for an AI Architect**, where every operation must be understood in three layers simultaneously:\n",
    "\n",
    "1. **Mathematical identity** (e.g., chain rule, matrix derivatives),\n",
    "2. **Computational implementation** (e.g., NumPy/PyTorch code),\n",
    "3. **Hardware implication** (e.g., how this maps to GEMM in VRAM).\n",
    "\n",
    "Starting with a **single neuron**‚Äînot a full matmul‚Äîis intentional. Why?\n",
    "\n",
    "Because **matmul is just a batched collection of dot products**, and a dot product is just a sum of scaled inputs. If you cannot derive the gradient of **one scalar output** with respect to **one weight**, you will misapply vectorized gradients later‚Äîeven if your code ‚Äúruns.‚Äù\n",
    "\n",
    "This is a **mastery gate**: prove you can do the atomic unit correctly, and we immediately scale to matmul.\n",
    "\n",
    "A linear layer computes:\n",
    "```{math}\n",
    "\\mathbf{Z} = \\mathbf{X} \\mathbf{W}^\\top + \\mathbf{b}\n",
    "```\n",
    "Each row of $\\mathbf{Z}$ is \n",
    "\n",
    "```{math}\n",
    "\\mathbf{z}^{(i)} = \\mathbf{x}^{(i)} \\mathbf{W}^\\top + \\mathbf{b}\n",
    "```\n",
    "\n",
    "and each element \n",
    "\n",
    "```{math}\n",
    "z^{(i)}_j = \\sum_{k=1}^K x^{(i)}_k w_{j,k} + b_j\n",
    "```\n",
    "\n",
    "or in Einstein Summation form\n",
    "\n",
    "```{math}\n",
    "z^{(i)}_j = x^{(i)}_k w_{j,k} + b_j\n",
    "```\n",
    "\n",
    "- **exactly a single neuron**\n",
    "\n",
    "\n",
    "\n",
    "Thus, the **gradient of a full layer** is just the **aggregate of single-neuron gradients** across batch and output dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbb5742-2123-40f3-8899-5f2ed01e6ea6",
   "metadata": {},
   "source": [
    "# **2. Backward Pass ‚Äî Scalar Case**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ccd707-0d82-4172-9b33-a95206d3512e",
   "metadata": {},
   "source": [
    "# 2.1 No micrograd in the course"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3c56e3-a217-4507-943d-ce21f855dfc5",
   "metadata": {},
   "source": [
    "Regarding your question about the `Value` object:\n",
    "\n",
    "You are likely referring to micrograd-style implementations (e.g., Andrej Karpathy‚Äôs *micrograd*), where a `Value` class tracks:\n",
    "- A scalar data value,\n",
    "- Its computational graph (parents),\n",
    "- And implements `backward()` for scalar autodiff.\n",
    "\n",
    "We **will not use `Value`**.\n",
    "\n",
    "**Reason**: Our goal is **not** to build a toy autodiff engine, but to understand **how backpropagation maps to efficient, vectorized operations on real hardware** (NumPy ‚Üí PyTorch ‚Üí CUDA). The `Value` abstraction obscures memory layout, batched computation, and the link to matrix calculus‚Äîprecisely what an AI Architect must master.\n",
    "\n",
    "Instead, we will:\n",
    "1. Derive gradients **analytically** using matrix calculus,\n",
    "2. Implement them **explicitly in NumPy** (no autograd),\n",
    "3. Then transition to **PyTorch with manual gradient checks**,\n",
    "4. Finally, analyze how these map to **CUDA kernels** (e.g., GEMM for linear layers).\n",
    "\n",
    "This path ensures you understand what PyTorch‚Äôs `.backward()` *actually does* under the hood‚Äîsomething `Value` hides."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84ecee7-b928-42c5-9f2b-aa1919adf7e6",
   "metadata": {},
   "source": [
    "```{hint}\n",
    ":class: dropdown\n",
    ":open: true\n",
    "\n",
    "**What is *micrograd*?**\n",
    "\n",
    "*micrograd* is an **educational autodifferentiation engine** written in pure Python. It implements:\n",
    "- A scalar `Value` node that stores data and pointers to children in a compute graph,\n",
    "- A recursive `.backward()` that traverses the graph and accumulates gradients via the chain rule.\n",
    "\n",
    "It is **not** a production system. It exists solely to **visually and mechanically demonstrate** how reverse-mode autodiff works at the scalar level.\n",
    "\n",
    "**Is *micrograd* used in real-world training systems?**\n",
    "\n",
    "**No.** Production deep learning frameworks (PyTorch, TensorFlow, JAX) use **vectorized, batched, GPU-accelerated autodiff** based on:\n",
    "- **Operator-level differentiation**: Each primitive (e.g., `matmul`, `softmax`) has a pre-defined backward kernel.\n",
    "- **Static or dynamic computational graphs** (with fusion, memory planning, and CUDA kernels).\n",
    "- **Memory-efficient gradient accumulation** (in-place ops, gradient checkpointing).\n",
    "\n",
    "These systems **never build per-scalar graph nodes**‚Äîdoing so would be catastrophically slow and memory-inefficient. For a 100M-parameter model, *micrograd*-style graphs would require >100 million interconnected Python objects‚Äîimpossible to train at scale.\n",
    "\n",
    "**Where might you encounter *micrograd*-like ideas in practice?**\n",
    "\n",
    "Only in **two narrow contexts**:\n",
    "1. **Research prototyping of novel differentiable operators** (e.g., custom physics simulators), where symbolic or manual gradient derivation is needed before vectorization.\n",
    "2. **Debugging gradient flow** in small subgraphs by manually computing derivatives‚Äî*not* by running *micrograd*, but by replicating its logic on paper or in NumPy.\n",
    "\n",
    "Even then, you **do not deploy** such code. You derive the math, then implement a fused, vectorized CUDA kernel or PyTorch custom autograd function.\n",
    "\n",
    "**Why we avoid *micrograd* in this course**\n",
    "\n",
    "You are preparing for an **AI Architect role**, where your job is to:\n",
    "- Design models that fit in 16 GB VRAM,\n",
    "- Understand how `torch.nn.Linear` maps to cuBLAS GEMM calls,\n",
    "- Optimize memory bandwidth during backprop.\n",
    "\n",
    "*micrograd* teaches none of this. It teaches graph traversal in Python‚Äîa skill irrelevant to high-performance LLM systems.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0b9532-80e9-45d1-9c0d-8aacc33534d2",
   "metadata": {},
   "source": [
    "# 2.2 Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc9da78-8b50-4650-951d-6456692ede7b",
   "metadata": {},
   "source": [
    "## Idea"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e061a3-fd5f-4809-a48f-eb6dc401a157",
   "metadata": {},
   "source": [
    "**Backpropagation** is the algorithm for efficiently computing the gradients through the entire computational graph.\n",
    "\n",
    "Think of it this way: If your neuron's computation is:\n",
    "```\n",
    "input ‚Üí linear ‚Üí activation ‚Üí output\n",
    "```\n",
    "\n",
    "Backpropagation answers:\n",
    "\n",
    "> \"*How much did each weight contribute to the final error?*\"\n",
    "\n",
    "The entire forward pass is this:\n",
    "```{math}\n",
    "x \\xrightarrow{\\theta, b} z = \\theta x + b \\xrightarrow{\\text{tanh}} a = \\tanh(z) \\xrightarrow{\\mathcal{L}} L = \\mathcal{L}(a, y_{\\text{true}})\n",
    "```\n",
    "\n",
    "```{tip} ‚ÄúLoss‚Äù vs. ‚ÄúCost‚Äù\n",
    ":class: dropdown\n",
    ":open: true\n",
    "\n",
    "- **Loss function** $\\ell(a, y)$: defined **per sample** (e.g., $\\frac{1}{2}(a-y)^2$).\n",
    "- **Cost function** $J(\\theta)$: the **aggregate** over a batch or dataset (e.g., mean of $\\ell$ over $B$ samples).\n",
    "\n",
    "In modern ML literature (including PyTorch, TensorFlow, and most LLM papers), the term **‚Äúloss‚Äù** is used **even for the batch-aggregated quantity**. For example, `loss.backward()` in PyTorch operates on the scalar batch loss.\n",
    "\n",
    "So while **‚Äúcost‚Äù = aggregate, ‚Äúloss‚Äù = per-sample** is a valid distinction (common in Andrew Ng‚Äôs early courses), **contemporary usage favors ‚Äúloss‚Äù for both**, with context implying scope.\n",
    "\n",
    "In this course, we will use **‚Äúloss‚Äù** for the scalar batch objective, consistent with PyTorch and LLM training codebases.\n",
    "```\n",
    "\n",
    "Let's break this down into manageable steps. If you were to implement gradient calculation for your single neuron, what would be the minimal information you'd need?\n",
    "\n",
    "Consider:\n",
    "- The error at the output\n",
    "- How the activation function affects the gradient\n",
    "- How the linear combination affects the gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e902a1-53ab-4822-a6aa-cc206b8183b6",
   "metadata": {},
   "source": [
    "## The Error Signal Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4721cb3-44db-4449-b979-8b6ce1a55eaf",
   "metadata": {},
   "source": [
    "The error signal begins with the difference between predicted and true values.\n",
    "\n",
    "You're correct: `error = true_y - predicted_y` (or some function of this difference)\n",
    "\n",
    "Now here's the key insight for backpropagation: **We need to trace how this error flows backward through each operation.**\n",
    "\n",
    "Think about your neuron's forward pass:\n",
    "```\n",
    "x ‚Üí linear (w¬∑x + b) ‚Üí tanh ‚Üí y_pred\n",
    "```\n",
    "\n",
    "For backpropagation, we go backward:\n",
    "```\n",
    "error ‚Üê d_tanh ‚Üê d_linear ‚Üê gradients for w and b\n",
    "```\n",
    "\n",
    "If you were to implement this gradient calculation manually for a single input, what operations would you need to compute?\n",
    "\n",
    "Consider:\n",
    "1. Gradient of the error with respect to the output\n",
    "2. How the activation function transforms this gradient\n",
    "3. How the linear operation distributes gradients to weights and bias\n",
    "\n",
    "**What's your thinking about the mathematical operations involved?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59ad4e5-f3b8-409a-a8b3-cfc345c910d6",
   "metadata": {},
   "source": [
    "## Chain Rule Intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c710c54-350c-4c6c-b49b-238ec9022015",
   "metadata": {},
   "source": [
    "Think about your neuron's computation:\n",
    "```\n",
    "output = leaky_relu(w¬∑x + b)\n",
    "error = some_cost_function()\n",
    "```\n",
    "\n",
    "If we want to know how much to change $\\omega_1$, we need to answer: \"How does changing $\\omega_1$ affect the final error?\"\n",
    "\n",
    "This is where the **chain rule** (–ø—Ä–æ–∏–∑–≤–æ–¥–Ω–∞—è —Å–ª–æ–∂–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏) from calculus comes in. We break the problem into steps:\n",
    "\n",
    "1. How does error change with output?\n",
    "2. How does output change with activation input?\n",
    "3. How does activation input change with $\\omega_1$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fe70e8-d562-4e51-baaa-1bfa24f51ae4",
   "metadata": {},
   "source": [
    "We use the **chain rule** to compute gradients through the **computational graph**.\n",
    "\n",
    "Think about your neuron:\n",
    "```\n",
    "x ‚Üí z = ùúÉ¬∑x + b ‚Üí a = tanh(z) ‚Üí J = loss_function(a, y_true)\n",
    "```\n",
    "\n",
    "where `a` is `y_pred`.\n",
    "\n",
    "To find $\\displaystyle \\frac {\\partial}{\\partial \\theta}J(\\theta)$, we can compute:\n",
    "\n",
    "$$\n",
    "\\frac {\\partial}{\\partial \\theta}J(\\theta) = \\frac {\\partial J(\\theta)}{\\partial a} \\times \\frac {\\partial a}{\\partial z} \\times \\frac {\\partial z}{\\partial \\theta}\n",
    "$$\n",
    "\n",
    "**Your implementation challenge**: If you were to compute these partial derivatives numerically for a single example, what would be your step-by-step approach?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b42c7b-4700-4f55-8660-5c387301512e",
   "metadata": {},
   "source": [
    "# 2.3 Why Tanh?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e602e41-d2ba-41a4-b9a2-54ef1ff991af",
   "metadata": {},
   "source": [
    "Now we can explain why we use tanh function in this step, not ReLU. \n",
    "\n",
    "To understand the trade-offs, we must look at the forward pass and the derivative (gradient) for each function.\n",
    "\n",
    "| Activation | Forward ($f(z)$) | Derivative ($f'(z)$) |\n",
    "| :--- | :--- | :--- |\n",
    "| **tanh(z)** | $\\tanh(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}$ | $1 - \\tanh^2(z)$ |\n",
    "| **Leaky ReLU(z)** | $\\begin{cases} z & \\text{if } z \\geq 0 \\\\ \\alpha z & \\text{if } z < 0 \\end{cases}$ | $\\begin{cases} 1 & z \\geq 0 \\\\ \\alpha & z < 0 \\end{cases}$ |\n",
    "\n",
    "We are currently in **Phase 1: Foundational Neurons & Backprop**. The priority is mathematical clarity and gradient validation, not building a production-ready LLM yet.\n",
    "\n",
    "* **Smoothness & Differentiability:** Tanh is \"smooth\" everywhere. Leaky ReLU has a \"kink\" at $z=0$ where the derivative is discontinuous. In scalar manual backprop, these kinks can cause numerical instability and confusing results during gradient checks.\n",
    "* **Bounded Output:** Tanh keeps outputs in $(-1; 1)$. This makes gradient magnitudes predictable and prevents values from \"exploding\" while you are still debugging your weight initializations.\n",
    "* **Historical Validation:** Most foundational backprop literature uses tanh. Using it here allows you to replicate classic experiments and ensure your chain rule implementation is 100% correct.\n",
    "\n",
    "**Why not Leaky ReLU yet?**\n",
    "\n",
    "Leaky ReLU's main advantage‚Äîavoiding \"dead neurons\"‚Äîis only truly relevant in **deep neural networks**. In a single scalar neuron, it adds an extra hyperparameter ($\\alpha$) with almost no benefit. Furthermore, modern Transformers (like GPT) have largely moved past Leaky ReLU in favor of **GELU**, which we will implement in Phase 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56693db5-4353-4662-9347-f2c79426df7c",
   "metadata": {},
   "source": [
    "## \"Vanishing\" vs. \"Dead\" Gradients"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e545a33a-0d0e-438e-8ada-321dc10392fa",
   "metadata": {},
   "source": [
    "It is important to distinguish between these two phenomena:\n",
    "\n",
    "1.  **Vanishing Gradients (The Tanh Problem):** This happens when $|z|$ is very large. The function becomes very flat, so the gradient becomes tiny (e.g., $0.00001$). Training slows down, but the neuron is still \"alive.\"\n",
    "2.  **Dead Gradients (The ReLU Problem):** In standard ReLU, if $z < 0$, the gradient is **exactly zero**. The neuron stops learning entirely because no signal passes back through it.\n",
    "\n",
    "**Leaky ReLU solves \"Dead Gradients\"**: By using $\\alpha = 0.01$, it ensures the gradient is never zero, even for negative inputs.\n",
    "\n",
    "**The Impact on Your Implementation**:\n",
    "\n",
    "We need non-zero, smooth gradients to validate your manual backprop code. If you used standard **ReLU**, any test input where $z \\leq 0$ would result in a gradient of exactly $0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5013a6c8-57d1-4ce2-88ef-784f4820fd3e",
   "metadata": {},
   "source": [
    "## A Finite-Difference Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4839b953-029e-4596-b8a8-56ee47ea95ef",
   "metadata": {},
   "source": [
    "It‚Äôs a **numerical method to approximate the derivative** of a loss function with respect to a parameter‚Äîusing only function evaluations, **no calculus required**.\n",
    "\n",
    "**Formula (forward difference):**\n",
    "\n",
    "```{math}\n",
    "\\frac{\\partial L}{\\partial \\theta} \\approx \\frac{L(\\theta + \\epsilon) - L(\\theta)}{\\epsilon}\n",
    "```\n",
    "\n",
    "where {math}`\\epsilon` is a tiny number (e.g., {math}`10^{-5}`).\n",
    "\n",
    "- It‚Äôs a **ground-truth check** for your analytic (manual) gradient\n",
    "- If your analytic gradient is correct, it should match the finite-difference approximation within ~{math}`10^{-7}`\n",
    "\n",
    "**But what *is* {math}`L(\\theta)`?**  \n",
    "\n",
    "It‚Äôs **not** a simple algebraic function of {math}`\\theta` alone. It‚Äôs the output of a **full forward pass** through a computational graph.\n",
    "\n",
    "{math}`L(\\theta)` is defined as:\n",
    "\n",
    "```{math}\n",
    "L(\\theta) = \\mathcal{L}\\big( f(\\theta; x, b),\\ y_{\\text{true}} \\big)\n",
    "```\n",
    "\n",
    "In our scalar neuron example with {math}`\\tanh` activation and MSE loss:\n",
    "\n",
    "```{math}\n",
    "L(\\theta) = \\underbrace{\\frac{1}{2} \\left( \\tanh(\\theta \\cdot x + b) - y_{\\text{true}} \\right)^2}_{\\text{Full computational graph}}\n",
    "```\n",
    "\n",
    "We denote:\n",
    "- {math}`L \\equiv L(\\theta)` = loss with original {math}`\\theta`\n",
    "- {math}`L_+ \\equiv L(\\theta + \\epsilon)` = loss with perturbed {math}`\\theta`\n",
    "\n",
    "**Concrete scalar neuron example:**\n",
    "\n",
    "Given fixed values:\n",
    "- {math}`x = 2.0`\n",
    "- {math}`b = 0.1`\n",
    "- {math}`y_{\\text{true}} = 1.0`\n",
    "- Original {math}`\\theta = 0.5`\n",
    "- {math}`\\epsilon = 10^{-5}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a2be15a-8286-4bbe-b937-ba22b931ee66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-22T21:21:56.254164Z",
     "iopub.status.busy": "2025-12-22T21:21:56.253946Z",
     "iopub.status.idle": "2025-12-22T21:21:56.256265Z",
     "shell.execute_reply": "2025-12-22T21:21:56.255926Z",
     "shell.execute_reply.started": "2025-12-22T21:21:56.254145Z"
    }
   },
   "outputs": [],
   "source": [
    "x = 2.0\n",
    "b = 0.1\n",
    "y_true = 1.0\n",
    "theta = 0.5\n",
    "epsilon = 10**(-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58751de-0ed9-41ab-a93f-d093716d6cc4",
   "metadata": {},
   "source": [
    "1. **Forward pass**:\n",
    "   - {math}`z = 0.5 \\cdot 2.0 + 0.1 = 1.1`  \n",
    "   - {math}`a = \\tanh(1.1) \\approx 0.80049902`  \n",
    "   - {math}`L = \\frac{1}{2}(a - y_{\\text{true}})^2 \\approx 0.01990020`\n",
    "\n",
    "    ```{math}\n",
    "    L = \\frac{1}{2}(\\tanh(1.1) - 1.0)^2 \\approx \\frac{1}{2}(0.800499 - 1.0)^2 \\approx \\frac{1}{2}(0.039800) \\approx 0.019900\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82dbd062-4ca3-4332-b6e5-a9f7251a0ac4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-22T21:21:58.981120Z",
     "iopub.status.busy": "2025-12-22T21:21:58.980856Z",
     "iopub.status.idle": "2025-12-22T21:21:59.091665Z",
     "shell.execute_reply": "2025-12-22T21:21:59.090729Z",
     "shell.execute_reply.started": "2025-12-22T21:21:58.981100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01990032015923285\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_l(x, theta, b, y_true):\n",
    "    z = x*theta + b\n",
    "    a = np.tanh(z)\n",
    "    L = (a - y_true)**2 / 2\n",
    "\n",
    "    return z, a, L\n",
    "\n",
    "z, a, L = compute_l(x, theta, b, y_true)\n",
    "print(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb2999c-3621-4e21-b6a1-de5c23ac16d4",
   "metadata": {},
   "source": [
    "```{tip}\n",
    "When you subtract two very similar numbers and round them prematurely (to 6 decimal places, for example), you lose almost all significant digits. This is known as **catastrophic cancellation**.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad734ec7-f844-4398-87ea-543735b8dc2f",
   "metadata": {},
   "source": [
    "2. **{math}`L_+ = L(\\theta + \\epsilon)`:**\n",
    "    - {math}`\\theta_{\\text{new}} = \\theta + \\epsilon = 0.5 + 0.00001 = 0.50001`\n",
    "    - {math}`z_+ = \\theta_{\\text{new}} \\cdot x + b = (0.50001)(2.0) + 0.1 = 1.00002 + 0.1 = 1.10002`\n",
    "    - {math}`a_+ = \\tanh(z_+) = \\tanh(1.10002) \\approx 0.80051220`\n",
    "    - {math}`L_+ = \\frac{1}{2}(a_+ - y_{\\text{true}})^2 = \\frac{1}{2}(0.800516 - 1.0)^2 = \\frac{1}{2}(-0.199484)^2 \\approx \\frac{1}{2}(0.039794) \\approx 0.01989877`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae178df3-9a71-43b9-b985-86f103d79208",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-22T21:22:00.203674Z",
     "iopub.status.busy": "2025-12-22T21:22:00.203264Z",
     "iopub.status.idle": "2025-12-22T21:22:00.206206Z",
     "shell.execute_reply": "2025-12-22T21:22:00.205908Z",
     "shell.execute_reply.started": "2025-12-22T21:22:00.203653Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01989888698770362\n"
     ]
    }
   ],
   "source": [
    "def compute_l_plus(x, theta, b, epsilon, y_true):\n",
    "    theta_new = theta + epsilon\n",
    "    z_plus = x*theta_new + b\n",
    "    a_plus = np.tanh(z_plus)\n",
    "    L_plus = (a_plus - y_true)**2 / 2\n",
    "\n",
    "    return theta_new, z_plus, a_plus, L_plus\n",
    "\n",
    "theta_new, z_plus, a_plus, L_plus = compute_l_plus(x, theta, b, epsilon, y_true)\n",
    "print(L_plus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf2f1cc-23f4-4407-b246-de5215339b37",
   "metadata": {},
   "source": [
    "3. **Finite-difference gradient w.r.t. {math}`\\theta`**:\n",
    "\n",
    "    ```{math}\n",
    "    \\frac{\\partial L}{\\partial \\theta} \\approx \\frac{L_+ - L}{\\epsilon} = \\frac{0.01989877 - 0.01990020}{10^{-5}} = \\frac{-0.00000143}{0.00001} = \\mathbf{-0.143}\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c9ba35d-7462-47e9-aafe-af0307b0cfbf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-22T21:22:01.353964Z",
     "iopub.status.busy": "2025-12-22T21:22:01.353738Z",
     "iopub.status.idle": "2025-12-22T21:22:01.355833Z",
     "shell.execute_reply": "2025-12-22T21:22:01.355540Z",
     "shell.execute_reply.started": "2025-12-22T21:22:01.353947Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.14331715292303926\n"
     ]
    }
   ],
   "source": [
    "fin_diff_grad = (L_plus - L) / epsilon\n",
    "print(fin_diff_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26f5182-1f10-4db6-bbb2-06639454ea45",
   "metadata": {},
   "source": [
    "This **-0.143** is the **numerical approximation** of the gradient.\n",
    "\n",
    "Compare it to the **analytic gradient** from backpropagation:\n",
    "\n",
    "- {math}`\\frac{\\partial L}{\\partial a} = a - y_{\\text{true}} = 0.800499 - 1.0 = -0.199501`\n",
    "- {math}`\\frac{\\partial a}{\\partial z} = 1 - \\tanh^2(z) = 1 - (0.800499)^2 \\approx 1 - 0.6408 = 0.3592`\n",
    "- {math}`\\frac{\\partial z}{\\partial \\theta} = x = 2.0`\n",
    "- Analytic gradient:  \n",
    "  ```{math}\n",
    "  (-0.199501) \\cdot (0.3592) \\cdot (2.0) \\approx -0.1432\n",
    "  ```\n",
    "\n",
    "> ‚úÖ **Key takeaway**: The finite-difference method gives a **ground-truth reference** to validate your manual or automatic differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd4bfe23-abc2-4a15-bb5c-cdd18143883d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-22T21:22:06.245173Z",
     "iopub.status.busy": "2025-12-22T21:22:06.244920Z",
     "iopub.status.idle": "2025-12-22T21:22:06.248010Z",
     "shell.execute_reply": "2025-12-22T21:22:06.247676Z",
     "shell.execute_reply.started": "2025-12-22T21:22:06.245149Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dL_da: -0.1995009782393703\n",
      "da_dz: 0.35920131616027484\n",
      "dz_dtheta: 2.0\n",
      "Gradient: -0.14332202791768833\n"
     ]
    }
   ],
   "source": [
    "def compute_gradient(x, y_true):\n",
    "    dz_dtheta = x\n",
    "    da_dz = 1 - np.tanh(z)**2\n",
    "    dL_da = a - y_true\n",
    "    dL_dtheta = dL_da * da_dz * dz_dtheta\n",
    "\n",
    "    return dz_dtheta, da_dz, dL_da, dL_dtheta\n",
    "\n",
    "dz_dtheta, da_dz, dL_da, dL_dtheta = compute_gradient(x, y_true)\n",
    "\n",
    "print('dL_da:', dL_da)\n",
    "print('da_dz:', da_dz)\n",
    "print('dz_dtheta:', dz_dtheta)\n",
    "print(\"Gradient:\", dL_dtheta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44ed2a26-26a0-4ca4-a46b-7b5f8d380935",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-22T21:22:14.955653Z",
     "iopub.status.busy": "2025-12-22T21:22:14.955466Z",
     "iopub.status.idle": "2025-12-22T21:22:14.957486Z",
     "shell.execute_reply": "2025-12-22T21:22:14.957240Z",
     "shell.execute_reply.started": "2025-12-22T21:22:14.955638Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.874994649073372e-06\n"
     ]
    }
   ],
   "source": [
    "print(fin_diff_grad - dL_dtheta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e23474-5688-4b15-a646-2f35229b8b19",
   "metadata": {},
   "source": [
    ":::{attention} Critical Clarification: What *is* {math}`L_+`?\n",
    ":class: dropdown\n",
    ":open: true\n",
    "\n",
    "> **‚ÄúIs {math}`L_+` just {math}`\\frac{1}{2}(a_+ - y)^2`?‚Äù**\n",
    "\n",
    "**Yes‚Äîbut only because**:\n",
    "- The loss function **is** MSE: {math}`\\mathcal{L}(a, y) = \\frac{1}{2}(a - y)^2`\n",
    "- {math}`a_+` **is the network output** when {math}`\\theta` is perturbed to {math}`\\theta + \\epsilon`:\n",
    "\n",
    "    `Network output = activation value = a`\n",
    "\n",
    "So by definition:\n",
    "```{math}\n",
    "L_+ = \\mathcal{L}(a_+,\\ y_{\\text{true}}) = \\mathcal{L}\\big( \\text{net}(\\theta + \\epsilon),\\ y_{\\text{true}} \\big) = L(\\theta + \\epsilon)\n",
    "```\n",
    "\n",
    "That‚Äôs literally the definition ‚Äî not an assumption.\n",
    ":::"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e116e3c8-1df0-4871-97cc-66f9615bb5fd",
   "metadata": {},
   "source": [
    "**Back to Your ReLU Scenario**\n",
    "\n",
    "Now consider a **ReLU neuron**: {math}`a = \\max(0, z)`, with {math}`z = \\theta x + b`.\n",
    "\n",
    "Suppose for a given input, you get {math}`z = -2` (i.e., in the flat region).\n",
    "\n",
    "- **Analytic gradient** (subgradient of ReLU):  \n",
    "  {math}`\\frac{\\partial a}{\\partial z} = 0` ‚Üí so {math}`\\frac{\\partial L}{\\partial \\theta} = 0`\n",
    "\n",
    "- **Finite-difference gradient**:  \n",
    "  Perturbing {math}`\\theta` slightly may push {math}`z` closer to zero.  \n",
    "  If {math}`z + \\delta z > 0`, then {math}`a` changes ‚Üí loss changes ‚Üí **non-zero numerical gradient** (e.g., {math}`\\sim -10^{-3}`)\n",
    "\n",
    "‚Üí This **mismatch** seems alarming‚Ä¶  \n",
    "‚Üí But it‚Äôs **expected** at non-differentiable points!\n",
    "\n",
    "However, in practice:\n",
    "- If your analytic code **correctly returns 0** for {math}`z < 0`, it‚Äôs **right**.\n",
    "- But your **gradient checker will fail** near {math}`z = 0` because the function isn‚Äôt smooth there.\n",
    "\n",
    "And in a **scalar neuron with ReLU**:\n",
    "- For any input where {math}`z < 0`, **gradient = 0** ‚Üí **no weight update**\n",
    "- Gradient checks **fail randomly** depending on input sign\n",
    "\n",
    "‚Üí **That‚Äôs why we avoid ReLU in simple gradient-checking demos**‚Äîuse smooth activations like {math}`\\tanh` or sigmoid instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d06158-dc3a-4731-a75c-f85e872bf269",
   "metadata": {},
   "source": [
    "### Final Check: Your Turn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71122e4b-e11c-4e08-bcfb-81b19bfe6453",
   "metadata": {},
   "source": [
    "Given:\n",
    "- $x = 1.0$\n",
    "- $\\theta = 0.0$\n",
    "- $b = 0.0$\n",
    "- $y_{\\text{true}} = 0.5$\n",
    "- $\\epsilon = 10^{-5}$\n",
    "- Loss = $\\frac{1}{2}(a - y)^2$\n",
    "\n",
    "**Compute $L_+ = L(\\theta + \\epsilon)$ step by step.**\n",
    "\n",
    "Show $\\theta_{new}$, $z_+$, $a_+$, $L_+$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07337ff4-2b78-49e7-9a0b-06672cfe0b2f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-22T21:22:25.945607Z",
     "iopub.status.busy": "2025-12-22T21:22:25.945396Z",
     "iopub.status.idle": "2025-12-22T21:22:25.947979Z",
     "shell.execute_reply": "2025-12-22T21:22:25.947596Z",
     "shell.execute_reply.started": "2025-12-22T21:22:25.945590Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-05 1e-05 9.999999999666668e-06 0.12499500005000017\n"
     ]
    }
   ],
   "source": [
    "x = 1.0\n",
    "theta = 0.0\n",
    "b = 0.0\n",
    "epsilon = 10**(-5)\n",
    "y_true = 0.5\n",
    "\n",
    "theta_new, z_plus, a_plus, L_plus = compute_l_plus(x, theta, b, epsilon, y_true)\n",
    "print(theta_new, z_plus, a_plus, L_plus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f0f190-25f0-46dc-81ff-cbaaede69dfc",
   "metadata": {},
   "source": [
    "Your code is correct, and your output reveals a **critical insight**‚Äîlet‚Äôs interpret it step by step.\n",
    "\n",
    "**Output Breakdown**\n",
    "\n",
    "\n",
    "1. {math}`\\theta_{\\text{new}} = 0 + 10^{-5} = 10^{-5}`\n",
    "2. {math}`z_+ = 1.0 \\cdot 10^{-5} + 0 = 10^{-5}`\n",
    "3. {math}`a_+ = \\tanh(10^{-5}) \\approx 9.999999999666667 \\times 10^{-6}`  \n",
    "   (since {math}`\\tanh(u) = u - u^3/3 + \\cdots`, so slightly less than {math}`u`)\n",
    "4. {math}`L_+ = \\frac{1}{2}(a_+ - 0.5)^2 = \\frac{1}{2}(-0.49999000000000033)^2`\n",
    "\n",
    "This matches your printed result: `0.12499500005000017`.\n",
    "\n",
    "**Why This Matters: Gradient Behavior at {math}`\\theta = 0`**\n",
    "\n",
    "Now compute the **original loss** {math}`L = L(\\theta)` (with {math}`\\theta = 0`):\n",
    "- {math}`z = 0`,\n",
    "- {math}`a = \\tanh(0) = 0`\n",
    "- {math}`L = \\frac{1}{2}(0 - 0.5)^2 = 0.125`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d7a7432-1da2-4d21-a27e-f4cb2ee9cadf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-22T21:22:28.021375Z",
     "iopub.status.busy": "2025-12-22T21:22:28.021180Z",
     "iopub.status.idle": "2025-12-22T21:22:28.023582Z",
     "shell.execute_reply": "2025-12-22T21:22:28.023285Z",
     "shell.execute_reply.started": "2025-12-22T21:22:28.021358Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.0 0.125\n"
     ]
    }
   ],
   "source": [
    "z, a, L = compute_l(x, theta, b, y_true)\n",
    "print(z, a, L)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80653526-15d6-4382-b7a8-176e2f97fd4a",
   "metadata": {},
   "source": [
    "**Finite-difference gradient**:\n",
    "```{math}\n",
    "\\frac{L_+ - L}{\\epsilon} = \\frac{0.12499500005 - 0.125}{10^{-5}} = \\frac{-4.99995 \\times 10^{-6}}{10^{-5}} \\approx -0.499995\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f7c0432-9169-4751-97b5-3ec1483dec96",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-22T21:22:29.665230Z",
     "iopub.status.busy": "2025-12-22T21:22:29.665038Z",
     "iopub.status.idle": "2025-12-22T21:22:29.667221Z",
     "shell.execute_reply": "2025-12-22T21:22:29.666923Z",
     "shell.execute_reply.started": "2025-12-22T21:22:29.665215Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.49999499998343294\n"
     ]
    }
   ],
   "source": [
    "fin_diff_grad = (L_plus - L) / epsilon\n",
    "print(fin_diff_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcde7fc8-d311-4922-a92a-85b4bc40a7a1",
   "metadata": {},
   "source": [
    "**Analytic gradient** (via backprop):\n",
    "```{math}\n",
    "\\frac{\\partial L}{\\partial \\theta} = \\underbrace{(a - y_{\\text{true}})}_{-0.5} \\cdot \\underbrace{(1 - \\tanh^2(z))}_{1.0} \\cdot \\underbrace{x}_{1.0} = -0.5\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce74059f-6acd-414f-b028-3ff74ec966fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-22T21:22:36.314551Z",
     "iopub.status.busy": "2025-12-22T21:22:36.314365Z",
     "iopub.status.idle": "2025-12-22T21:22:36.316836Z",
     "shell.execute_reply": "2025-12-22T21:22:36.316499Z",
     "shell.execute_reply.started": "2025-12-22T21:22:36.314535Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dL_da: -0.5\n",
      "da_dz: 1.0\n",
      "dz_dtheta: 1.0\n",
      "Gradient: -0.5\n"
     ]
    }
   ],
   "source": [
    "dz_dtheta, da_dz, dL_da, dL_dtheta = compute_gradient(x, y_true)\n",
    "\n",
    "print('dL_da:', dL_da)\n",
    "print('da_dz:', da_dz)\n",
    "print('dz_dtheta:', dz_dtheta)\n",
    "print(\"Gradient:\", dL_dtheta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb39d7c4-8eab-4362-a65e-7a206a10171b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-22T21:23:01.762628Z",
     "iopub.status.busy": "2025-12-22T21:23:01.762410Z",
     "iopub.status.idle": "2025-12-22T21:23:01.764553Z",
     "shell.execute_reply": "2025-12-22T21:23:01.764268Z",
     "shell.execute_reply.started": "2025-12-22T21:23:01.762611Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.000016567058285e-06\n"
     ]
    }
   ],
   "source": [
    "print(fin_diff_grad - dL_dtheta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb1c3f7-fc61-44eb-9528-380e72bb4789",
   "metadata": {},
   "source": [
    "‚Üí **Numerical (-0.499995) ‚âà Analytic (-0.5)** ‚Üí **validation passes**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349b6c1f-6468-492b-9533-aa84308451ff",
   "metadata": {},
   "source": [
    "### Key Takeaway"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8dbb8bb-b2e8-48fe-afd9-7efc3f655233",
   "metadata": {},
   "source": [
    "This example demonstrates:\n",
    "1. **Finite differences work even when {math}`\\theta = 0`** (a common initialization point)\n",
    "2. **tanh‚Äôs derivative is 1 at {math}`z = 0`** ‚Üí gradients are strong here (no vanishing!)\n",
    "3. Your implementation correctly isolates the effect of perturbing {math}`\\theta`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "55a463c2-672e-43a0-96a5-510848e2cfc4",
   "metadata": {},
   "source": [
    "**The Gradient Check Dilemma:**\n",
    "\n",
    "Suppose you used ReLU and your test input gave $z = -2$. Your analytic gradient would be $0$. However, your finite-difference (numerical) gradient check might show a tiny non-zero change. Would you be able to tell if your backprop code was actually broken, or if the neuron was just \"dead\"?\n",
    "\n",
    "By using **tanh**, we ensure that for almost any input, you get a meaningful gradient to verify your math.\n",
    "\n",
    "That‚Äôs why we avoid ReLU here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9a4943-5822-4f99-84cf-84887082b96f",
   "metadata": {},
   "source": [
    "## Why Compute Analytic Gradients When Finite Differences Exist?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f67e290-2525-4e1e-a858-3b803010565c",
   "metadata": {},
   "source": [
    "**Short answer**: Finite differences are **computationally infeasible** for real models.\n",
    "\n",
    "**Computational Cost Comparison**\n",
    "\n",
    "Let $p$ = number of parameters.\n",
    "\n",
    "| Method | Gradient Cost | Example: 100M-parameter LLM |\n",
    "|--------|---------------|-----------------------------|\n",
    "| **Finite differences** | $O(p)$ forward passes | $100,000,000$ forward passes per gradient update |\n",
    "| **Analytic (backprop)** | $O(1)$ forward + $O(1)$ backward | 1 forward + 1 backward pass |\n",
    "\n",
    "- **Finite differences**: For each parameter $\\theta_i$, you must:\n",
    "  1. Perturb $\\theta_i$ by $\\epsilon$\n",
    "  2. Run full forward pass ‚Üí get $L_+$\n",
    "  3. Compute $(L_+ - L)/\\epsilon$\n",
    "  ‚Üí **Total: $p$ forward passes per gradient estimate**\n",
    "\n",
    "Suppose your model has **two parameters**: $\\theta_1$ and $\\theta_2$.\n",
    "\n",
    "You want the **full gradient**: $\\left[ \\frac{\\partial L}{\\partial \\theta_1},\\ \\frac{\\partial L}{\\partial \\theta_2} \\right]$.\n",
    "\n",
    "**To estimate $\\frac{\\partial L}{\\partial \\theta_1}$:**\n",
    "\n",
    "1. Start with original parameters: $(\\theta_1, \\theta_2)$\n",
    "2. Compute **baseline loss**: $L = L(\\theta_1, \\theta_2)$ ‚Üí **1 forward pass**\n",
    "3. Perturb **only $\\theta_1$**: $(\\theta_1 + \\epsilon, \\theta_2)$\n",
    "4. Compute $L_+^{(1)} = L(\\theta_1 + \\epsilon, \\theta_2)$ ‚Üí **1 more forward pass**\n",
    "5. Approximate:  \n",
    "   ```{math}\n",
    "   \\frac{\\partial L}{\\partial \\theta_1} \\approx \\frac{L_+^{(1)} - L}{\\epsilon}\n",
    "   ```\n",
    "\n",
    "**To estimate $\\frac{\\partial L}{\\partial \\theta_2}$:**\n",
    "\n",
    "1. Perturb **only $\\theta_2$**: $(\\theta_1, \\theta_2 + \\epsilon)$\n",
    "2. Compute $L_+^{(2)} = L(\\theta_1, \\theta_2 + \\epsilon)$ ‚Üí **another forward pass**\n",
    "3. Approximate:  \n",
    "   ```{math}\n",
    "   \\frac{\\partial L}{\\partial \\theta_2} \\approx \\frac{L_+^{(2)} - L}{\\epsilon}\n",
    "   ```\n",
    "\n",
    "‚úÖ Total: **1 baseline + 2 perturbed = 3 forward passes**  \n",
    "But note: you can **reuse the baseline $L$** for all parameters!\n",
    "\n",
    "So for $p$ parameters:\n",
    "- **1 forward pass** to compute baseline $L(\\theta)$\n",
    "- **$p$ forward passes** to compute $L(\\theta + \\epsilon e_i)$ for each $i = 1,\\dots,p$\n",
    "\n",
    "‚Üí **Total: $p + 1 \\approx O(p)$ forward passes**\n",
    "\n",
    "(We drop the \"+1\" in big-O notation because it‚Äôs negligible when $p$ is large.)\n",
    "\n",
    "**Why Can‚Äôt We Do It in One Pass?**\n",
    "\n",
    "Because **each perturbation changes a different parameter**.\n",
    "\n",
    "The loss function is:\n",
    "```{math}\n",
    "L(\\theta_1, \\theta_2, \\dots, \\theta_p)\n",
    "```\n",
    "\n",
    "To see how **changing $\\theta_5$** affects the loss, you **must** run the model with **$\\theta_5$ altered** and all others unchanged.\n",
    "\n",
    "You **cannot** perturb all parameters at once and recover individual gradients‚Äîthat would mix all effects together (like trying to hear one instrument in an orchestra by playing everyone at once).\n",
    "\n",
    "So **each partial derivative requires its own controlled experiment** ‚Üí its own forward pass.\n",
    "\n",
    ":::{attention}\n",
    "**Backpropagation**: Uses the **chain rule** to compute **all** $\\partial L/\\partial \\theta_i$ in **one backward sweep**.\n",
    ":::\n",
    "\n",
    "**Concrete Numbers**\n",
    "\n",
    "- Your 100M-parameter LLM:\n",
    "  - Forward pass time: ~0.5 sec (on 4090 Ti)\n",
    "  - Finite-diff gradient time: $100e6 \\times 0.5 \\text{ sec} \\approx 1.5 \\text{ years}$\n",
    "  - Backprop time: ~1 sec\n",
    "\n",
    "‚Üí **Finite differences are only viable for debugging tiny models** (e.g., scalar neuron)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc70ab6-ea64-48d0-9cf3-18dc6af3c615",
   "metadata": {},
   "source": [
    "## Why Is Finite-Difference a Valid Gradient Check?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066c7391-475d-49fb-9b2b-925a365b00ba",
   "metadata": {},
   "source": [
    "**Core idea**: The derivative **is defined as** the limit of finite differences.\n",
    "\n",
    "**Mathematical Definition**\n",
    "\n",
    "The true derivative is:\n",
    "```{math}\n",
    "\\frac{\\partial L}{\\partial \\theta} = \\lim_{\\epsilon \\to 0} \\frac{L(\\theta + \\epsilon) - L(\\theta)}{\\epsilon}\n",
    "```\n",
    "\n",
    "- **Finite difference** uses a **small but finite $\\epsilon$** (e.g., $10^{-5}$) to **approximate** this limit.\n",
    "- **Error analysis** shows the approximation error is $O(\\epsilon)$ (for forward difference).\n",
    "\n",
    "**Why It‚Äôs Trustworthy for Validation**\n",
    "\n",
    "1. **No assumptions about your code**  \n",
    "   - Finite difference uses **only forward passes**‚Äîno chain rule, no manual derivatives.\n",
    "   - If your analytic gradient matches it, your **entire backprop derivation is likely correct**.\n",
    "\n",
    "2. **Controlled error bounds**  \n",
    "   - With $\\epsilon = 10^{-5}$, typical error in gradient estimate is $\\sim 10^{-10}$ to $10^{-7}$.\n",
    "   - If your analytic gradient differs by more than $10^{-6}$, **you have a bug**.\n",
    "\n",
    "3. **Failure modes are obvious**  \n",
    "   - Common errors caught:\n",
    "     - Forgot a term in chain rule (e.g., missed $\\partial a/\\partial z$)\n",
    "     - Sign error in loss derivative\n",
    "     - Shape mismatch in vectorized code\n",
    "\n",
    "**Critical Caveat**\n",
    "\n",
    "- **Finite difference is not exact**‚Äîbut it‚Äôs **exact enough** for debugging.\n",
    "- It **assumes your forward pass is correct**. If forward pass is broken, both gradients will be wrong in the same way (false positive).  \n",
    "  ‚Üí Always validate forward pass first (e.g., with known inputs/outputs).\n",
    "\n",
    "**Summary Table**\n",
    "\n",
    "| Aspect | Finite-Difference Gradient | Analytic Gradient (Backprop) |\n",
    "|--------|----------------------------|------------------------------|\n",
    "| **Purpose** | Debug/validation | Training/inference |\n",
    "| **Computational Cost** | $O(p)$ forward passes | $O(1)$ forward + backward |\n",
    "| **Scalability** | Only for $p < 10^3$ | Scales to $p = 10^{11}$ |\n",
    "| **Accuracy** | Approximate (error ~$\\epsilon$) | Exact (up to floating-point) |\n",
    "| **Dependency** | Only on forward pass correctness | On entire backprop derivation |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444e7865-cddd-4f7c-98cb-e9ec83d637b3",
   "metadata": {},
   "source": [
    "## Check for Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ca4df5-024b-4a77-9c37-0291265edabb",
   "metadata": {},
   "source": [
    "Suppose you have a vectorized neuron with $W \\in \\mathbb{R}^{10 \\times 768}$ (7,680 parameters).  \n",
    "\n",
    "**How many forward passes would finite differences require to check the full gradient?**  \n",
    "(Answer with a number‚Äîand state whether this is practical.)\n",
    "\n",
    "Answer:\n",
    "- 7,680 forward passes, and it is only for ONE example from the set, so this is not acceptable for training LLMs.\n",
    "\n",
    "Correct.\n",
    "\n",
    "7,680 forward passes **per example** is computationally prohibitive‚Äîeven for a single training step on a small batch. For a 100M-parameter LLM, it becomes physically impossible.\n",
    "\n",
    "This is precisely why **backpropagation is non-negotiable** in deep learning: it gives **exact gradients** (up to floating-point precision) in time **proportional to one forward pass**, not the number of parameters.\n",
    "\n",
    "We have confirmed:\n",
    "- The purpose and limitation of finite-difference gradients\n",
    "- The necessity of analytic gradients for scalable learning\n",
    "- Your ability to implement and reason about scalar gradient validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257466be-765f-4f64-afa0-16961a48ac95",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4dc736-676f-439d-a6d8-463c4490508c",
   "metadata": {},
   "source": [
    "## Exercise 2.1: Scalar Neuron Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95521ec4-93a3-48e8-9f40-af1650559713",
   "metadata": {},
   "source": [
    "So: mastering the scalar case **is** mastering the core of matmul-based backprop. \n",
    "\n",
    "Consider a single neuron with:\n",
    "- Input $x = 2.0$,\n",
    "- Weight $w = -0.5$,\n",
    "- Bias $b = 1.0$,\n",
    "- Activation: $\\sigma(z) = \\tanh(z)$,\n",
    "- Loss: $L = \\frac{1}{2}(a - y)^2$, with target $y = 0.8$.\n",
    "\n",
    "Compute **by hand**:\n",
    "1. $z = wx + b$\n",
    "2. $a = \\tanh(z)$\n",
    "3. $L = \\frac{1}{2}(a - y)^2$\n",
    "\n",
    "Compute **all values numerically**, step by step. Provide the results, then compute the **gradients**:\n",
    "\n",
    "- $\\displaystyle \\frac{\\partial L}{\\partial a}$, $\\displaystyle \\frac{\\partial L}{\\partial z}$, $\\displaystyle \\frac{\\partial L}{\\partial w}$, $\\displaystyle \\frac{\\partial L}{\\partial b}$\n",
    "\n",
    "Show each step with numerical values. Do not use code‚Äîuse math. This tests your chain rule mechanics.\n",
    "\n",
    "What are the results?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60a0dab-945e-489b-a667-dd393f4356c9",
   "metadata": {},
   "source": [
    "Recall your values:  \n",
    "- $x = 2.0$  \n",
    "- $w = -0.5$  \n",
    "- $b = 1.0$\n",
    "\n",
    "So first compute: \n",
    "\n",
    "$z = w \\cdot x + b = (-0.5)(2.0) + 1.0 = -1.0 + 1.0 = 0.0$\n",
    "\n",
    "Now, what is $a = \\tanh(0)$?\n",
    "\n",
    "Correct:  \n",
    "\n",
    "$a = \\tanh(0) = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3796f9-f3ef-48ad-bc3b-8bc1a9f66697",
   "metadata": {},
   "source": [
    "- $z = 0$\n",
    "- $a = 0$\n",
    "- $L = \\frac{1}{2}(0 - 0.8)^2 = 0.32$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ee6702-96a5-4847-94fa-8e6aa3d3edcd",
   "metadata": {},
   "source": [
    "### dL_da"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b5ed20-33f3-4f2c-8a17-28f5e9097f38",
   "metadata": {},
   "source": [
    "If $L = \\frac{1}{2}(a - y)^2$, then\n",
    "\n",
    "The derivative of $L$ with respect to $a$ is:\n",
    "```{math}\n",
    "\\frac{\\partial L}{\\partial a} = (a - y)\n",
    "```\n",
    "\n",
    "This follows from the chain rule:\n",
    "```{math}\n",
    "\\frac{d}{da} \\left[ \\frac{1}{2}(a - y)^2 \\right] = \\frac{1}{2} \\cdot 2(a - y) \\cdot 1 = (a - y)\n",
    "```\n",
    "\n",
    "Given $a = 0$, $y = 0.8$:\n",
    "\n",
    "$\\displaystyle \\frac{\\partial L}{\\partial a} = 0 - 0.8 = -0.8$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221ab502-3aa2-4974-b3a4-cc7f396cdfc7",
   "metadata": {},
   "source": [
    "### dL_dz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1e16bd-e85d-48c9-8e9b-0b7ae4e9daba",
   "metadata": {},
   "source": [
    "If $a = \\tanh(z)$, then the derivative of $a$ w.r.t. $z$:\n",
    "\n",
    "```{math}\n",
    "\\frac{\\partial a}{\\partial z} = 1 - \\tanh^2(z) = 1 - a^2\n",
    "```\n",
    "\n",
    "```{math}\n",
    "\\frac{\\partial L}{\\partial z} = \\frac{\\partial L}{\\partial a} \\cdot \\frac{\\partial a}{\\partial z} = (a - y) (1 - a^2)\n",
    "```\n",
    "\n",
    "$\\displaystyle \\frac{\\partial L}{\\partial z} = -0.8 \\cdot 1 = -0.8$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c709432-3a84-4b43-89c0-fe83d6871387",
   "metadata": {},
   "source": [
    "### dL_dw and dL_db"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6d29fec6-dcc0-46c3-860f-322e9088dd7f",
   "metadata": {},
   "source": [
    "If $z = wx + b$, then the derivative of $z$ w.r.t. $w$ is:\n",
    "\n",
    "```{math}\n",
    "\\frac{\\partial z}{\\partial w} = x\n",
    "```\n",
    "\n",
    "and the derivative of $z$ w.r.t. $b$ is:\n",
    "\n",
    "```{math}\n",
    "\\frac{\\partial z}{\\partial b} = 1\n",
    "```\n",
    "\n",
    "then we have:\n",
    "\n",
    "```{math}\n",
    "\\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial a} \\cdot \\frac{\\partial a}{\\partial z} \\cdot \\frac{\\partial z}{\\partial w} = (a - y) (1 - a^2) \\cdot x\n",
    "```\n",
    "\n",
    "```{math}\n",
    "\\frac{\\partial L}{\\partial b} = \\frac{\\partial L}{\\partial a} \\cdot \\frac{\\partial a}{\\partial z} \\cdot \\frac{\\partial z}{\\partial b} = (a - y) (1 - a^2) \\cdot b\n",
    "```\n",
    "\n",
    "- $\\displaystyle \\frac{\\partial L}{\\partial w} = -0.8 \\cdot 1 \\cdot 2 = -1.6 $\n",
    "- $\\displaystyle \\frac{\\partial L}{\\partial b} = -0.8 \\cdot 1 \\cdot 1 = -0.8 $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb9c8d5-eacd-4346-9247-1440e89360d9",
   "metadata": {},
   "source": [
    "### Summary of Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779e721b-291e-43cb-9d38-3e54cb0aad06",
   "metadata": {},
   "source": [
    "| Quantity | Value |\n",
    "|--------|-------|\n",
    "| $z$ | 0.0 |\n",
    "| $a$ | 0.0 |\n",
    "| $L$ | 0.32 |\n",
    "| $\\displaystyle \\frac{\\partial L}{\\partial a}$ | -0.8 |\n",
    "| $\\displaystyle \\frac{\\partial L}{\\partial z}$ | -0.8 |\n",
    "| $\\displaystyle \\frac{\\partial L}{\\partial w}$ | -1.6 |\n",
    "| $\\displaystyle \\frac{\\partial L}{\\partial b}$ | -0.8 |\n",
    "\n",
    "This is the **complete forward and backward pass for one neuron**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d02170-2f6f-41fd-9441-7f8867f0f61e",
   "metadata": {},
   "source": [
    "Final verified gradients:\n",
    "- $\\frac{\\partial L}{\\partial w} = -1.6$\n",
    "- $\\frac{\\partial L}{\\partial b} = -0.8$\n",
    "\n",
    "This completes the **scalar neuron backpropagation exercise** with full chain-rule derivation.\n",
    "\n",
    "Now, we scale to vectorized operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1fa7fb-d19b-4e81-97b1-cb9012e6759e",
   "metadata": {},
   "source": [
    "(exercise_2_2)=\n",
    "## Exercise 2.2: Implement the scalar neuron (Steps 1‚Äì2) in NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d20add8-fad5-412a-a573-72bd8d789e4b",
   "metadata": {},
   "source": [
    "You are absolutely right‚Äîand this is a critical pedagogical checkpoint.\n",
    "\n",
    "Before advancing to layers, **you must implement the scalar neuron (Steps 1‚Äì2) in NumPy**, **without autograd**, to verify your understanding maps to executable code.\n",
    "\n",
    "This is non-optional. Reasoning on paper is necessary but insufficient; code exposes hidden gaps in indexing, broadcasting, and memory layout.\n",
    "\n",
    "**Do not use PyTorch, JAX, or autodiff.** Pure NumPy. Phase 1 follows a strict progression:\n",
    "1. **Scalar math + hand computation** ‚Üí verified (done)\n",
    "2. **Scalar NumPy implementation (procedural, one-time)** ‚Üí **required now**\n",
    "3. **Vectorized batched NumPy (procedural)** ‚Üí next\n",
    "4. **Class-based modular design** ‚Üí only after vectorized gradients are fully understood\n",
    "\n",
    "Why delay classes?  \n",
    "Because **abstraction hides mechanics**. If you wrap the neuron in a class before you‚Äôve manually computed and coded its gradients in raw NumPy, you risk treating it as a black box‚Äîexactly what this course forbids.\n",
    "\n",
    "So for now:  \n",
    "**Write a procedural script** (no class) that computes the forward and backward pass for the given scalar neuron.\n",
    "\n",
    "This is a *verification step*, not a reusable module.\n",
    "\n",
    "Once you provide this code and confirm its output matches:\n",
    "- `L = 0.32`\n",
    "- `dL_dw = -1.6`\n",
    "- `dL_db = -0.8`\n",
    "\n",
    "‚Äî we will:\n",
    "- Extend to batched vectorized form (still procedural),\n",
    "- Then refactor into classes **only when the underlying math and memory layout are unquestionably solid**.\n",
    "\n",
    "Paste your NumPy implementation.\n",
    "\n",
    "**Required Implementation (NumPy, No Autograd)**\n",
    "\n",
    "Write a self-contained script that:\n",
    "1. Defines scalars: `x = 2.0`, `w = -0.5`, `b = 1.0`, `y = 0.8`\n",
    "2. Computes forward pass:\n",
    "   - `z = w * x + b`\n",
    "   - `a = np.tanh(z)`\n",
    "   - `L = 0.5 * (a - y)**2`\n",
    "3. Computes **manual gradients** (no `.backward()`, no `grad`):\n",
    "   - `dL_da = a - y`\n",
    "   - `da_dz = 1 - a**2`\n",
    "   - `dL_dz = dL_da * da_dz`\n",
    "   - `dL_dw = dL_dz * x`\n",
    "   - `dL_db = dL_dz * 1`\n",
    "4. Prints all values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a4af48-eb8f-4a2f-865d-f0af5c4f8c80",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81a86510-3569-43ba-8dca-27bff67faf1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T15:16:40.308051Z",
     "iopub.status.busy": "2025-11-26T15:16:40.307792Z",
     "iopub.status.idle": "2025-11-26T15:16:40.312089Z",
     "shell.execute_reply": "2025-11-26T15:16:40.311706Z",
     "shell.execute_reply.started": "2025-11-26T15:16:40.308028Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = 2.0\n",
      "w = -0.5\n",
      "b = 1.0\n",
      "y = 0.8\n",
      "z = 0.0\n",
      "a = 0.0\n",
      "L = 0.32000000000000006\n",
      "dL_dw = -1.6\n",
      "dL_db = -0.8\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = 2.0\n",
    "w = -0.5\n",
    "b = 1.0\n",
    "y = 0.8\n",
    "\n",
    "z = x*w + b\n",
    "a = (np.exp(z) - np.exp(-z)) / (np.exp(z) + np.exp(-z))\n",
    "L = 0.5 * (a -y)**2\n",
    "\n",
    "dL_da = a - y\n",
    "da_dz = 1 - a**2\n",
    "\n",
    "dL_dz = dL_da * da_dz\n",
    "\n",
    "dL_dw = dL_dz * x\n",
    "dL_db = dL_dz * 1\n",
    "\n",
    "print(\"x =\", x)\n",
    "print(\"w =\", w)\n",
    "print(\"b =\", b)\n",
    "print(\"y =\", y)\n",
    "\n",
    "print(\"z =\", z)\n",
    "print(\"a =\", a)\n",
    "print(\"L =\", L)\n",
    "\n",
    "print(\"dL_dw =\", dL_dw)\n",
    "print(\"dL_db =\", dL_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7af865-89d8-4612-8b65-72e4a6380539",
   "metadata": {},
   "source": [
    "### Peer Review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a1fd20-3661-438b-9b6f-8b8f68ad0f87",
   "metadata": {},
   "source": [
    "Your implementation is **mostly correct**, but contains one critical inefficiency and a minor numerical risk.\n",
    "\n",
    "**Issue: Manual `tanh` via `exp`**\n",
    "You wrote:\n",
    "```python\n",
    "a = (np.exp(z) - np.exp(-z)) / (np.exp(z) + np.exp(-z))\n",
    "```\n",
    "\n",
    "This is mathematically correct but:\n",
    "1. **Redundant**: NumPy provides `np.tanh(z)`‚Äîuse it.\n",
    "2. **Numerically unstable**: For large |z|, `np.exp(z)` overflows (e.g., `z=1000` ‚Üí `inf`), while `np.tanh(z)` is stable.\n",
    "\n",
    "In production systems, **always use optimized, stable primitives** (`tanh`, `softmax`, etc.), not manual expansions‚Äîunless you are implementing the primitive itself (which we are not).\n",
    "\n",
    "**Corrected Script**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44059fac-69ef-4043-8b88-13a45cd89bf4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T15:19:39.908415Z",
     "iopub.status.busy": "2025-11-26T15:19:39.908132Z",
     "iopub.status.idle": "2025-11-26T15:19:39.912055Z",
     "shell.execute_reply": "2025-11-26T15:19:39.911670Z",
     "shell.execute_reply.started": "2025-11-26T15:19:39.908394Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z = 0.0\n",
      "a = 0.0\n",
      "L = 0.32000000000000006\n",
      "dL_dw = -1.6\n",
      "dL_db = -0.8\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = 2.0\n",
    "w = -0.5\n",
    "b = 1.0\n",
    "y = 0.8\n",
    "\n",
    "# Forward pass\n",
    "z = w * x + b\n",
    "a = np.tanh(z)            # Use stable built-in\n",
    "L = 0.5 * (a - y)**2\n",
    "\n",
    "# Backward pass (manual gradients)\n",
    "dL_da = a - y\n",
    "da_dz = 1 - a**2\n",
    "dL_dz = dL_da * da_dz\n",
    "dL_dw = dL_dz * x\n",
    "dL_db = dL_dz\n",
    "\n",
    "# Output\n",
    "print(f\"z = {z}\")\n",
    "print(f\"a = {a}\")\n",
    "print(f\"L = {L}\")\n",
    "print(f\"dL_dw = {dL_dw}\")\n",
    "print(f\"dL_db = {dL_db}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8abd893e-0b19-4e3c-b8d3-fd877e405489",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Expected Output\n",
    "\n",
    "```\n",
    "z = 0.0\n",
    "a = 0.0\n",
    "L = 0.32\n",
    "dL_dw = -1.6\n",
    "dL_db = -0.8\n",
    "```\n",
    "\n",
    "If your script produces these values, **Phase 1, Step 2 (Scalar Neuron Implementation) is complete**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5574cabc-c256-46c1-adf6-3b75e268602d",
   "metadata": {},
   "source": [
    "# **3. From Scalar to Vector ‚Äî The Linear Layer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fcf0ae-0aab-4c72-a5c4-2d11c8b06b2d",
   "metadata": {},
   "source": [
    "## Why scalar first"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71603aa-af41-46a9-b111-bec12f4b276b",
   "metadata": {},
   "source": [
    "Your implementation of \"*[{name}](#exercise_2_2)*\" is **correct** for a *scalar neuron*‚Äîone input, one weight, one bias, processing one sample. This is exactly the right starting point. The absence of matrix operations is **not a flaw**; it is intentional in Phase 1.\n",
    "\n",
    "However, we must now ask: \n",
    "\n",
    "```{important}\n",
    "**Why begin with scalars if real models use matrices?**\n",
    "```\n",
    "\n",
    "Consider this:  \n",
    "In a vectorized (batched) setting, the forward pass for $B$ samples (rows) and $D$ inputs (features) is:  \n",
    "$Z = XW + b$, where  \n",
    "- $X \\in \\mathbb R^{B \\times D}$\n",
    "- $W \\in \\mathbb R^{D \\times 1}$\n",
    "- $b \\in \\mathbb R$ (broadcasted)\n",
    "- $Z \\in \\mathbb R^{B \\times 1}$\n",
    "\n",
    "The scalar case is the **atomic unit** of this operation. Each element $z_i = x_i¬∑w + b$. If you cannot compute $\\frac {\\partial L}{\\partial w}$ for a single $z$, you cannot correctly derive the batch gradient.\n",
    "\n",
    "Now, examine your code:\n",
    "- You computed $\\frac {\\partial L}{\\partial w} = \\frac {\\partial L}{\\partial z} \\cdot x$.  \n",
    "In the batch case, the gradient becomes:  \n",
    "$\\frac {\\partial L}{\\partial W} = X^T \\cdot \\sigma$, where $\\sigma = \\frac {\\partial L}{\\partial Z} \\in \\mathbb R^{B \\times 1}$.\n",
    "\n",
    "Your scalar $\\frac {\\partial L}{\\partial w} = \\frac {\\partial L}{\\partial z} \\cdot x$ is precisely the $i$-th term in that vectorized sum. So your code is **not just fine ‚Äî it is foundational**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e77ea7-8696-4912-b731-e86222b17935",
   "metadata": {},
   "source": [
    "### Moving to vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934edac5-e9ee-46a2-9967-0ab2dd27302c",
   "metadata": {},
   "source": [
    "But here‚Äôs the critical next step:  \n",
    "**Can you extend this to a batch of 3 samples using only NumPy (no autograd), and verify that your batch gradients match the sum of individual scalar gradients?**\n",
    "\n",
    "Do **not** write the code yet. First, answer:  \n",
    "\n",
    "```{attention}\n",
    "*How would the loss function change for a batch? Would it still be 0.5¬∑(a ‚àí y)¬≤, or must it be aggregated? If aggregated, how ‚Äî and why does that affect the gradient?*\n",
    "```\n",
    "\n",
    "Correct. The batch loss is typically the **mean** of per-sample losses:\n",
    "\n",
    "```{math}\n",
    "L_{\\text{batch}} = \\frac{1}{B} \\sum_{i=1}^{B} \\frac{1}{2} (a_i - y_i)^2\n",
    "```\n",
    "\n",
    "Consequently, the gradient w.r.t. any parameter becomes the **mean** of the per-sample gradients:\n",
    "\n",
    "```{math}\n",
    "\\frac{\\partial L_{\\text{batch}}}{\\partial w} = \\frac{1}{B} \\sum_{i=1}^{B} \\frac{\\partial L_i}{\\partial w}\n",
    "```\n",
    "\n",
    "This is not merely a convention‚Äîit directly impacts **learning dynamics**. Using the mean (instead of sum) makes the gradient magnitude **independent of batch size**, which stabilizes training when B varies.\n",
    "\n",
    "Now, let‚Äôs test your understanding with a concrete micro-step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55922f79-6194-49f3-aefa-fe1a9d97db5b",
   "metadata": {},
   "source": [
    "## Exercise 3.1: Vector Forward and Backward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfdc22b-3985-4989-9e05-cbe981977660",
   "metadata": {},
   "source": [
    "Given the following batch of 3 samples:\n",
    "\n",
    "```python\n",
    "X = np.array([[2.0],\n",
    "              [1.5],\n",
    "              [-1.0]])   # Shape: (3, 1)\n",
    "W = np.array([[-0.5]])    # Shape: (1, 1)\n",
    "b = 1.0\n",
    "Y = np.array([[0.8],\n",
    "              [0.5],\n",
    "              [-0.3]])  # Shape: (3, 1)\n",
    "```\n",
    "\n",
    "Write the **forward pass** (compute Z, A, L) and **backward pass** (compute dL_dW, dL_db) **using only NumPy**, without autograd or loops. Use the same tanh activation and MSE loss (mean-reduced over batch).\n",
    "\n",
    "Do **not** encapsulate in functions‚Äîkeep it procedural, like your scalar version.\n",
    "\n",
    "Before you write code:\n",
    "\n",
    "```{attention}\n",
    "*What are the expected shapes of Z, A, dL_dZ, dL_dW, and dL_db?*\n",
    "```\n",
    "\n",
    "```{tip} Answer\n",
    ":class: dropdown\n",
    ":open: false\n",
    "\n",
    "Let‚Äôs recompute **dimensionally**, step by step.\n",
    "\n",
    "Given:\n",
    "- $X$: (3, 1) ‚Äî 3 samples, 1 feature  \n",
    "- $W$: (1, 1) ‚Äî weight matrix (input dim ‚Üí output dim)  \n",
    "- $b$: scalar (broadcasted)\n",
    "\n",
    "**Forward**:\n",
    "- $Z = X @ W + b \\rightarrow (3,1) @ (1,1) = (3,1) \\rightarrow Z \\in \\mathbb R^{3 \\times 1}$ \n",
    "- $A = tanh(Z) \\rightarrow \\text {same shape as Z} \\rightarrow A \\in \\mathbb R^{3 \\times 1}$\n",
    "- $Y \\in \\mathbb R^{3 \\times 1}$\n",
    "\n",
    "**Loss**:\n",
    "- Elementwise error: $(A ‚àí Y) \\rightarrow (3,1)$  \n",
    "- Squared error: $(A ‚àí Y)^2 \\rightarrow (3,1)$  \n",
    "- Mean over batch: $L = \\frac{1}{2B} \\sum_{i=1}^{B} (a_i - y_i)^2 \\rightarrow \\text {scalar}$  \n",
    "\n",
    "**Backward**:\n",
    "- $\\displaystyle \\frac{\\partial L}{\\partial A} = \\frac{A - Y}{B} \\rightarrow (3,1)$  \n",
    "  (Because $\\frac{\\partial}{\\partial A} [\\frac{1}{2} \\cdot \\text{mean}((A-Y)^2)] = \\frac{A-Y}{B}$)\n",
    "- $\\displaystyle \\frac{\\partial A}{\\partial Z} = 1 - A^2  \\rightarrow (3,1)$\n",
    "- $\\displaystyle \\frac{\\partial L}{\\partial Z} = \\frac{\\partial L}{\\partial A} \\odot \\frac{\\partial A}{\\partial Z}  \\rightarrow (3,1) \\odot (3,1) = (3,1)$\n",
    "\n",
    "Where $\\odot$ represents element-wise multiplication.\n",
    "\n",
    "Now, $\\displaystyle \\frac{\\partial L}{\\partial W}$:\n",
    "\n",
    "- By chain rule: $\\displaystyle \\frac{\\partial L}{\\partial W} = X^\\top \\cdot \\displaystyle \\frac{\\partial L}{\\partial Z}$\n",
    "- $X^\\top$: $(1,3)$, $\\displaystyle \\frac{\\partial L}{\\partial Z}$: $(3,1) \\rightarrow$ result: $(1,1)$\n",
    "\n",
    "Similarly, $\\displaystyle \\frac{\\partial L}{\\partial b} = \\text {sum}$ of $\\displaystyle \\frac{\\partial L}{\\partial Z}$ over batch ‚Üí scalar, but typically kept as $(1,)$ or scalar.\n",
    "\n",
    "So correct shapes:\n",
    "\n",
    "- $Z$: $(3,1)$\n",
    "- $A$: $(3,1)$\n",
    "- $\\displaystyle \\frac{\\partial L}{\\partial Z}$: $(3,1)$\n",
    "- $\\displaystyle \\frac{\\partial L}{\\partial W}$: $(1,1)$\n",
    "- $\\displaystyle\\frac{\\partial L}{\\partial b}$: scalar (or $(1,)$ if using array)\n",
    "\n",
    "Your claim that $\\displaystyle \\frac{\\partial L}{\\partial Z}$ is $(1,)$ is incorrect. The gradient w.r.t. $Z$ must have the same shape as $Z$ ‚Äî it's an elementwise derivative.\n",
    "\n",
    "This matters for CUDA: $(3,1)$ implies 3 independent activations, each contributing to the gradient. Mis-shaping breaks vectorization and leads to incorrect memory access patterns on GPU.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb25ad5-d166-4a92-8114-b16c4bafe9b1",
   "metadata": {},
   "source": [
    "# DRAFT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cc9767-cfae-48eb-b24c-edf68709e437",
   "metadata": {},
   "source": [
    "Now, back to the exercise:  \n",
    "Given the shape correction, **write the full NumPy code** for forward and backward passes with the provided X, W, b, Y.\n",
    "\n",
    "Keep it procedural. Compute:\n",
    "- Z, A, L\n",
    "- dL_dA, dL_dZ\n",
    "- dL_dW, dL_db\n",
    "\n",
    "Use **matrix operations only** (no loops)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48c94ae-b38f-4950-a1b4-503295793cf9",
   "metadata": {},
   "source": [
    "Consider a **single-layer network** with:\n",
    "- Input batch $\\mathbf{X} \\in \\mathbb{R}^{B \\times d}$ (B = batch size, d = input dim),\n",
    "- Weight matrix $\\mathbf{W} \\in \\mathbb{R}^{d_{\\text{out}} \\times d}$,\n",
    "- Bias $\\mathbf{b} \\in \\mathbb{R}^{d_{\\text{out}}}$,\n",
    "- Output pre-activations $\\mathbf{Z} = \\mathbf{X} \\mathbf{W}^\\top + \\mathbf{1}_B \\mathbf{b}^\\top$\n",
    "\n",
    "Assume a **single output neuron** first ($d_{\\text{out}} = 1$), so $\\mathbf{W} \\in \\mathbb{R}^{1 \\times d}$, $\\mathbf{b} \\in \\mathbb{R}$.\n",
    "\n",
    "Given a loss $L = \\frac{1}{B} \\sum_{i=1}^B \\frac{1}{2}(a^{(i)} - y^{(i)})^2$, what is the **gradient of $L$ with respect to $\\mathbf{W}$** in matrix form?\n",
    "\n",
    "Hint:  \n",
    "- From the scalar case, $\\frac{\\partial L^{(i)}}{\\partial \\mathbf{w}} = \\frac{\\partial L^{(i)}}{\\partial z^{(i)}} \\cdot \\mathbf{x}^{(i)}$\n",
    "- The full gradient is the average over the batch.\n",
    "\n",
    "Express $\\frac{\\partial L}{\\partial \\mathbf{W}}$ using matrix operations (e.g., outer product, matrix multiplication).\n",
    "\n",
    "What is the formula?\n",
    "\n",
    "Now, answer this:  \n",
    "**If we had a batch of 32 inputs, how would the gradient computation for $\\mathbf{W}$ and $\\mathbf{b}$ change in structure?**  \n",
    "\n",
    "Be specific:  \n",
    "- Would you compute gradients per sample then average?  \n",
    "- How does this relate to matrix multiplication in the backward pass?  \n",
    "- What is the shape of $\\frac{\\partial L}{\\partial \\mathbf{W}}$ if $\\mathbf{X} \\in \\mathbb{R}^{32 \\times d}$ and $\\mathbf{W} \\in \\mathbb{R}^{1 \\times d}$?  \n",
    "\n",
    "Explain your reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cde6bc2-8f1e-4838-a97b-5a3617541363",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8822805b-5aff-4ecf-9e78-246c196fadb0",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "slm_from_scratch",
   "language": "python",
   "name": "slm_from_scratch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
