{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "420552a9-f93a-415d-a060-e95ffb759621",
   "metadata": {},
   "source": [
    "*\"The real understanding comes when we get our hands dirty and build these things\"* - says my first AI mentor Richard Feynman. \n",
    "\n",
    "The goal of this course is a preparation for AI-backend optimization (e.g., CUDA, tensor cores, memory hierarchy tuning).\n",
    "\n",
    "I use my own mentor prompt generated with the help from my another prompt [\"mentor_generator v0.24.3\"](https://github.com/lefthand67/mentor_generator) to learn LLMs under the hood by building it from scratch. The idea stems from the [Stanford CS336 course](https://www.youtube.com/playlist?list=PLoROMvodv4rOY23Y0BoGoBGgQ1zmU_MT_) but this course is interactive though it needs hallucination checks.\n",
    "\n",
    "This is already the third mentor I regenerated for the course. I run my current mentor in free Qwen3-Max chat in no reasoning mode, previous ones were in Deepseek.\n",
    "\n",
    "***\n",
    "\n",
    "*\"What I cannot create, I do not understand.\"* - Richard Feynman\n",
    "\n",
    "Hello, and welcome. We are beginning **“Building a Small LLM from First Principles”**, a depth-first course tailored to your profile:  \n",
    "- You are a strong intermediate practitioner (NumPy backprop, CNNs, DL libraries).  \n",
    "- Your goal is professional readiness as an **AI Architect**, with emphasis on **CUDA-aware design** and **bare-metal understanding**.  \n",
    "- You operate in two environments: a high-VRAM GPU (RTX 4090ti, 16 GB) and a CPU-only Debian 13 system—so **VRAM and latency constraints** are central to every design decision.\n",
    "\n",
    "### Entire Learning Plan\n",
    "\n",
    "The entire curriculum is structured into five phases, designed to take you from the fundamental neuron to a fully functional, efficient 100M parameter LLM. Here is the roadmap.\n",
    "\n",
    "| Phase | Focus | Hands-On | Estimated Time (1h/day) |\n",
    "|------|--------|----------|--------------------------|\n",
    "| **Phase 1** | Foundational Neurons & Backprop | NumPy: single neuron, activation, gradient descent | 5–7 days |\n",
    "| **Phase 2** | Core Transformer Components | PyTorch: tokenization, positional encoding, attention | 10–14 days |\n",
    "| **Phase 3** | Optimization & Architecture | KV Cache, Flash Attention, LayerNorm, AdamW | 10–12 days |\n",
    "| **Phase 4** | Training & Alignment | Dataset prep, training loop, SFT, basic PPO/DPO | 12–15 days |\n",
    "| **Phase 5** | Architectural Review & Deployment | Quantization (INT8), CPU inference, full system audit | 7–10 days |\n",
    "\n",
    "*Total estimated commitment: **~50–60 hours**, paced by **mastery**, not calendar.*\n",
    "\n",
    "### Goals & Practical Skills\n",
    "\n",
    "By the end, you will:\n",
    "- **Architect** a 100M-parameter LLM from scratch.\n",
    "- **Explain** every component down to memory layout and gradient flow.\n",
    "- **Optimize** for both VRAM (4090ti) and CPU (Debian 13) constraints.\n",
    "- **Defend** your design choices in an AI Architect interview with mathematical and hardware-aware reasoning.\n",
    "\n",
    "### Administrative Notes\n",
    "\n",
    "- Each session is **~1 hour**.\n",
    "- **No deadlines**—progress is gated solely by verified understanding.\n",
    "- You must **demonstrate reasoning**, not just recall. Passive acknowledgment (“I get it”) is insufficient.\n",
    "- We use **one small step at a time**, with micro-validation.\n",
    "\n",
    "## Can I run this course on the weaker laptop?\n",
    "\n",
    "**Hardware Requirements Analysis:**\n",
    "\n",
    "| Component | RTX 4090ti | Weaker Laptop | Impact |\n",
    "|-----------|------------|---------------|---------|\n",
    "| **VRAM** | 16GB | Shared system RAM (8GB total) | Limits model size and batch size |\n",
    "| **Compute** | High parallel throughput | Limited CPU cores | Slower training times |\n",
    "| **Memory Bandwidth** | ~1 TB/s | ~50 GB/s | Significant bottleneck for matrix ops |\n",
    "| **Practical Limits** | 100M param model feasible | ~10M param model maximum | Scales final project scope |\n",
    "\n",
    "If you proceed with the weaker laptop, we'll need to adjust:\n",
    "\n",
    "1. **Phase 1-3**: Completely feasible - these are algorithmic implementations\n",
    "2. **Phase 4**: We'll target a smaller model (~10M parameters vs 100M)\n",
    "3. **Training Time**: Expect 10-50x slower execution\n",
    "4. **Batch Size**: Will need to be minimal (possibly 1)\n",
    "\n",
    "The core concepts of computational graphs, backpropagation, and transformer mechanics remain identical. The implementation principles for CUDA optimization still apply - we just won't be able to run the full-scale final project.\n",
    "\n",
    "The mathematical understanding and architectural insights are what matter for your CUDA preparation, not the absolute scale.\n",
    "\n",
    "**Hardware Selection Rule:**\n",
    "\n",
    "Use **only the Lenovo Legion 7 Pro (with RTX 4090ti)** for all active development, experimentation, and implementation during the course.\n",
    "\n",
    "The **Dell (Debian 13, CPU-only)** is reserved for **Phase 5**, when we conduct deployment analysis, latency profiling, and efficiency testing under CPU-only constraints. Introducing it earlier would add unnecessary overhead and distract from the core goal: building and understanding the LLM on a capable system first.\n",
    "\n",
    "Therefore:  \n",
    "- **Phases 1–4**: Lenovo only (GPU-enabled, Fedora 42).  \n",
    "- **Phase 5**: Both systems—Lenovo for reference, Dell for efficiency validation.\n",
    "\n",
    "This aligns with your dual-environment goal while maintaining focus on deep learning fundamentals during the build phase."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
