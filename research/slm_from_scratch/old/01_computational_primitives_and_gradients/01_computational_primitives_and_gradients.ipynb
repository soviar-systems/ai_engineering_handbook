{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d42fe633-0869-4f6f-aef8-f4a27537d71f",
   "metadata": {},
   "source": [
    "# Phase 1: Computational Primitives and Gradients "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450bcae6-ebbd-443c-b36b-8c71961f3dd4",
   "metadata": {},
   "source": [
    "# **1. The Computational Graph**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c23997c-b166-4d52-8780-693ab569852c",
   "metadata": {},
   "source": [
    "We'll build auto-differentiation from scratch, similar to the micrograd approach. The core concept is representing mathematical operations as a directed acyclic graph (DAG) where nodes are values and edges are operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773f58d8-be6c-4b5d-84ea-c1757647ad16",
   "metadata": {},
   "source": [
    "## Computational Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45db17a8-12ce-4cdb-bc68-072329f3c818",
   "metadata": {},
   "source": [
    "Micrograd is a minimal autograd engine - a tiny library that implements automatic differentiation (backpropagation) from scratch. It's about 100 lines of code that demonstrates the core mechanics behind PyTorch's autograd system.\n",
    "\n",
    "**Core Concept: Computational Graphs**\n",
    "\n",
    "Think of every mathematical operation as building a graph:\n",
    "\n",
    "Simple expression: $y = (a \\times b) + c$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caaf9833-4301-4ea6-b52e-12be5c93c35d",
   "metadata": {},
   "source": [
    "```{mermaid}\n",
    "graph LR\n",
    "    a --> mul\n",
    "    b --> mul\n",
    "    c --> add\n",
    "    mul --> add\n",
    "    add --> y\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79765fb2-f53c-4a0c-a66e-ebab2ec85f2c",
   "metadata": {},
   "source": [
    "Each node in this graph:\n",
    "- Stores its current numerical value\n",
    "- Knows how to compute its gradient during backward pass\n",
    "- Remembers which operation created it and what its inputs were\n",
    "\n",
    "Even though you'll use PyTorch later, building micrograd teaches you:\n",
    "1. How backpropagation actually works under the hood\n",
    "2. The chain rule in practice\n",
    "3. What PyTorch tensors are doing automatically\n",
    "4. Foundation for CUDA optimization (understanding the computation flow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ad0929-5024-438f-bd58-878d99a0c2e8",
   "metadata": {},
   "source": [
    "## Chain Rule Application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524a1098-57e8-490a-bae5-e4572e8f68bb",
   "metadata": {},
   "source": [
    "Let's say we have:\n",
    "- `a = 2.0`, `b = 3.0`, `c = 1.0`\n",
    "- `y = (a * b) + c = (2 * 3) + 1 = 7`\n",
    "\n",
    "During backward pass, we compute how `y` changes when we tweak `a`, `b`, or `c`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e1319b-7eff-431a-9e2a-44e50231db73",
   "metadata": {},
   "source": [
    "For the expression:\n",
    "\n",
    "$$\n",
    "y = (a \\times b) + c\n",
    "$$\n",
    "\n",
    "We can compute the gradients (partial derivatives) step by step:\n",
    "\n",
    "**Forward Pass:**\n",
    "1. $mult = a \\times b$\n",
    "2. $y = mult + c$\n",
    "\n",
    "**Backward Pass (Chain Rule):**\n",
    "\n",
    "Starting from the output $y$:\n",
    "- $\\displaystyle \\frac {\\partial y}{\\partial y} = 1.0$ (gradient of output with respect to itself)\n",
    "\n",
    "Now, for the addition operation $y = mult + c$:\n",
    "- $\\displaystyle \\frac {\\partial y}{ \\partial mult} = 1.0$ (derivative of addition w.r.t first operand)\n",
    "- $\\displaystyle \\frac {\\partial y}{ \\partial c} = 1.0$ (derivative of addition w.r.t second operand)\n",
    "\n",
    "For the multiplication operation $mult = a \\times b$:\n",
    "- $\\displaystyle \\frac {\\partial mult}{ \\partial a} = b$ (derivative of multiplication w.r.t first operand)\n",
    "- $\\displaystyle \\frac {\\partial mult}{ \\partial b} = a$ (derivative of multiplication w.r.t second operand)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ab3fa5-66e0-4d03-a375-9736034d4b6c",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "graph LR\n",
    "    y --> add\n",
    "    add --> mul\n",
    "    add --> c\n",
    "    mul --> b\n",
    "    mul --> a\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48c4208-0dfb-4603-9896-72a0f30016a4",
   "metadata": {},
   "source": [
    "**Applying Chain Rule:**\n",
    "\n",
    "- ∂y/∂a = (∂y/∂mult) * (∂mult/∂a) = 1.0 * b = b\n",
    "- ∂y/∂b = (∂y/∂mult) * (∂mult/∂b) = 1.0 * a = a  \n",
    "- ∂y/∂c = 1.0\n",
    "\n",
    "So with our values $a=2.0, b=3.0, c=1.0$:\n",
    "- ∂y/∂a = 3.0\n",
    "- ∂y/∂b = 2.0\n",
    "- ∂y/∂c = 1.0\n",
    "\n",
    "This means if we increase $a$ by a small amount $\\epsilon$, $y$ increases by approximately $3 \\epsilon$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff87cfa-0642-4ec2-8566-0a9074423e1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "C",
   "language": "c",
   "name": "c"
  },
  "language_info": {
   "file_extension": ".c",
   "mimetype": "text/plain",
   "name": "c"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
