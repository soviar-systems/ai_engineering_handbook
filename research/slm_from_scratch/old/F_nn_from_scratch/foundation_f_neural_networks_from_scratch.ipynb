{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfa725d5-2957-4536-8ec8-82ed546d6b03",
   "metadata": {},
   "source": [
    "# FOUNDATION F: NEURAL NETWORKS FROM SCRATCH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca004a6-7e46-40ac-b5a6-761a330dfa83",
   "metadata": {},
   "source": [
    "**The Core Idea**: A neural network is just a mathematical function that can be represented as a computational graph. The \"learning\" happens by adjusting the parameters of this function to minimize some error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360aeb4d-f902-440a-b577-8321841e1165",
   "metadata": {},
   "source": [
    "# <b>1. Forward Pass</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b34ae0-059b-4d9e-9f08-a5e446df7738",
   "metadata": {},
   "source": [
    "Think about a single neuron - the simplest building block. If you were to implement this from scratch in NumPy (no PyTorch yet), what would be the minimal components you'd need?\n",
    "\n",
    "Consider:\n",
    "- What inputs does it take?\n",
    "- What parameters does it have? \n",
    "- What computation does it perform?\n",
    "- What output does it produce?\n",
    "\n",
    "1. **Linear combination**: $y = w_1x_1 + w_2x_2 + \\ldots + w_nx_n + b$\n",
    "2. **Weights initialization**: Random values\n",
    "3. **Activation function**: Breaks linearity\n",
    "4. **Matrix operations**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3792ba-fda5-4d5d-bc6b-bab4ae81910f",
   "metadata": {},
   "source": [
    "# 1.1 The Linear Algebra Gap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e7f8ef-17c0-4c30-abe3-5a31ec3f4489",
   "metadata": {},
   "source": [
    "Think about processing one input vector vs many inputs:\n",
    "\n",
    "**Single input**: $[x_1, x_2, x_3]$ with weights $[\\theta_1, \\theta_2, \\theta_3]$  \n",
    "$output = x_1\\theta_1 + x_2\\theta_2 + x_3\\theta_3 + b$\n",
    "\n",
    "**Multiple inputs (as matrix)**: \n",
    "```\n",
    "Inputs: [ [xâ‚â‚, xâ‚â‚‚, xâ‚â‚ƒ],   Weights: [ðœƒâ‚, ðœƒâ‚‚, ðœƒâ‚ƒ]áµ€\n",
    "          [xâ‚‚â‚, xâ‚‚â‚‚, xâ‚‚â‚ƒ],\n",
    "          [xâ‚ƒâ‚, xâ‚ƒâ‚‚, xâ‚ƒâ‚ƒ] ]\n",
    "```\n",
    "\n",
    "What linear algebra operation would efficiently compute all outputs at once? Matrix multiplication."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b372b9-f12c-40b2-b2ec-8a53e3f8cfd5",
   "metadata": {},
   "source": [
    "# 1.2 Single neuron code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c28bba8-a3a1-4c24-a25b-04a971ea77dd",
   "metadata": {},
   "source": [
    "Try implementing the **single input case** first:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self, input_size):\n",
    "        # Initialize weights and bias here\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Compute y = wÂ·x + b\n",
    "        # Then apply activation function\n",
    "        pass\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1772379a-5a41-4cdc-856b-6c0e9e7dfd6d",
   "metadata": {},
   "source": [
    "# 1.3 Activation Function Options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa2b2f0-d6ad-45c5-a8be-8861abf56c4b",
   "metadata": {},
   "source": [
    "| Function | Formula | Range | Key Properties |\n",
    "|----------|---------|-------|----------------|\n",
    "| **Sigmoid** | 1/(1+eâ»Ë£) | (0,1) | Smooth, bounded, but can saturate (vanishing gradients) |\n",
    "| **Tanh** | (eË£-eâ»Ë£)/(eË£+eâ»Ë£) | (-1,1) | Zero-centered, but still can saturate |\n",
    "| **ReLU** | max(0,x) | [0,âˆž) | Simple, avoids saturation, but \"dying ReLU\" problem |\n",
    "| **Leaky ReLU** | max(0.01x,x) | (-âˆž,âˆž) | Fixes dying ReLU, small gradient for negatives |\n",
    "\n",
    "**Historical Context & Modern Practice**\n",
    "\n",
    "- **1980s-2000s**: Sigmoid/tanh were dominant (biological plausibility)\n",
    "- **2010s**: ReLU became standard for hidden layers (training speed)\n",
    "- **Today**: Variants like Leaky ReLU, GELU are common\n",
    "\n",
    "For your **first from-scratch implementation**, which activation function would you choose and **why**? Consider:\n",
    "\n",
    "1. **Simplicity of implementation**\n",
    "2. **Gradient behavior** (important for backpropagation)\n",
    "3. **Training stability**\n",
    "\n",
    "\n",
    "\n",
    "Leaky ReLU is actually a very smart pick for your first implementation.\n",
    "\n",
    "**Why Leaky ReLU is a Great Choice**\n",
    "\n",
    "1. **Simple to implement** - just `max(0.01*x, x)` \n",
    "2. **Avoids the \"dying neuron\" problem** of regular ReLU\n",
    "3. **Computationally efficient** - no expensive exponentials like sigmoid/tanh\n",
    "4. **Good gradient flow** - never completely zero gradient\n",
    "\n",
    "**Your Implementation Challenge**\n",
    "\n",
    "Now, here's what I want you to think about for the implementation:\n",
    "\n",
    "```python\n",
    "def leaky_relu(x, alpha=0.01):\n",
    "    # Your implementation here\n",
    "    pass\n",
    "```\n",
    "\n",
    "**Key consideration**: Should this be a method of your `Neuron` class, or a separate function? What are the trade-offs?\n",
    "\n",
    "Also think about: How will this affect your backpropagation implementation later? (We'll get to that soon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2423ddfb-c9b4-4254-aa5e-c9c52f46d4c9",
   "metadata": {},
   "source": [
    "Putting the activation function in the forward pass is exactly right.\n",
    "\n",
    "**Your Architectural Decision Makes Sense**\n",
    "\n",
    "Having the activation as part of the neuron's forward pass:\n",
    "- Keeps the neuron self-contained\n",
    "- Makes the computation clear: `output = activation(linear_combination)`\n",
    "- Will make backpropagation more straightforward (when we get there)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23676f4f-e2cb-42a1-bac6-c223ec807387",
   "metadata": {},
   "source": [
    "# 1.4 Code implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c44b2369-d4ca-424d-a998-e8b79d9bef41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T10:02:08.298334Z",
     "iopub.status.busy": "2025-11-09T10:02:08.294394Z",
     "iopub.status.idle": "2025-11-09T10:02:08.428169Z",
     "shell.execute_reply": "2025-11-09T10:02:08.426235Z",
     "shell.execute_reply.started": "2025-11-09T10:02:08.298028Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Neuron:\n",
    "    \"\"\"Implement single neuron class.\"\"\"\n",
    "\n",
    "    def __init__(self, input_size: tuple) -> None:\n",
    "        \"\"\"Initialize neuron entity.\"\"\"\n",
    "        self.input_size = input_size\n",
    "        self.w = np.random.default_rng().random(self.input_size[-1])\n",
    "        self.b = np.random.default_rng().random()\n",
    "        # activation function hyperparameter\n",
    "        self.alpha = 0.01\n",
    "\n",
    "    def leaky_relu(self, vector: np.array) -> np.array:\n",
    "        \"\"\"\n",
    "        Break linearity.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        vector : np.array\n",
    "            y_pred before activation function applied.\n",
    "\n",
    "        \"\"\"\n",
    "        return np.maximum(self.alpha * vector, vector)\n",
    "\n",
    "    def _get_linear_transformation(self, x: np.array) -> np.array:\n",
    "        \"\"\"\n",
    "        Make the matrix multiplication of x and weights.\n",
    "\n",
    "        The result is y_pred before activation function.\n",
    "        \"\"\"\n",
    "        return np.dot(x, self.w) + self.b\n",
    "\n",
    "    def forward(self, x: np.array) -> np.array:\n",
    "        \"\"\"Calculate forward pass with activation function.\"\"\"\n",
    "        vector = self._get_linear_transformation(x)\n",
    "        return self.leaky_relu(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "44f62454-e623-4b52-bedf-3e09d403e38b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T10:02:48.032181Z",
     "iopub.status.busy": "2025-11-09T10:02:48.031252Z",
     "iopub.status.idle": "2025-11-09T10:02:48.053346Z",
     "shell.execute_reply": "2025-11-09T10:02:48.051971Z",
     "shell.execute_reply.started": "2025-11-09T10:02:48.032041Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_neuron.input_size: (4, 5)\n",
      "my_neuron.w: [0.96963449 0.88127309 0.29762979 0.32335681 0.75624471]\n",
      "linear transformation: [19.55678708 20.20853882 31.18914713 -5.57078139]\n",
      "my_neuron.forward(X): [19.55678708 20.20853882 31.18914713 -0.05570781]\n"
     ]
    }
   ],
   "source": [
    "X = np.array(\n",
    "    [\n",
    "        [-1, 2, 3, 13, 17],\n",
    "        [4, -5, 6, 14, 18],\n",
    "        [7, 8, -9, 15, 19],\n",
    "        [-10, 11, 12, 16, -20],\n",
    "    ],\n",
    "    dtype=np.float64,\n",
    ")\n",
    "\n",
    "my_neuron = Neuron((X.shape))\n",
    "print(\"my_neuron.input_size:\", my_neuron.input_size)\n",
    "print(\"my_neuron.w:\", my_neuron.w)\n",
    "print(\"linear transformation:\", my_neuron._get_linear_transformation(X))\n",
    "print(\"my_neuron.forward(X):\", my_neuron.forward(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932a9a08-5393-46e1-b02c-28cd181b0b5a",
   "metadata": {},
   "source": [
    "## Weight Initialization Bug"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad434ff-4cb8-45c5-a559-331ea3ef1eb6",
   "metadata": {},
   "source": [
    "Your reasoning is correct about avoiding zeros, but the implementation has scaling issues:\n",
    "\n",
    "```python\n",
    "# Current: weights in [0.001, 1.001)\n",
    "self.w = np.random.default_rng().random(input_size) + 0.001\n",
    "\n",
    "# Problem: Large weights cause large pre-activations â†’ gradient explosion\n",
    "# Better: Xavier/Glorot initialization for linear layers\n",
    "self.w = (np.random.default_rng().random(input_size) - 0.5) * np.sqrt(2.0 / input_size)\n",
    "```\n",
    "\n",
    "The `+ 0.001` only solves the \"dead neuron\" problem for ReLU, but creates gradient instability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dee5c3f-cab5-40ad-bf1f-5f65fa3f143e",
   "metadata": {},
   "source": [
    "## Type Hint Best Practices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66930354-caa6-4d5d-afbd-439ee3140989",
   "metadata": {},
   "source": [
    "For deep learning systems:\n",
    "\n",
    "```python\n",
    "# Standard practice:\n",
    "import numpy.typing as npt\n",
    "\n",
    "class Neuron:\n",
    "    def forward(\n",
    "        self,\n",
    "        X: npt.NDArray[np.float32],  # Consistent precision\n",
    "        activation_f: str = \"leaky_relu\",\n",
    "    ) -> npt.NDArray[np.float32]:\n",
    "        \n",
    "    # Use float32 throughout for GPU compatibility\n",
    "    self.w: npt.NDArray[np.float32] = None  \n",
    "    self.b: np.float32 = np.float32(0.0)\n",
    "    self.lr: np.float32 = np.float32(0.01)\n",
    "```\n",
    "\n",
    "**Rationale:** GPUs are optimized for float32, and mixed precision causes unnecessary casting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d41117-3ac6-4dbd-9c39-8d6d4ad7f615",
   "metadata": {},
   "source": [
    "# <b>2. Gradient derivation</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00178c04-038b-44b4-991d-c2be8aabaccf",
   "metadata": {},
   "source": [
    "Your single neuron implementation is solid. The next critical concept is: **how does learning actually happen?**\n",
    "\n",
    "We have this neuron that can compute outputs, but how do we adjust `self.w` and `self.b` to make it produce better outputs?\n",
    "\n",
    "What's your current understanding of how the \"learning\" process works in neural networks?\n",
    "\n",
    "You've identified the three key components:\n",
    "\n",
    "1. **Error/Loss**\n",
    "2. **Gradient**\n",
    "3. **Weight Update**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a68fe6d-16c5-4c20-9d18-3336c2defe09",
   "metadata": {},
   "source": [
    "# 2.1 The Missing Piece: Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d65338f-3f5c-4ee2-b33a-e3778f2bc80c",
   "metadata": {},
   "source": [
    "What you described is **gradient descent**. The magic that makes it work for neural networks is **backpropagation** - the algorithm for efficiently computing those gradients through the entire computational graph.\n",
    "\n",
    "Think of it this way: If your neuron's computation is:\n",
    "```\n",
    "input â†’ linear â†’ activation â†’ output\n",
    "```\n",
    "\n",
    "Backpropagation answers:\n",
    "\n",
    "> \"*How much did each weight contribute to the final error?*\"\n",
    "\n",
    "Let's break this down into manageable steps. If you were to implement gradient calculation for your single neuron, what would be the minimal information you'd need?\n",
    "\n",
    "Consider:\n",
    "- The error at the output\n",
    "- How the activation function affects the gradient\n",
    "- How the linear combination affects the gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7339eab2-5b5b-4131-b214-d6326450b767",
   "metadata": {},
   "source": [
    "## The Error Signal Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43aa634e-af90-46e6-bfe4-6ee84cad1d4f",
   "metadata": {},
   "source": [
    "The error signal begins with the difference between predicted and true values.\n",
    "\n",
    "You're correct: `error = true_y - predicted_y` (or some function of this difference)\n",
    "\n",
    "Now here's the key insight for backpropagation: **We need to trace how this error flows backward through each operation.**\n",
    "\n",
    "Think about your neuron's forward pass:\n",
    "```\n",
    "x â†’ linear (wÂ·x + b) â†’ leaky_relu â†’ y_pred\n",
    "```\n",
    "\n",
    "For backpropagation, we go backward:\n",
    "```\n",
    "error â† d_leaky_relu â† d_linear â† gradients for w and b\n",
    "```\n",
    "\n",
    "If you were to implement this gradient calculation manually for a single input, what operations would you need to compute?\n",
    "\n",
    "Consider:\n",
    "1. Gradient of the error with respect to the output\n",
    "2. How the activation function transforms this gradient\n",
    "3. How the linear operation distributes gradients to weights and bias\n",
    "\n",
    "**What's your thinking about the mathematical operations involved?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552ba838-8872-4371-a8ba-717b562a4232",
   "metadata": {},
   "source": [
    "## Chain Rule Intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa94c36f-224e-4864-a266-0f0716d11604",
   "metadata": {},
   "source": [
    "Think about your neuron's computation:\n",
    "```\n",
    "output = leaky_relu(wÂ·x + b)\n",
    "error = some_cost_function()\n",
    "```\n",
    "\n",
    "If we want to know how much to change $\\omega_1$, we need to answer: \"How does changing $\\omega_1$ affect the final error?\"\n",
    "\n",
    "This is where the **chain rule** (Ð¿Ñ€Ð¾Ð¸Ð·Ð²Ð¾Ð´Ð½Ð°Ñ ÑÐ»Ð¾Ð¶Ð½Ð¾Ð¹ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¸) from calculus comes in. We break the problem into steps:\n",
    "\n",
    "1. How does error change with output?\n",
    "2. How does output change with activation input?\n",
    "3. How does activation input change with $\\omega_1$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef07f8b-494a-4246-8698-c2631fa79d92",
   "metadata": {},
   "source": [
    "You've done exactly the right preparation. Re-reading the math and connecting it to CS229 shows excellent learning discipline.\n",
    "\n",
    "Your understanding is spot on:\n",
    "\n",
    "- $J(\\theta)$ is our cost function (like MSE)\n",
    "- **Partial derivatives** give us the gradient $\\nabla J(\\theta)$\n",
    "- **Gradient descent** uses this to update weights: $\\theta = \\theta - \\alpha\\nabla J(\\theta)$\n",
    "\n",
    "You asked: *\"Do we really need to work with the error itself?\"* The answer reveals the elegance of backpropagation:\n",
    "\n",
    "> We don't actually need the explicit mathematical formula for $J(\\theta)$.\n",
    "\n",
    "Instead, we use the **chain rule** to compute gradients through the **computational graph**.\n",
    "\n",
    "Think about your neuron:\n",
    "```\n",
    "x â†’ z = ðœƒÂ·x + b â†’ a = leaky_relu(z) â†’ J = cost_function(a, y_true)\n",
    "```\n",
    "\n",
    "where `a` is `y_pred`.\n",
    "\n",
    "To find $\\displaystyle \\frac {\\partial}{\\partial \\theta}J(\\theta)$, we can compute:\n",
    "\n",
    "$$\n",
    "\\frac {\\partial}{\\partial \\theta}J(\\theta) = \\frac {\\partial J(\\theta)}{\\partial a} \\times \\frac {\\partial a}{\\partial z} \\times \\frac {\\partial z}{\\partial \\theta}\n",
    "$$\n",
    "\n",
    "**Your implementation challenge**: If you were to compute these partial derivatives numerically for a single example, what would be your step-by-step approach?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c4d1ba-e9d5-4c4a-83cf-74c5a0b253fa",
   "metadata": {},
   "source": [
    "## Cost function `J`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814731f2-9fb9-4114-bb92-9cf6dd55a32a",
   "metadata": {},
   "source": [
    "Let's define the cost function we're going to use. Modern frameworks such as [PyTorch](https://www.digitalocean.com/community/tutorials/pytorch-loss-functions) and TensorFlow define **Mean Squared Error (MSE)** with mean reduction by default.\n",
    "\n",
    "This keeps the loss and gradients **batch-size invariant**, so that hyperparameters (like learning rate) can be reused independently of the dataset or mini-batch size.\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{m} \\cdot \\frac{1}{2} \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})^2\n",
    "$$\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)}) \\cdot x^{(i)}\n",
    "$$\n",
    "\n",
    "**Why average matters:**\n",
    "- Without `/m`, your gradients grow linearly with batch size\n",
    "- Learning rate becomes batch-size dependent\n",
    "- Large batches cause instability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b650e8e-6998-4e20-81b5-499723d6c61b",
   "metadata": {},
   "source": [
    "For one example it is going to be:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac {\\partial}{\\partial \\theta_j} J(\\theta) & = \\frac {\\partial}{\\partial \\theta_j} \\frac{1}{2} \\left(h_{\\theta}(x) - y\\right)^2 = \\\\\n",
    "& = 2 \\frac {1}{2} (h_{\\theta}(x) - y) \\cdot \\frac {\\partial}{\\partial \\theta_j}(h_{\\theta}(x) - y) = \\\\\n",
    "& = (h_{\\theta}(x) - y) \\cdot \\frac {\\partial}{\\partial \\theta_j} (\\theta_0 x_0 + \\theta_1 x_1 + \\cdots + \\theta_j x_j \\cdots + \\theta_n x_n -y) \\\\\n",
    "& = (h_{\\theta}(x) - y) \\cdot x_j\n",
    "\\end{align*}\n",
    "\n",
    "You've derived the gradient for linear regression perfectly! \n",
    "\n",
    "$$\n",
    "\\frac {\\partial}{\\partial \\theta_j} J(\\theta) = (h_{\\theta}(x) - y) \\cdot x_j\n",
    "$$\n",
    "\n",
    "The gradient for each weight is proportional to:\n",
    "1. The **error signal** (prediction - true value)\n",
    "2. The **input** that weight connects to."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a3763b-f9c6-43d1-975c-8512c17a502f",
   "metadata": {},
   "source": [
    "## Activation function `a`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b51584f-0f8f-43c7-84bf-ecb9b19b0685",
   "metadata": {},
   "source": [
    "Your neuron has one extra step: the activation function. So the chain becomes:\n",
    "\n",
    "For weight $\\theta_j$:\n",
    "\n",
    "$$\n",
    "\\frac {\\partial}{\\partial \\theta_j}J(\\theta) = \\frac {\\partial J(\\theta)}{\\partial a} \\times \\frac {\\partial a}{\\partial z} \\times \\frac {\\partial z}{\\partial \\theta_j}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $a = \\text {leaky relu}(z)$ = final output (`y_pred`)\n",
    "- $z = \\theta x + b$ = linear combination\n",
    "- $\\displaystyle \\frac {\\partial z}{\\partial \\theta_j} = x_j$ (as you correctly derived)\n",
    "\n",
    "Given your `leaky_relu` function:\n",
    "```python\n",
    "def leaky_relu(self, self.z, alpha=0.01):\n",
    "    return np.maximum(alpha * self.z, self.z)\n",
    "```\n",
    "\n",
    "What would be the derivative $\\displaystyle \\frac {\\partial a}{\\partial z}$? Think about how `leaky_relu` behaves differently for positive vs negative inputs.\n",
    "\n",
    "For `leaky_relu`:\n",
    "\n",
    "- if `z > 0`: $a = z$, so $\\displaystyle \\frac {\\partial a}{\\partial z} = 1$\n",
    "- if `z <= 0`: $a = \\alpha \\cdot z$, so $\\displaystyle \\frac {\\partial a}{\\partial z} = \\alpha$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621b100c-56d8-43a7-a016-074868b04d3c",
   "metadata": {},
   "source": [
    "# 2.2 Code implementaion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d104b663-3829-45ee-bb44-8abe76bda787",
   "metadata": {},
   "source": [
    "Now you have all the pieces for the gradient:\n",
    "\n",
    "$$\n",
    "\\frac {\\partial J}{\\partial \\theta_j} = (a - y_{true}) \\cdot [\\text {activation derivative}(z)] \\cdot x_j\n",
    "$$\n",
    "\n",
    "Where $[\\text {activation derivative}(z)]$ is $[1\\ \\text {or}\\ \\alpha]$ depends on whether $z$ was positive or negative.\n",
    "\n",
    "**Before you implement**: How would you handle computing this derivative efficiently in code, given that `z` might be a single value or a batch of values?\n",
    "\n",
    "How would you compute the gradient for all weights simultaneously, such that each weight $\\theta_j$ gets:\n",
    "\n",
    "$$\n",
    "\\text {gradient}[\\theta_j] = \\text {sum over examples} (\\ \\text {error} \\cdot \\text {derivative} \\cdot x_j\\ )\n",
    "$$\n",
    "\n",
    "What's your thinking about the vectorized operation needed here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2af52fee-c312-4835-9aff-ef16478da2a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T10:50:23.258706Z",
     "iopub.status.busy": "2025-11-09T10:50:23.258402Z",
     "iopub.status.idle": "2025-11-09T10:50:23.278528Z",
     "shell.execute_reply": "2025-11-09T10:50:23.277383Z",
     "shell.execute_reply.started": "2025-11-09T10:50:23.258680Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Neuron:\n",
    "    \"\"\"Implement single neuron class.\"\"\"\n",
    "\n",
    "    def __init__(self, input_size: tuple) -> None:\n",
    "        \"\"\"Initialize neuron entity.\"\"\"\n",
    "        self.input_size = input_size\n",
    "        self.w = np.random.default_rng().random(self.input_size[-1])\n",
    "        self.b = np.random.default_rng().random()\n",
    "        # activation function hyperparameter\n",
    "        self.alpha = 0.01\n",
    "\n",
    "    def leaky_relu(self, vector: np.array) -> np.array:\n",
    "        \"\"\"\n",
    "        Break linearity.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        vector : np.array\n",
    "            y_pred before activation function applied.\n",
    "\n",
    "        \"\"\"\n",
    "        return np.maximum(self.alpha * vector, vector)\n",
    "\n",
    "    def derivative_of_leaky_relu(self, vector: np.array) -> np.array:\n",
    "        \"\"\"\n",
    "        Calculate the derivative of the activation function.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        vector : np.array\n",
    "            y_pred before activation function applied.\n",
    "\n",
    "        \"\"\"\n",
    "        derivative = np.asarray(vector, copy=True)\n",
    "        return np.where(derivative < 0, self.alpha, 1)\n",
    "\n",
    "    def _get_linear_transformation(self, x: np.array) -> np.array:\n",
    "        \"\"\"\n",
    "        Make the matrix multiplication of x and weights.\n",
    "\n",
    "        The result is y_pred before activation function.\n",
    "        \"\"\"\n",
    "        return np.dot(x, self.w) + self.b\n",
    "\n",
    "    def forward(self, x: np.array) -> np.array:\n",
    "        \"\"\"Calculate forward pass with activation function.\"\"\"\n",
    "        vector = self._get_linear_transformation(x)\n",
    "        return self.leaky_relu(vector)\n",
    "\n",
    "    def gradient_of_J(self, y_true: np.array, x: np.array) -> np.array:\n",
    "        \"\"\"Compute the gradient after forward pass.\"\"\"\n",
    "        y_pred = self.forward(x)\n",
    "        error = y_pred - y_true\n",
    "\n",
    "        leaky_relu_derivative = self.derivative_of_leaky_relu(\n",
    "            self._get_linear_transformation(x)\n",
    "        )\n",
    "\n",
    "        return np.dot((error * leaky_relu_derivative), x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "89cde762-14af-446c-aa7a-e5cc739d5cfc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T10:51:33.543422Z",
     "iopub.status.busy": "2025-11-09T10:51:33.542927Z",
     "iopub.status.idle": "2025-11-09T10:51:33.556034Z",
     "shell.execute_reply": "2025-11-09T10:51:33.554779Z",
     "shell.execute_reply.started": "2025-11-09T10:51:33.543378Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_neuron.input_size: (4, 5)\n",
      "my_neuron.w: [0.92542368 0.63684431 0.63029109 0.48955112 0.96805626]\n",
      "linear transformation: [25.99848804 29.51617692 32.5746651  -5.27553477]\n",
      "my_neuron.forward(X): [25.99848804 29.51617692 32.5746651  -0.05275535]\n",
      "my_neuron.gradient_of_J(y_true=y, x=X): [-2439.9058491  -1478.99239077  1113.91420904 -7424.18164295\n",
      " -9327.80533079]\n"
     ]
    }
   ],
   "source": [
    "X = np.array(\n",
    "    [\n",
    "        [-1, 2, 3, 13, 17],\n",
    "        [4, -5, 6, 14, 18],\n",
    "        [7, 8, -9, 15, 19],\n",
    "        [-10, 11, 12, 16, -20],\n",
    "    ],\n",
    "    dtype=np.float64,\n",
    ")\n",
    "y = np.array([100, 200, 300, 400], dtype=np.float64)\n",
    "\n",
    "my_neuron = Neuron((X.shape))\n",
    "print(\"my_neuron.input_size:\", my_neuron.input_size)\n",
    "print(\"my_neuron.w:\", my_neuron.w)\n",
    "print(\"linear transformation:\", my_neuron._get_linear_transformation(X))\n",
    "print(\"my_neuron.forward(X):\", my_neuron.forward(X))\n",
    "print(\n",
    "    \"my_neuron.gradient_of_J(y_true=y, x=X):\",\n",
    "    my_neuron.gradient_of_J(y_true=y, x=X),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbdc592-d1be-4921-a312-189885686c98",
   "metadata": {},
   "source": [
    "I've carefully reviewed your code and this is a **massive improvement**. You've solved the key issues beautifully.\n",
    "\n",
    "**What You Fixed Perfectly âœ…**\n",
    "\n",
    "1. **Correct derivative application**: Using `_get_linear_transformation(x)` as input to the derivative\n",
    "2. **Proper vectorization**: `error * leaky_relu_derivative` element-wise multiplication\n",
    "3. **Efficient gradient computation**: `np.dot((error * leaky_relu_derivative), x)` is exactly right!\n",
    "\n",
    "**Your Solution is Mathematically Correct**\n",
    "\n",
    "Let's trace the dimensions:\n",
    "- `error`: shape `(4,)` \n",
    "- `leaky_relu_derivative`: shape `(4,)`\n",
    "- `error * leaky_relu_derivative`: shape `(4,)` (element-wise)\n",
    "- `x`: shape `(4, 5)`\n",
    "- `np.dot(..., x)`: shape `(5,)` - perfect! One gradient per weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b67693a-81cf-4b3b-b15a-c4552cea364b",
   "metadata": {},
   "source": [
    "## Missing Bias Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77210476-b56a-4975-8661-b86f76d2dd3c",
   "metadata": {},
   "source": [
    "**Problem**: Your `gradient_of_J` returns only `dJ_dðœƒ` but completely omits `dJ_db`!\n",
    "\n",
    "**Mathematical Reason**: \n",
    "- For bias: $\\displaystyle \\frac{\\partial z}{\\partial b} = 1$ where $z = \\theta \\cdot x + b$ (and it is always true because we always have $b^1 => 1$)\n",
    "- Therefore: $\\displaystyle \\frac{\\partial J}{\\partial b} = \\frac{\\partial J}{\\partial a} \\cdot \\frac{\\partial a}{\\partial z} \\cdot \\frac{\\partial z}{\\partial b}= \\text{error} \\times \\text{activation_derivative} \\times 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74380e7-8d2d-4f77-a38c-acaf26c851ae",
   "metadata": {},
   "source": [
    "**Visualizing the Bias Gradient**\n",
    "\n",
    "Think of our neuron processing a batch of 4 examples:\n",
    "\n",
    "```\n",
    "Example 1: xâ‚ â†’ zâ‚ = ðœƒÂ·xâ‚ + b â†’ aâ‚ = leaky_relu(zâ‚) â†’ J = (y_true - aâ‚)Â²\n",
    "Example 2: xâ‚‚ â†’ zâ‚‚ = ðœƒÂ·xâ‚‚ + b â†’ aâ‚‚ = leaky_relu(zâ‚‚) â†’ J = (y_true - aâ‚‚)Â²\n",
    "Example 3: xâ‚ƒ â†’ zâ‚ƒ = ðœƒÂ·xâ‚ƒ + b â†’ aâ‚ƒ = leaky_relu(zâ‚ƒ) â†’ J = (y_true - aâ‚ƒ)Â²\n",
    "Example 4: xâ‚„ â†’ zâ‚„ = ðœƒÂ·xâ‚„ + b â†’ aâ‚„ = leaky_relu(zâ‚„) â†’ J = (y_true - aâ‚„)Â²\n",
    "```\n",
    "\n",
    "**Key Insight**: The same bias `b` affects **every example** in the batch!\n",
    "\n",
    "**Mathematical Derivation**\n",
    "\n",
    "For squared error loss: $$J = \\frac{1}{2} \\sum_{i=1}^{m} (y_{pred}^{(i)} - y_{true}^{(i)})^2$$\n",
    "\n",
    "Let's trace the gradient flow for bias:\n",
    "\n",
    "1. **For one example**: $\\displaystyle \\frac{\\partial J^{(i)}}{\\partial b} = (y_{pred}^{(i)} - y_{true}^{(i)}) \\cdot \\text{leaky_relu}'(z^{(i)}) \\cdot 1$\n",
    "\n",
    "2. **For the whole batch**: $\\displaystyle \\frac{\\partial J}{\\partial b} = \\sum_{i=1}^{m} \\frac{\\partial J^{(i)}}{\\partial b}$\n",
    "\n",
    "So: `dJ_db = sum_over_all_examples(error * activation_derivative)`\n",
    "\n",
    "**Why Sum Instead of Average?**\n",
    "\n",
    "Let's see what happens with a concrete batch:\n",
    "\n",
    "```python\n",
    "# Example batch of 4 examples\n",
    "errors = [0.5, -0.2, 0.3, -0.1]           # y_pred - y_true\n",
    "derivatives = [1.0, 0.01, 1.0, 0.01]      # leaky_relu_derivative\n",
    "\n",
    "# Correct: SUM (total effect of bias on all examples)\n",
    "dJ_db = (0.5*1.0) + (-0.2*0.01) + (0.3*1.0) + (-0.1*0.01) = 0.5 - 0.002 + 0.3 - 0.001 = 0.797\n",
    "\n",
    "# Wrong: AVERAGE (would underestimate total bias contribution)\n",
    "dJ_db_avg = 0.797 / 4 = 0.199  # This would make bias updates 4x too small!\n",
    "```\n",
    "\n",
    "**The intuition**: Bias is a **shared parameter** across all examples, so we need to accumulate its effect from the entire batch.\n",
    "\n",
    "**Now Let's Compare with Weight Gradients**\n",
    "\n",
    "For weights, each weight connects to specific input features:\n",
    "\n",
    "```\n",
    "Example 1: wÂ·xâ‚ = wâ‚Â·xâ‚â‚ + wâ‚‚Â·xâ‚â‚‚ + ... + wâ‚™Â·xâ‚â‚™  \n",
    "Example 2: wÂ·xâ‚‚ = wâ‚Â·xâ‚‚â‚ + wâ‚‚Â·xâ‚‚â‚‚ + ... + wâ‚™Â·xâ‚‚â‚™\n",
    "```\n",
    "\n",
    "The gradient for weight wâ±¼ comes from: $\\displaystyle \\frac{\\partial J}{\\partial w_j} = \\sum_{i=1}^{m} (\\text{error}^{(i)} \\cdot \\text{derivative}^{(i)} \\cdot x_{ij})$\n",
    "\n",
    "This is exactly what `np.dot(error * derivative, x)` computes!\n",
    "\n",
    "**Let's Build This Understanding Together**\n",
    "\n",
    "**Step 1**: Does the \"bias affects all examples equally\" intuition make sense?\n",
    "\n",
    "**Step 2**: Can you see why we sum across examples for bias gradient but use dot product for weights?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2442edae-782a-4703-b8a0-2648912e9a11",
   "metadata": {},
   "source": [
    "### Why Sum for Bias vs Dot Product for Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c9dff9-5039-4c13-b179-0fa2d1c4055d",
   "metadata": {},
   "source": [
    "**Bias Gradient: Simple Sum**\n",
    "\n",
    "For bias, every example contributes equally:\n",
    "```\n",
    "dJ_db = sum_over_all_examples(error * activation_derivative)\n",
    "```\n",
    "\n",
    "Why? Because bias affects **every example identically** - it's like a universal offset.\n",
    "\n",
    "**Weight Gradient: Dot Product**\n",
    "\n",
    "For weights, each example contributes **differently** based on its input features:\n",
    "```\n",
    "dJ_dw = dot_product(error * activation_derivative, x)\n",
    "```\n",
    "\n",
    "Why dot product? Let's break it down:\n",
    "\n",
    "**Concrete Example**\n",
    "\n",
    "Say we have 3 examples and 2 features:\n",
    "\n",
    "```python\n",
    "# Input data (3 examples, 2 features)\n",
    "x = [[0.1, 0.2],    # example 1\n",
    "     [0.3, 0.4],    # example 2  \n",
    "     [0.5, 0.6]]    # example 3\n",
    "\n",
    "# For each example:\n",
    "errors = [0.5, -0.2, 0.3]                    # (y_pred - y_true)\n",
    "derivatives = [1.0, 1.0, 1.0]                # activation derivatives\n",
    "\n",
    "# Bias gradient (simple sum):\n",
    "dJ_db = (0.5*1.0) + (-0.2*1.0) + (0.3*1.0) = 0.5 - 0.2 + 0.3 = 0.6\n",
    "\n",
    "# Weight gradient (dot product):\n",
    "# We need: sum over examples of (error * derivative * x_ij) for each feature j\n",
    "\n",
    "# For weight wâ‚ (first feature):\n",
    "dJ_dw1 = (0.5*1.0*0.1) + (-0.2*1.0*0.3) + (0.3*1.0*0.5) \n",
    "       = 0.05 + (-0.06) + 0.15 = 0.14\n",
    "\n",
    "# For weight wâ‚‚ (second feature):  \n",
    "dJ_dw2 = (0.5*1.0*0.2) + (-0.2*1.0*0.4) + (0.3*1.0*0.6)\n",
    "       = 0.10 + (-0.08) + 0.18 = 0.20\n",
    "\n",
    "dJ_dw = [0.14, 0.20]  # This is exactly what np.dot(error * derivative, x) computes!\n",
    "```\n",
    "\n",
    "**The Key Difference**\n",
    "\n",
    "- **Bias**: Same \"contribution weight\" for every example (always Ã—1)\n",
    "- **Weights**: Contribution depends on the input feature values\n",
    "\n",
    "So we accumulate across examples for both, but **how** we accumulate differs:\n",
    "\n",
    "- Bias: Simple sum (all examples treated equally)\n",
    "- Weights: Weighted sum (examples weighted by their input features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e72dd7-6076-48cc-a6a7-5272f3d02f32",
   "metadata": {},
   "source": [
    "## `self.forward` dependency problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5dfa40-2478-4f2c-a8a2-508603d9118c",
   "metadata": {},
   "source": [
    "```python\n",
    "def gradient(self, X: np.array, y_true: np.array) -> np.array:\n",
    "    \"\"\"Gradients for squared error loss: J = (y_pred - y_true)^2.\"\"\"\n",
    "    error = self.a - y_true  # â† RELIES ON PREVIOUS forward() CALL!\n",
    "```\n",
    "\n",
    "**Problem**: `gradient()` depends on `self.a` being populated by a previous `forward()` call. This creates hidden state dependencies.\n",
    "\n",
    "**Risk**: If someone calls `gradient()` without calling `forward()` first, they get stale or missing `self.a` values.\n",
    "\n",
    "**Fix Options**:\n",
    "- Option A: Call `forward()` internally within `gradient()`\n",
    "- Option B: Require `y_pred` as parameter to `gradient()`\n",
    "- Option C: Document the dependency clearly and add validation\n",
    "\n",
    "How do you want to handle the state dependency between `forward()` and `gradient()`? This is the main architectural decision remaining.\n",
    "\n",
    "Answer: \"I decided to add self.forward to the self.gradient method. I do not like such approaches because one can call self.forward twice, but it looks like this is the standard engineering way so people know this works like this and if I do it in my style they may miss the documentation and be confused.\"\n",
    "\n",
    "You've chosen the pragmatic, standard approach.\n",
    "\n",
    "**Why This is the Right Choice**\n",
    "\n",
    "1. **Principle of Least Surprise**: Most ML practitioners expect `gradient()` to be self-contained\n",
    "2. **API Safety**: Prevents subtle bugs from incorrect call sequences  \n",
    "3. **Standard Pattern**: Matches how frameworks like PyTorch and TensorFlow work internally\n",
    "4. **Documentation Simplicity**: Reduces the cognitive load on users\n",
    "\n",
    "**The Engineering Trade-off**\n",
    "\n",
    "You correctly identified the trade-off:\n",
    "- **Your preferred style**: Explicit dependencies, cleaner separation of concerns\n",
    "- **Standard approach**: Implicit forward call, more robust for general use\n",
    "\n",
    "In systems architecture, we often choose the **more robust** pattern over the **theoretically pure** one, especially for user-facing APIs.\n",
    "\n",
    "**Performance Note**\n",
    "\n",
    "Yes, this means `forward()` might be called twice if someone does:\n",
    "```python\n",
    "y = neuron.forward(X)\n",
    "grads = neuron.gradient(X, y_true)  # forward() called again internally\n",
    "```\n",
    "\n",
    "But in practice:\n",
    "- The computational cost is negligible for a single neuron\n",
    "- The safety benefit outweighs the minor redundancy\n",
    "- In training loops, you typically only call `gradient()` anyway\n",
    "\n",
    "More over, this design is correct for the educational context. The forward pass within gradient computation ensures fresh state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3217d2b1-df56-4b42-84e0-20e10b25d35e",
   "metadata": {},
   "source": [
    "# 2.3 Code updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a975c38-99f0-49c8-9e77-df923476d12a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T11:13:28.403603Z",
     "iopub.status.busy": "2025-11-15T11:13:28.403249Z",
     "iopub.status.idle": "2025-11-15T11:13:28.728508Z",
     "shell.execute_reply": "2025-11-15T11:13:28.721682Z",
     "shell.execute_reply.started": "2025-11-15T11:13:28.403571Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Neuron:\n",
    "    \"\"\"Implement single neuron class.\"\"\"\n",
    "\n",
    "    def __init__(self: \"Neuron\") -> None:\n",
    "        \"\"\"Initialize neuron entity.\"\"\"\n",
    "        self.w: np.ndarray = None\n",
    "        self.b: np.float64 = np.float64(0.0)\n",
    "        # linear transformation\n",
    "        self.z: np.ndarray = None\n",
    "        # activation transformation\n",
    "        self.a: np.ndarray = None\n",
    "        # activation function hyperparameter\n",
    "        self.alpha: np.float32 = np.float32(0.01)\n",
    "        # learning rate for weights update\n",
    "        self.lr: np.float32 = np.float32(0.01)\n",
    "\n",
    "    def activation(\n",
    "        self,\n",
    "        alpha: np.float32 = None,\n",
    "        activation_algorithm: str = \"leaky_relu\",\n",
    "    ):\n",
    "        \"\"\"Break linearity.\"\"\"\n",
    "        if alpha is not None:\n",
    "            self.alpha = alpha\n",
    "        if activation_algorithm == \"leaky_relu\":\n",
    "            self.a = np.maximum(self.alpha * self.z, self.z)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Activation function {activation_algorithm} not implemented (yet)\"\n",
    "            )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        X: np.ndarray,\n",
    "        alpha: np.float32 = None,\n",
    "        activation_algorithm: str = \"leaky_relu\",\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"Calculate forward pass with activation function.\"\"\"\n",
    "        # Xavier/Glorot weights initialization\n",
    "        if self.w is None:\n",
    "            # size of one example\n",
    "            self.input_size = X.shape[-1]\n",
    "            self.w = (np.random.default_rng().random(self.input_size) - 0.5) * np.sqrt(\n",
    "                2.0 / self.input_size\n",
    "            )\n",
    "\n",
    "        # compute linear transformation\n",
    "        self.z = np.dot(X, self.w) + self.b\n",
    "\n",
    "        # compute activation transformation\n",
    "        self.activation(alpha, activation_algorithm)\n",
    "\n",
    "        return self.a\n",
    "\n",
    "    def gradient_of_a(self, activation_algorithm: str = \"leaky_relu\"):\n",
    "        \"\"\"Compute a'.\"\"\"\n",
    "        if activation_algorithm == \"leaky_relu\":\n",
    "            da_dz = np.where(self.z < 0, self.alpha, 1)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Activation function {activation_algorithm} not implemented (yet)\"\n",
    "            )\n",
    "\n",
    "        return da_dz\n",
    "\n",
    "    def gradient(\n",
    "        self,\n",
    "        X: np.ndarray,\n",
    "        y_true: np.ndarray,\n",
    "        alpha: np.float32 = None,\n",
    "        activation_algorithm: str = \"leaky_relu\",\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"Gradients for squared error loss: J = (y_pred - y_true)^2 / 2.\"\"\"\n",
    "        # X and y_pred shape check\n",
    "        batch_size = X.shape[0]\n",
    "        y_true_number = y_true.shape[0]\n",
    "        if batch_size != y_true.shape[0]:\n",
    "            raise ValueError(\n",
    "                f\"Batch size of X={batch_size} must be of y_pred={y_true_number} size.\"\n",
    "            )\n",
    "\n",
    "        # make forward pass\n",
    "        self.forward(\n",
    "            X,\n",
    "            alpha=alpha,\n",
    "            activation_algorithm=activation_algorithm,\n",
    "        )\n",
    "\n",
    "        # compute J', where J is MSE\n",
    "        dJ_da = (self.a - y_true) / batch_size\n",
    "\n",
    "        # compute a'\n",
    "        da_dz = self.gradient_of_a(activation_algorithm=activation_algorithm)\n",
    "\n",
    "        # compute complete gradients\n",
    "        dJ_dw = np.dot((dJ_da * da_dz), X)\n",
    "        dJ_db = np.sum(dJ_da * da_dz)\n",
    "\n",
    "        return dJ_dw, dJ_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9100a5c-825a-487d-86a8-f2aea0c6a8df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T11:19:07.339343Z",
     "iopub.status.busy": "2025-11-15T11:19:07.338846Z",
     "iopub.status.idle": "2025-11-15T11:19:07.348997Z",
     "shell.execute_reply": "2025-11-15T11:19:07.347269Z",
     "shell.execute_reply.started": "2025-11-15T11:19:07.339293Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights: [-0.21315176  0.18414799  0.22363435  0.16931667  0.23618769]\n",
      "linear transformation: [7.46865836 6.19027101 4.99572878 4.8260706 ]\n",
      "activation: [7.46865836 6.19027101 4.99572878 4.8260706 ]\n",
      "dJ_dw: [  301.00045528 -1480.74035788  -881.87527767 -3666.0226465\n",
      "  -690.80262376]\n",
      "dJ_db: -244.12981781452544\n"
     ]
    }
   ],
   "source": [
    "X = np.array(\n",
    "    [\n",
    "        [-1, 2, 3, 13, 17],\n",
    "        [4, -5, 6, 14, 18],\n",
    "        [7, 8, -9, 15, 19],\n",
    "        [-10, 11, 12, 16, -20],\n",
    "    ],\n",
    "    dtype=np.float64,\n",
    ")\n",
    "y = np.array([100, 200, 300, 400], dtype=np.float64)\n",
    "\n",
    "my_neuron = Neuron()\n",
    "dJ_dw, dJ_db = my_neuron.gradient(y_true=y, X=X)\n",
    "\n",
    "# print(\"my_neuron.w:\", my_neuron.w)\n",
    "# print(\"linear transformation:\", my_neuron._get_linear_transformation(X))\n",
    "# print(\"my_neuron.forward(X):\", my_neuron.forward(X))\n",
    "print(\"weights:\", my_neuron.w)\n",
    "print(\"linear transformation:\", my_neuron.z)\n",
    "print(\"activation:\", my_neuron.a)\n",
    "print(\"dJ_dw:\", dJ_dw)\n",
    "print(\"dJ_db:\", dJ_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8c52d3-e910-4564-9145-f8dbbaaf06c1",
   "metadata": {},
   "source": [
    "## Type Hint Best Practices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b02c80d-7735-4e7d-bee9-0deec705ed66",
   "metadata": {},
   "source": [
    "For deep learning systems:\n",
    "\n",
    "```python\n",
    "# Standard practice:\n",
    "import numpy.typing as npt\n",
    "\n",
    "class Neuron:\n",
    "    def forward(\n",
    "        self,\n",
    "        X: npt.NDArray[np.float32],  # Consistent precision\n",
    "        activation_f: str = \"leaky_relu\",\n",
    "    ) -> npt.NDArray[np.float32]:\n",
    "        \n",
    "    # Use float32 throughout for GPU compatibility\n",
    "    self.w: npt.NDArray[np.float32] = None  \n",
    "    self.b: np.float32 = np.float32(0.0)\n",
    "    self.lr: np.float32 = np.float32(0.01)\n",
    "```\n",
    "\n",
    "**Rationale:** GPUs are optimized for float32, and mixed precision causes unnecessary casting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925a3b44-825f-4fb0-b273-955ab20c916e",
   "metadata": {},
   "source": [
    "# <b>3. Weights update</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27081ca9-ace7-47c1-84f2-88c5fab781e5",
   "metadata": {},
   "source": [
    "# 3.1 Update weights test oracle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895bf839-6349-41e7-a755-0e0d36ce8580",
   "metadata": {},
   "source": [
    "A **test oracle** - a separate, verified implementation used to validate your main code.\n",
    "\n",
    "**Test Oracle Pattern**: Creating an independent implementation to verify the system under test. This is valid when:\n",
    "- The oracle is simpler/more transparent than the main implementation\n",
    "- It's manually verified by a human before use\n",
    "- It serves as a reference implementation\n",
    "\n",
    "**Your framework below is actually sophisticated**:\n",
    "- It documents the complete mathematical reasoning\n",
    "- Allows manual verification of each test case\n",
    "- Creates reproducible, mathematically consistent scenarios\n",
    "- Serves as both test data AND documentation\n",
    "\n",
    "**When This Approach Is Valuable**\n",
    "\n",
    "1. **Complex mathematical systems** (like neural networks)\n",
    "2. **Learning contexts** where understanding the derivation matters\n",
    "3. **Reference implementations** for algorithm validation\n",
    "4. **Documentation of expected behavior**\n",
    "\n",
    "**The Professional Balance**\n",
    "\n",
    "In production, you'd likely:\n",
    "- Keep your detailed oracle for complex cases\n",
    "- Use simple manual tests for basic functionality\n",
    "- Have both verification strategies\n",
    "\n",
    "**Your approach is architecturally sound for a learning context**. The key is ensuring you manually verify the oracle outputs before using them to test your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b78b051d-a79b-4b97-b036-fed4c7393e7e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-11T21:17:01.627462Z",
     "iopub.status.busy": "2025-11-11T21:17:01.627118Z",
     "iopub.status.idle": "2025-11-11T21:17:01.636300Z",
     "shell.execute_reply": "2025-11-11T21:17:01.634852Z",
     "shell.execute_reply.started": "2025-11-11T21:17:01.627438Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_backprop_test_case(case_name, X, y_true, w, b, lr, activation_f=\"leaky_relu\", cost_f=\"mse\",):\n",
    "    \"\"\"Create backpropagation test case.\"\"\"\n",
    "    # final result\n",
    "    result = {\n",
    "        \"name\": case_name,\n",
    "        \"initial_X\": X,\n",
    "        \"y_true\": y_true,\n",
    "        \"initial_w\": w,\n",
    "        \"initial_b\": b,\n",
    "        \"learning_rate\": lr\n",
    "    }\n",
    "\n",
    "    # linear transformation\n",
    "    z = np.dot(X, w) + b\n",
    "    result[\"z\"] = z\n",
    "    \n",
    "    # activation function\n",
    "    alpha = 0.01\n",
    "    if activation_f == \"leaky_relu\":\n",
    "        a = np.maximum(z * alpha, z)\n",
    "    result[\"a\"] = a\n",
    "\n",
    "    # cost function\n",
    "    if cost_f == \"mse\":\n",
    "        u = a - y_true\n",
    "        J = u**2 / 2\n",
    "    else:\n",
    "        raise ValueError(f\"Cost function \\\"{cost_function}\\\" is not implemented yet.\")\n",
    "\n",
    "    result[\"J\"] = J\n",
    "\n",
    "    # backpropagation\n",
    "    \n",
    "    # J'\n",
    "    dJ_da = u\n",
    "    result[\"dJ_da\"] = dJ_da\n",
    "    \n",
    "    # a'\n",
    "    da_dz = np.where(z < 0, alpha, 1)\n",
    "    result[\"da_dz\"] = da_dz\n",
    "    \n",
    "    # z'\n",
    "    dz_dw = X\n",
    "    result[\"dz_dw\"] = dz_dw\n",
    "\n",
    "    # b'\n",
    "    dz_b = 1\n",
    "    result[\"dz_b\"] = dz_b\n",
    "\n",
    "    dJ_dw = np.dot(dJ_da * da_dz, dz_dw)  # where dz_dw = X\n",
    "    result[\"dJ_dw\"] = dJ_dw\n",
    "\n",
    "    dJ_db = np.sum(dJ_da * da_dz)*dz_b  # where dz_b = 1\n",
    "    result[\"dJ_db\"] = dJ_db\n",
    "\n",
    "    # w - lr * (-gradient) = w + lr*|gradient|\n",
    "    result[\"expected_w\"] = np.subtract(w, (lr * dJ_dw))\n",
    "    result[\"expected_b\"] = np.subtract(b, (lr * dJ_db))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ff1954cf-30e7-41a9-9167-fc7ce743f73d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-11T21:17:02.333308Z",
     "iopub.status.busy": "2025-11-11T21:17:02.332425Z",
     "iopub.status.idle": "2025-11-11T21:17:02.360573Z",
     "shell.execute_reply": "2025-11-11T21:17:02.357786Z",
     "shell.execute_reply.started": "2025-11-11T21:17:02.333237Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'negative_gradients_increase_weights',\n",
       " 'initial_X': array([[ 1., -2.]]),\n",
       " 'y_true': 1.5,\n",
       " 'initial_w': array([0.3, 0.4]),\n",
       " 'initial_b': 0.1,\n",
       " 'learning_rate': 0.01,\n",
       " 'z': array([-0.4]),\n",
       " 'a': array([-0.004]),\n",
       " 'J': array([1.131008]),\n",
       " 'dJ_da': array([-1.504]),\n",
       " 'da_dz': array([0.01]),\n",
       " 'dz_dw': array([[ 1., -2.]]),\n",
       " 'dz_b': 1,\n",
       " 'dJ_dw': array([-0.01504,  0.03008]),\n",
       " 'dJ_db': np.float64(-0.01504),\n",
       " 'expected_w': array([0.3001504, 0.3996992]),\n",
       " 'expected_b': np.float64(0.1001504)}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case1 = create_backprop_test_case(\n",
    "    \"negative_gradients_increase_weights\",\n",
    "     np.array([[1.0, -2.0]]),\n",
    "     y_true = 1.5,\n",
    "     w = np.array([0.3, 0.4]),\n",
    "     b = 0.1,\n",
    "     lr = 0.01\n",
    ")\n",
    "case1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb622de-db04-4ab0-beee-952ad64da28c",
   "metadata": {},
   "source": [
    "Your gradient computation is now mathematically correct.\n",
    "\n",
    "These gradients make perfect sense:\n",
    "- The weight gradients are proportional to their input values (1.0 and 2.0)\n",
    "- All gradients point downward (negative) since we need to increase the weights to reduce the loss\n",
    "\n",
    "We now have mathematically consistent test data:\n",
    "- Initial: `w = [0.3, 0.4]`, `b = 0.1`\n",
    "- Gradients: `dJ_dw = [-0.6, -1.2]`, `dJ_db = [-0.6]`\n",
    "- Learning rate: `0.01`\n",
    "\n",
    "**Gradient Descent Update:**\n",
    "```\n",
    "w_new = w_old - learning_rate * gradient\n",
    "```\n",
    "\n",
    "If gradient is **negative**, then:\n",
    "```\n",
    "w_new = w_old - learning_rate * (negative_number)\n",
    "w_new = w_old + learning_rate * positive_number\n",
    "```\n",
    "\n",
    "**In our case:**\n",
    "- Gradients are all negative: `[-0.3, -0.6]` and `[-0.3]`\n",
    "- This means we need to increase weights and bias to reduce loss\n",
    "\n",
    "**Expected after update:**\n",
    "```\n",
    "w_new = [0.3, 0.4] - 0.01 * [-0.3, -0.6] = [0.303, 0.406]\n",
    "b_new = 0.1 - 0.01 * [-0.3] = [0.103]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ea5df84b-d8be-4e31-bc4a-a5190549ee1e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-11T21:17:10.010968Z",
     "iopub.status.busy": "2025-11-11T21:17:10.010595Z",
     "iopub.status.idle": "2025-11-11T21:17:10.018949Z",
     "shell.execute_reply": "2025-11-11T21:17:10.017752Z",
     "shell.execute_reply.started": "2025-11-11T21:17:10.010940Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'negative_gradients_increase_weights',\n",
       " 'initial_X': array([[ 1., -2.]]),\n",
       " 'y_true': 1.5,\n",
       " 'initial_w': array([0.3, 0.4]),\n",
       " 'initial_b': 0.1,\n",
       " 'learning_rate': 0.01,\n",
       " 'z': array([-0.4]),\n",
       " 'a': array([-0.004]),\n",
       " 'J': array([1.131008]),\n",
       " 'dJ_da': array([-1.504]),\n",
       " 'da_dz': array([0.01]),\n",
       " 'dz_dw': array([[ 1., -2.]]),\n",
       " 'dz_b': 1,\n",
       " 'dJ_dw': array([-0.01504,  0.03008]),\n",
       " 'dJ_db': np.float64(-0.01504),\n",
       " 'expected_w': array([0.3001504, 0.3996992]),\n",
       " 'expected_b': np.float64(0.1001504)}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16889a2f-89c3-401e-a5fd-42faafaf1178",
   "metadata": {},
   "source": [
    "Create more test cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "985d88e9-ac7f-4727-8b60-4afade7a64f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-11T21:17:16.895536Z",
     "iopub.status.busy": "2025-11-11T21:17:16.895236Z",
     "iopub.status.idle": "2025-11-11T21:17:16.923745Z",
     "shell.execute_reply": "2025-11-11T21:17:16.922467Z",
     "shell.execute_reply.started": "2025-11-11T21:17:16.895511Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'negative_gradients_increase_weights',\n",
       "  'initial_X': array([[ 1., -2.]]),\n",
       "  'y_true': 1.5,\n",
       "  'initial_w': array([0.3, 0.4]),\n",
       "  'initial_b': 0.1,\n",
       "  'learning_rate': 0.01,\n",
       "  'z': array([-0.4]),\n",
       "  'a': array([-0.004]),\n",
       "  'J': array([1.131008]),\n",
       "  'dJ_da': array([-1.504]),\n",
       "  'da_dz': array([0.01]),\n",
       "  'dz_dw': array([[ 1., -2.]]),\n",
       "  'dz_b': 1,\n",
       "  'dJ_dw': array([-0.01504,  0.03008]),\n",
       "  'dJ_db': np.float64(-0.01504),\n",
       "  'expected_w': array([0.3001504, 0.3996992]),\n",
       "  'expected_b': np.float64(0.1001504)},\n",
       " {'name': 'zero_gradients_no_change',\n",
       "  'initial_X': array([[1., 2.]]),\n",
       "  'y_true': 1.2,\n",
       "  'initial_w': array([0.3, 0.4]),\n",
       "  'initial_b': 0.1,\n",
       "  'learning_rate': 0.01,\n",
       "  'z': array([1.2]),\n",
       "  'a': array([1.2]),\n",
       "  'J': array([2.46519033e-32]),\n",
       "  'dJ_da': array([2.22044605e-16]),\n",
       "  'da_dz': array([1.]),\n",
       "  'dz_dw': array([[1., 2.]]),\n",
       "  'dz_b': 1,\n",
       "  'dJ_dw': array([2.22044605e-16, 4.44089210e-16]),\n",
       "  'dJ_db': np.float64(2.220446049250313e-16),\n",
       "  'expected_w': array([0.3, 0.4]),\n",
       "  'expected_b': np.float64(0.1)},\n",
       " {'name': 'large_learning_rate',\n",
       "  'initial_X': array([[1., 2.]]),\n",
       "  'y_true': 2.0,\n",
       "  'initial_w': array([0.3, 0.4]),\n",
       "  'initial_b': 0.1,\n",
       "  'learning_rate': 0.1,\n",
       "  'z': array([1.2]),\n",
       "  'a': array([1.2]),\n",
       "  'J': array([0.32]),\n",
       "  'dJ_da': array([-0.8]),\n",
       "  'da_dz': array([1.]),\n",
       "  'dz_dw': array([[1., 2.]]),\n",
       "  'dz_b': 1,\n",
       "  'dJ_dw': array([-0.8, -1.6]),\n",
       "  'dJ_db': np.float64(-0.7999999999999998),\n",
       "  'expected_w': array([0.38, 0.56]),\n",
       "  'expected_b': np.float64(0.18)},\n",
       " {'name': 'batch_input_gradients',\n",
       "  'initial_X': array([[1. , 2. ],\n",
       "         [0.5, 1. ]]),\n",
       "  'y_true': array([1.5, 0.8]),\n",
       "  'initial_w': array([0.3, 0.4]),\n",
       "  'initial_b': 0.1,\n",
       "  'learning_rate': 0.01,\n",
       "  'z': array([1.2 , 0.65]),\n",
       "  'a': array([1.2 , 0.65]),\n",
       "  'J': array([0.045  , 0.01125]),\n",
       "  'dJ_da': array([-0.3 , -0.15]),\n",
       "  'da_dz': array([1., 1.]),\n",
       "  'dz_dw': array([[1. , 2. ],\n",
       "         [0.5, 1. ]]),\n",
       "  'dz_b': 1,\n",
       "  'dJ_dw': array([-0.375, -0.75 ]),\n",
       "  'dJ_db': np.float64(-0.44999999999999984),\n",
       "  'expected_w': array([0.30375, 0.4075 ]),\n",
       "  'expected_b': np.float64(0.10450000000000001)},\n",
       " {'name': 'single_feature',\n",
       "  'initial_X': array([[1.]]),\n",
       "  'y_true': 0.5,\n",
       "  'initial_w': array([0.3]),\n",
       "  'initial_b': 0.1,\n",
       "  'learning_rate': 0.01,\n",
       "  'z': array([0.4]),\n",
       "  'a': array([0.4]),\n",
       "  'J': array([0.005]),\n",
       "  'dJ_da': array([-0.1]),\n",
       "  'da_dz': array([1.]),\n",
       "  'dz_dw': array([[1.]]),\n",
       "  'dz_b': 1,\n",
       "  'dJ_dw': array([-0.1]),\n",
       "  'dJ_db': np.float64(-0.09999999999999998),\n",
       "  'expected_w': array([0.301]),\n",
       "  'expected_b': np.float64(0.101)},\n",
       " {'name': 'extreme_gradients',\n",
       "  'initial_X': array([[100., 200.]]),\n",
       "  'y_true': 500.0,\n",
       "  'initial_w': array([0.3, 0.4]),\n",
       "  'initial_b': 0.1,\n",
       "  'learning_rate': 0.001,\n",
       "  'z': array([110.1]),\n",
       "  'a': array([110.1]),\n",
       "  'J': array([76011.005]),\n",
       "  'dJ_da': array([-389.9]),\n",
       "  'da_dz': array([1.]),\n",
       "  'dz_dw': array([[100., 200.]]),\n",
       "  'dz_b': 1,\n",
       "  'dJ_dw': array([-38990., -77980.]),\n",
       "  'dJ_db': np.float64(-389.9),\n",
       "  'expected_w': array([39.29, 78.38]),\n",
       "  'expected_b': np.float64(0.4899)}]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Edge case: Zero gradients (no learning should occur)\n",
    "case2 = create_backprop_test_case(\n",
    "    \"zero_gradients_no_change\",\n",
    "    np.array([[1.0, 2.0]]),\n",
    "    y_true=1.2,  # Exactly matches our forward pass output\n",
    "    w=np.array([0.3, 0.4]),\n",
    "    b=0.1,\n",
    "    lr=0.01\n",
    ")\n",
    "\n",
    "# Edge case: Large learning rate\n",
    "case3 = create_backprop_test_case(\n",
    "    \"large_learning_rate\",\n",
    "    np.array([[1.0, 2.0]]),\n",
    "    y_true=2.0,  # Larger error\n",
    "    w=np.array([0.3, 0.4]),\n",
    "    b=0.1,\n",
    "    lr=0.1  # 10x larger learning rate\n",
    ")\n",
    "\n",
    "# Batch input case (multiple examples)\n",
    "case4 = create_backprop_test_case(\n",
    "    \"batch_input_gradients\",\n",
    "    np.array([[1.0, 2.0], [0.5, 1.0]]),  # 2 examples\n",
    "    y_true=np.array([1.5, 0.8]),  # Batch targets\n",
    "    w=np.array([0.3, 0.4]),\n",
    "    b=0.1,\n",
    "    lr=0.01\n",
    ")\n",
    "\n",
    "# Single feature case\n",
    "case5 = create_backprop_test_case(\n",
    "    \"single_feature\",\n",
    "    np.array([[1.0]]),  # Single input feature\n",
    "    y_true=0.5,\n",
    "    w=np.array([0.3]),\n",
    "    b=0.1,\n",
    "    lr=0.01\n",
    ")\n",
    "\n",
    "# Extreme values case\n",
    "case6 = create_backprop_test_case(\n",
    "    \"extreme_gradients\",\n",
    "    np.array([[100.0, 200.0]]),  # Large inputs\n",
    "    y_true=500.0,  # Large target\n",
    "    w=np.array([0.3, 0.4]),\n",
    "    b=0.1,\n",
    "    lr=0.001  # Smaller LR for stability\n",
    ")\n",
    "\n",
    "test_cases = [case1, case2, case3, case4, case5, case6]\n",
    "\n",
    "test_cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350af3f7-1aa6-403f-8539-37aa6ea41d69",
   "metadata": {},
   "source": [
    "## Test Oracle Separation: Best Practices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767e2cee-89b5-486f-a4b0-62d6bcce0d9f",
   "metadata": {},
   "source": [
    "**Separate responsibilities:**\n",
    "\n",
    "If your `Neuron.gradient()` has a bug, your test oracle won't have the same bug.\n",
    "\n",
    "```python\n",
    "def compute_expected_gradients(X, w, b, y_true, alpha=0.01):\n",
    "    \"\"\"Pure function: independent mathematical verification\"\"\"\n",
    "    # Manual computation without Neuron class\n",
    "    z = X @ w + b\n",
    "    a = np.maximum(alpha * z, z)\n",
    "    dJ_da = a - y_true\n",
    "    da_dz = np.where(z < 0, alpha, 1.0)\n",
    "    dJ_dw = (dJ_da * da_dz) @ X / X.shape[0]  # Note: batch average!\n",
    "    dJ_db = np.sum(dJ_da * da_dz) / X.shape[0]\n",
    "    return dJ_dw, dJ_db\n",
    "\n",
    "class TestNeuron:\n",
    "    def create_test_case(self, name, X, w, b, y_true):\n",
    "        # Just store test parameters\n",
    "        test_case = {\"name\": name, \"X\": X, \"w\": w, \"b\": b, \"y_true\": y_true}\n",
    "        \n",
    "        # Compute expected results independently\n",
    "        expected_dw, expected_db = compute_expected_gradients(X, w, b, y_true)\n",
    "        test_case[\"expected_dw\"] = expected_dw\n",
    "        test_case[\"expected_db\"] = expected_db\n",
    "        \n",
    "        self.test_cases.append(test_case)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0395a1be-4d80-41ac-90ff-c046972309f5",
   "metadata": {},
   "source": [
    "# 3.2 TDD cycle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9552773e-08ed-4d24-bf46-105f134358f8",
   "metadata": {},
   "source": [
    "We follow the TDD cycle: **Red** â†’ **Green** â†’ **Refactor**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114e16c3-aa81-473f-84ca-38b0189e3fc9",
   "metadata": {},
   "source": [
    "## Current Phase: RED (Write Failing Test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47fbc7a-97f9-4af9-86f9-7d126eb3660c",
   "metadata": {},
   "source": [
    "Using your test oracle data:\n",
    "\n",
    "```python\n",
    "def test_weight_update_negative_gradients():\n",
    "    \"\"\"Test that negative gradients correctly increase weights.\"\"\"\n",
    "    neuron = Neuron()\n",
    "    \n",
    "    # Setup initial state from oracle\n",
    "    neuron.w = np.array([0.3, 0.4])\n",
    "    neuron.b = 0.1\n",
    "    \n",
    "    # Gradients from oracle calculation\n",
    "    dJ_dw = np.array([-0.3, -0.6])\n",
    "    dJ_db = -0.3\n",
    "    learning_rate = 0.01\n",
    "    \n",
    "    # Call method (doesn't exist yet - this will fail)\n",
    "    neuron.update_weights(dJ_dw, dJ_db, learning_rate)\n",
    "    \n",
    "    # Expected results from oracle\n",
    "    expected_w = np.array([0.303, 0.406])\n",
    "    expected_b = 0.103\n",
    "    \n",
    "    # Assertions (will fail initially)\n",
    "    np.testing.assert_array_almost_equal(neuron.w, expected_w, decimal=6)\n",
    "    assert abs(neuron.b - expected_b) < 1e-6\n",
    "```\n",
    "\n",
    "**Your turn**: This test will fail because `update_weights` doesn't exist yet. Should we:\n",
    "1. Write this test and watch it fail (Red phase)\n",
    "2. Then implement the minimal `update_weights` method to make it pass (Green phase)\n",
    "\n",
    "*(I'll wait for your confirmation to proceed with the test)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7e10b2-5679-4b0c-8328-3e2b22996868",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb45c5bf-2616-4de5-935f-e2cb97827b1d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b430f080-1a8e-45ad-8ab6-9c74d3569ab8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e237b871-7a0d-4e8d-a5db-b15b74b4d22f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be4b250d-8300-472b-933d-bed4b0c26cca",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1cfb6d7-2ea2-4bbc-b199-f73bb589c253",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4446bb18-4532-4fce-9108-82e049bfa896",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b9c49f37-db7c-484c-a223-b67d54ff9467",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
