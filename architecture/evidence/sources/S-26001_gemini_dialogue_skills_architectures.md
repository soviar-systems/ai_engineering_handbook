---
id: S-26001
title: "Gemini — Skills Architecture, Tiered Memory, and Package-Driven Infrastructure"
date: 2026-02-28
model: gemini-3.0-flash
extracted_into: null
---

# S-26001: Gemini — Skills Architecture, Tiered Memory, and Package-Driven Infrastructure

Extended dialogue (2026-02-21 to 2026-02-27) exploring three architectural patterns for production-grade AI agent systems.

**Extracted from:** `2026_02_28_gemini_dialogue_skills_new_architectures.html` via `tools/scripts/extract_html_text.py`

---



‎Gemini - direct access to Google AI Gemini Understanding AI Assistant Skills https://gemini.google.com/share/10c1438e1c05  Created with Fast February 21, 2026 at 02:41 PM Published February 27, 2026 at 02:48 PM You said  what is this? I have never created skills  Tip: Create skills by adding .md files to .claude/skills/ in your project or ~/.claude/skills/ for skills that work in any  project It looks like you’ve encountered a feature designed to give your AI assistant "persistent memory" or specialized instructions for specific tasks.In the context of CLI tools (like the Claude Engineer or similar open-source wrappers), Skills are essentially custom playbooks. They allow you to define how the AI should behave or what knowledge it should prioritize without you having to re-explain it in every new chat session.What are "Skills" exactly?Think of a skill as a reusable prompt snippet. Instead of typing, "Always use TypeScript, follow the Airbnb style guide, and use Vitest for testing," every time you start a project, you save those instructions into a .md (Markdown) file.How they work:Project-Specific: Files in .claude/skills/ only activate when you are working inside that specific folder. This is great for project-specific coding standards or documentation.Global: Files in ~/.claude/skills/ (your home directory) are active regardless of where you are. These are perfect for general preferences, like "always be concise" or "explain code like I'm a senior dev."Why would you use them?Since you haven't created any yet, here are a few ways they actually make life easier:Use CaseWhat the Skill file doesCode StylingEnsures the AI always uses your preferred indentation or naming conventions.API ReferenceGive it a "skill" containing the documentation for a niche library it might not know well.WorkflowTell it to always create a pull request description after finishing a task.Export to SheetsHow to create your first oneIf you want to try it out, you can create a simple text file. For example, create ~/.claude/skills/brevity.md and paste this inside:"Always provide the code solution first, then a brief explanation in bullet points. Avoid conversational filler."From then on, the tool will automatically read that file and apply those rules to your interactions.Would you like me to help you draft a specific "Skill" file for a programming language or workflow you use often? You said  Show the the common developer example of creating a skill To give you a practical look at this, the most common "Skill" developers create is a Project Architect or Style Guide skill. This ensures the AI doesn't hallucinate libraries you don't use or write code that clashes with your linting rules.Here is a standard example of what a Skill file looks like for a modern web development workflow.Example: web-dev-standards.mdYou would save this file in .claude/skills/web-dev-standards.md.Markdown# Web Development Standards

When writing code for this project, always adhere to the following technical stack and patterns:

## Tech Stack
* **Framework:** Next.js 14 (App Router)
* **Styling:** Tailwind CSS (no CSS modules)
* **State Management:** TanStack Query (React Query)
* **Validation:** Zod

## Coding Rules
* Use **Arrow Functions** for functional components.
* Use **TypeScript** strictly; avoid the `any` type at all costs.
* Follow a **"Data-First"** approach: Define the Zod schema before writing the fetching logic.
* Use `lucide-react` for all icons.

## Error Handling
* Always wrap asynchronous fetch calls in a try/catch block that logs to our internal `logger.ts` utility.
Why this is the "Common" ExampleDevelopers usually create this specific skill because it solves three main frustrations:Version Control: It stops the AI from using the old "Pages" router if you are using the new "App" router in Next.js.Consistency: It prevents a mix of function Example() {} and const Example = () => {}.Library Drift: It prevents the AI from suggesting Axios if the project strictly uses the native fetch API.How to "Install" itIf you are using a terminal, you can create it instantly by running:Bashmkdir -p .claude/skills
touch .claude/skills/web-dev-standards.md
Then, just paste the Markdown content above into that file. The next time you ask the tool to "Build a login page," it will automatically use Next.js, Tailwind, and Zod without you mentioning them.Would you like me to generate a specific Skill file tailored to a different language like Python, Go, or Rust? You said  so, it's like the system prompt instruction? Exactly. You’ve hit the nail on the head.It is essentially a modular System Prompt.Instead of one giant, messy block of text in a settings menu, "Skills" allow you to break your system instructions into smaller, manageable files that the AI injects into the conversation based on your current project.The Key DifferencesWhile they act like system prompts, there are three main reasons developers prefer this "Skill" file approach:FeatureStandard System PromptSkills (.md files)ScopeGlobal (applies to every single chat).Contextual (only applies to the specific folder/project).OrganizationOne long wall of text.Modular (one file for CSS, one for Testing, one for API docs).PortabilityStuck in your UI settings.Version Controlled (you can commit them to Git so your whole team uses them).Export to SheetsHow it actually behavesWhen you start a chat, the tool looks at your directory, sees the .md files in that folder, and basically tells the AI:"In addition to your normal rules, also follow these specific instructions found in database-rules.md and typescript-style.md."A Pro Tip for "Skill" WritingSince these function as system instructions, use imperative language.Weak: "I would like it if you used trailing commas."Strong (Skill Style): "Always use trailing commas in all object literals and arrays."Would you like me to show you a "Global Skill" example—one you'd put in your home directory (~/.claude/skills/) to control how I talk to you across all projects? You said  it is the same as aider's CONVENTIONS.md in a project level but granular Spot on. If you’ve used Aider, you already understand the core concept. It is exactly the same "instructions-as-code" philosophy, but with a more modular architecture.While a CONVENTIONS.md file is usually one giant "source of truth" for a project, the Skills approach is designed to be granular and composable.The Comparison: Aider vs. SkillsFeatureAider CONVENTIONS.md.claude/skills/StructureSingle monolithic file.Multiple specific files (e.g., testing.md, ui.md).OrganizationHarder to manage as rules grow.Easy to "plug and play" specific rules.InheritanceProject-specific only.Layered: Global rules (~) + Project rules (./).LogicThe AI reads one long document.The AI treats each file as a discrete "capability."Export to SheetsWhy "Granular" WinsBy breaking a CONVENTIONS.md into smaller skills, you avoid "Prompt Fatigue" where the AI starts ignoring rules at the bottom of a 200-line file.Imagine your project structure looking like this:database.md: Only contains Prisma/Postgres rules.testing.md: Only contains Vitest/Playwright setups.styling.md: Only contains Tailwind/Shadcn configurations.The AI pulls these in as a "bundle" of system instructions. If you move to a new project that uses the same styling but a different database, you can just drop your styling.md skill into the new project's folder and you're instantly ready to go.One Unique Advantage: The "Global" LayerThe biggest difference from Aider's local-only conventions is the Home Directory (~/.claude/skills/).You can have a "Global Skill" that dictates your personality preferences (e.g., "Always be brief," "Never apologize for mistakes," "Use emojis when code works") that follows you into every project, while your project-level skills handle the technical heavy lifting.Would you like me to help you refactor a long list of instructions into a few "Granular Skills" so they’re easier for the AI to follow? You said  I can set the aider to read any file using /read-only You are exactly right—the logic is very similar to Aider's /read-only or its CONVENTIONS.md feature. However, Claude Skills introduce a "lazy loading" mechanism that changes how you manage context.Here is the breakdown of how they differ in practice:1. The "Lazy Loading" DifferenceIn Aider, when you /add a file or use a convention file, that text is typically "pinned" to the chat. It sits in your context window, eating up tokens and potentially distracting the model if the file is massive.Claude Skills use a tiered system:Tier 1 (Frontmatter): Claude only reads a tiny YAML header (the description) at the start.Tier 2 (The Content): Claude only "opens" and reads the full instructions if and when it decides they are relevant to your current prompt.Tier 3 (Linked Files): You can have a skill that points to other huge docs. Claude will only fetch those specific sub-files if the task requires that specific deep-dive.2. Global vs. Local "Conventions"Aider is very project-centric. Claude Skills allow you to have Global Conventions without cluttering your project.~/.claude/skills/: These are your "Personal Defaults." (e.g., "Always use tabs," "Never use emojis," "Always write a joke in the code comments")../.claude/skills/: These are your "Project Rules." (e.g., "Use this specific API key format," "Follow this internal naming convention").3. "Skills" as Tools (Slash Commands)Unlike a standard read-only file, a Skill can act as a custom command.
If you name a file .claude/skills/refactor.md, you can actually type /refactor in the terminal. Claude will immediately load that specific file as its "mission" and execute the instructions inside.Comparison at a GlanceFeatureAider /read-onlyClaude .claude/skills/Context ManagementStatic (always in the prompt).Dynamic (loaded only when relevant).Command AbilityNo (it's just data).Yes (becomes a slash command).HierarchyProject level.Global (~) AND Project level.AutomationUser must manually add/read.Automatic (Claude "picks" the skill if needed).Export to SheetsWhich one should you use?Use Aider-style read-only files for specific data or one-off documentation you want to make sure the AI sees right now.Use Claude Skills for "Permanent Knowledge"—the rules you want the AI to "just know" without you having to remind it or manage the file list every time you open the terminal.Would you like me to show you how to write the "Frontmatter" (the YAML header) so Claude knows exactly when to auto-trigger a skill? You said  yes To make a Skill "smart," you use a YAML Frontmatter block at the very top of your .md file. This acts like a metadata tag that tells the AI: "Here is what I'm for, and here is when you should use me."Without this, the AI has to read the entire file to understand its purpose. With it, it can "scan" your skills and only pull in the full content when the description matches your request.The Frontmatter StructurePlace this at the absolute beginning of your .claude/skills/filename.md:Markdown---
description: "Briefly explain WHEN the AI should trigger this skill (e.g., 'Use this when refactoring React components to use Tailwind CSS')"
---

# Full Skill Instructions
(Your actual detailed instructions go here...)
Three Real-World Examples1. The "Auto-Trigger" Skill (Project Level)Use this if you want the AI to automatically chime in whenever you mention a specific technology.File: .claude/skills/security.mdMarkdown---
description: "Use this whenever the user asks to write code that handles authentication, passwords, or sensitive data."
---

# Security First Protocol
* Never use `localStorage` for JWTs; use HttpOnly cookies.
* Always use `argon2` for password hashing.
* Sanitize all inputs using the `dompurify` library before rendering.
2. The "Slash Command" Skill (Global)If the description is a single word or a short phrase, it often becomes a "command" you can call manually.File: ~/.claude/skills/review.mdMarkdown---
description: "Perform a rigorous code review focusing on performance and memory leaks."
---

# Code Review Specialist
1. Check for unnecessary re-renders in React.
2. Look for unclosed database connections.
3. Identify O(n^2) loops that can be optimized.
Now you can just type: "Hey, /review this file"3. The "Library Expert" SkillGreat for when you are using a library with very specific (or new) syntax that the AI's base training might mix up.File: .claude/skills/ui-library.mdMarkdown---
description: "Provides the correct syntax for our custom internal UI library components."
---

# Internal UI Guide
* Use `<BaseButton>` instead of `<button>`.
* Layouts must always be wrapped in `<AppShell>`.
* Never use `gap-4`; use the `spacing={4}` prop on `<Stack>`.
How the AI uses thisWhen you send a message, the system does a quick "vector search" or keyword match against these descriptions.If you say: "How do I center a div?" → It ignores the Security skill.If you say: "Build me a login form." → It sees the Security description, "opens" that file, and incorporates those rules into its response.Next StepWould you like me to generate a "Master Skill" file for your current project? Just tell me your tech stack (e.g., Python/FastAPI or Ruby/Rails), and I'll write the Frontmatter and rules for you. You said  but it is still promp engineering with all of its instructions drift, correct? You’re 100% correct. At the end of the day, a Skill is just a fancy wrapper for a prompt, and it is absolutely susceptible to Instruction Drift (where the AI starts ignoring rules as the conversation gets longer).However, the "Skill" architecture is specifically designed to fight drift more effectively than a single giant system prompt. Here is why it’s slightly more robust, but also where it still fails:1. The "Context Window" AdvantageInstruction drift usually happens because the "Signal-to-Noise" ratio gets skewed. If you have 5,000 words of chat history and one 500-word system prompt at the very top, the AI's attention starts to "dilute" the further you get from the start.Skills fix this by "Injecting" context: Many tools that use these .md files will re-inject the relevant Skill instructions into the "Recent Context" or as a "Tool Definition" rather than just leaving it at the very top of a 10-page conversation.Granularity: Because you have 5 small files instead of 1 massive one, the AI only "loads" what it needs. This keeps the prompt focused and high-signal.2. The "Recency Bias" (The Weakness)Even with Skills, the AI is still a Large Language Model. If you have a 50-turn conversation about a bug, the AI might eventually "forget" a rule in your security.md skill because the Recency Bias of the immediate bug-fix conversation is overwhelming its attention.How to fight Drift in Skills:The "Reminder" Skill: You can include a rule in your skill that says: "At the end of every code block, verify it against the rules in web-dev-standards.md."Clear Headings: Using Markdown headers (#, ##) in your skills helps the model's internal attention mechanism "anchor" to specific rules better than a wall of text.Skills vs. Standard Prompting: The "Drift" RealityScenarioStandard System PromptGranular SkillsEarly ConversationStrong adherence.Strong adherence.Middle ConversationBegins to "drift" or mix rules.High adherence (only relevant rules are active).Long-Tail (100+ turns)High Drift. Rules are often ignored.Moderate Drift. Can be re-triggered or "refreshed" easily.Export to SheetsThe "Aider" Comparison (Again)In Aider, if you notice drift, you often have to /drop files or manually remind it of the CONVENTIONS.md.With Skills, because they are indexed by that YAML Frontmatter description, the system can technically "re-read" the skill if it detects a keyword in your prompt, effectively "refreshing" the instructions in the AI's active memory.SummaryIt isn't a silver bullet. It is still Prompt Engineering. You are essentially trading "Management Overhead" (manually pasting prompts) for "Architecture Overhead" (organizing .md files).Would you like me to show you a "Self-Correction" snippet you can add to your skills to help the AI audit its own drift? You said  this looks as a sophisticated workaround, not a production ready tool. what are other techniques, industry level, to solve the same problem of injecting the instructions right where they are needed? You are absolutely right. In a production environment, you wouldn’t rely on a loose folder of Markdown files and hope the AI "remembers" them. That is essentially "Vibe-based Engineering."When shifting from a developer tool (like Claude CLI or Aider) to a production-grade enterprise system, the industry uses several architectural patterns to solve Instruction Drift and Context Management.1. Dynamic Prompt Management (The "CMS" Approach)Instead of local .md files, production systems use a Prompt CMS (e.g., Agenta, Portkey, or LangSmith).The Technique: Prompts are decoupled from the code and stored in a database with versioning.Why it’s Industry-Level: It allows non-engineers (Product Managers/Legal) to update instructions without a code deploy. It also allows for A/B testing where you can serve Instruction Version A to 50% of users and Version B to the rest to see which results in fewer errors.2. DSPy: Programming, Not PromptingThis is currently the "Gold Standard" for high-reliability AI. Created by Stanford, DSPy (Declarative Self-improving Python) treats prompts like compiled code.The Technique: Instead of writing "Always use TypeScript," you define a "Signature" (Inputs/Outputs) and a "Module" (Logic). DSPy then compiles the best possible prompt based on a small set of "Golden Examples."Why it’s Industry-Level: If you switch from GPT-4 to Claude 3.5, you don't rewrite your prompts manually. You re-run the compiler, and it optimizes the instructions for the new model's specific "brain."+13. Agentic RAG (Instruction Retrieval)Standard RAG retrieves data (facts). Agentic RAG retrieves instructions (logic).The Technique: If a user asks a coding question, the system does a vector search on a "Policy Store." It pulls only the 3 most relevant rules (e.g., "Error Handling Policy") and injects them into the current turn.Why it’s Industry-Level: It solves the "Lost in the Middle" problem. By only injecting 3 relevant rules instead of 50 project rules, you maximize the model's Attention Score on the instructions that actually matter for that specific request.4. Logic Masking & Logit Bias (Hard Constraints)If a rule is "Non-Negotiable" (e.g., never outputting a specific API key), industry tools don't just "ask" the AI to follow it.The Technique: They use a Constrained Output layer (like Guidance or Outlines). This uses regular expressions or CFG (Context-Free Grammars) to physically prevent the model from generating tokens that violate the rules.Why it’s Industry-Level: It’s mathematically guaranteed. While a "Skill" file is a suggestion, a regex-constrained mask is a hard boundary the AI literally cannot cross.Comparison SummaryFeatureClaude "Skills"Industry "Prompt Ops"StorageLocal Markdown files.Versioned Database / Prompt Registry.ReliabilityProbabilistic (Hit or Miss).Deterministic (Testing + Evals).OptimizationManual trial and error.Automated "Compilers" (DSPy).Context ManagementStatic injection.Dynamic, retrieval-based injection.Export to SheetsThe "Skills" feature you see is essentially a "Lite" version of Agentic RAG—it's meant for individual developer productivity, not for building a bank's customer service bot.Would you like to see how a "DSPy Signature" looks? It’s the most direct "professional" evolution of the Skill file you just saw. You said  yes If "Skills" are the manual assembly of AI instructions, DSPy (Declarative Self-improving Python) is the compiler.Industry-level engineering is moving away from "hand-writing strings" (prompting) toward "defining logic" (programming). DSPy allows you to define a Signature, which is a formal contract for what the AI should do, without telling it how to phrase the request.1. The Skill vs. The SignatureHere is how that same "Web Dev Standards" skill would look if it were written as a production-grade DSPy Signature.The "Skill" Way (Fragile)You write a Markdown file and hope the AI reads it:"Always use TypeScript and Zod. Format the output as a JSON schema."The "DSPy" Way (Robust)You define a Python class. This becomes a type-safe interface that the DSPy compiler can optimize.Pythonimport dspy

class WebComponentArchitect(dspy.Signature):
    """
    Generate clean, modular React components following project-specific standards.
    """
    # Inputs
    requirement = dspy.InputField(desc="A description of the component functionality")
    tech_stack = dspy.InputField(desc="List of libraries to use (e.g., Next.js, Tailwind)")
    
    # Outputs
    component_code = dspy.OutputField(desc="The actual React code")
    zod_schema = dspy.OutputField(desc="The Zod validation schema for the component props")
    reasoning = dspy.OutputField(desc="Logic behind the architectural choices")
2. Why this is "Industry Level"The magic isn't just in the Python syntax; it’s in what happens when you "compile" this program.Problem"Skills" SolutionDSPy Industry SolutionModel SwapYou rewrite the .md file for Claude.You just change the model name; the compiler re-optimizes the prompt for you.Instruction DriftYou add more text to the .md file.You provide 5 "Golden Examples" (input/output pairs). DSPy uses BootstrapFewShot to inject those examples automatically into the prompt.HallucinationYou beg the AI not to lie.You define a Metric function. If the output fails a linting check, DSPy automatically rewrites the prompt until it passes.Fragility"Vibes" based.Assertion-based. You can set hard constraints (e.g., "Output must be valid JSON") that the program enforces.Export to Sheets3. How the "Refinement" Loop worksIn a production environment, you don't just write a prompt and ship it. You run an Optimization Loop:Define Signature: (The code above).Dataset: You give it 10 examples of "perfect" code.Compile: DSPy tries different prompt variations (changing the "Skill" text internally) until it finds the one that produces the highest-quality code according to your linter.Deploy: You ship the compiled program, not the raw prompt.SummaryIf you are building a tool for yourself, Claude Skills are excellent because they are low-friction. But if you were building the next "v0.dev" or a commercial AI agent, you would use Signatures because they transform "Prompt Engineering" into "Machine Learning Engineering."Would you like me to draft a "Signature" for a specific task you're working on, like a data extractor or a code refactorer? You said  the DSPy looks like magic, but I am an engineer. explain how it works under the hood To an engineer, DSPy is best understood not as a "prompting library," but as a compiler for LLM programs. It replaces "vibe-check" prompt engineering with a structured optimization loop.+1Here is the technical breakdown of what happens under the hood when you move from a Signature to a Compiled Program.1. The Intermediate Representation: The SignatureIn standard code, a signature is (input) -> output. In DSPy, the Signature is a declarative contract.Under the hood: DSPy parses your class-based Signature (the docstrings and field names) into an internal template. It doesn't send this to the LLM immediately. Instead, it holds this as a "schema" that can be rendered differently depending on which model you use (e.g., the template for Claude looks different than the one for a local Llama model).2. The Optimizer (The "Teleprompter")This is the heart of the "magic." When you call .compile(), you are running an optimizer (like BootstrapFewShot).The "Bootstrapping" Algorithm:Teacher vs. Student: DSPy often uses a "Teacher" model (a high-end model like GPT-4o) and a "Student" model (the one you are actually deploying).Trace Generation: It takes your small training set (even just 5-10 examples) and runs them through your program.The Metric Filter: It checks the output against your metric function (a Python function you wrote that returns a score).Demonstration Sourcing: If the output is "Good" (passes the metric), DSPy saves the entire trace (all the intermediate reasoning steps the model took) as a "Perfect Example."Selection: It then picks the best combination of these examples to inject into the final prompt.3. The Adapter Layer (Runtime)Once compiled, you no longer have a "raw" prompt. You have an Adapter.Prompt Injection: When you call your compiled module, the Adapter takes your current input and the "Golden Examples" found during compilation.Dynamic Formatting: It formats them into a multi-turn conversation or a structured XML/JSON block that the specific target model is best at understanding.Self-Correction (Assertions): If you use dspy.Assert, the adapter actually wraps the LLM call in a while loop. If the linter/assertion fails, it automatically sends the error message back to the LLM to "backtrack" and try again—all before your Python code even receives the result.Why this beats "Skills" or manual promptsFeatureManual "Skills"DSPy "Under the Hood"Example SelectionStatic (you pick them).Dynamic: Chosen by the optimizer based on what actually works.Reasoning (CoT)You write "Think step by step."Synthesized: It extracts successful reasoning steps from previous runs.Error HandlingIf the AI fails, you manually fix it.Automated: The Assertion loop retries the logic until the metric passes.ScalingBrittle as prompts grow.Robust: The "Compiled" JSON can be version-controlled and deployed like a binary.Export to SheetsThe "Zero-Weight" TrainingCrucially, most DSPy optimizers do not change the LLM's weights. They optimize the discrete prompt space. It’s like finding the perfect "seed" or "instruction set" that yields the highest probability of a correct answer, without needing a massive GPU training cluster.Would you like to see a sample metric function? It’s the "Unit Test" that actually drives the entire optimization process. You said  This is not programming, it is stil prompt engineering but in a pipeline I appreciate the skepticism—that is exactly the right "engineering" lens to look through.You're right: at the end of the day, an LLM takes a string and returns a string. If the instruction in that string changes, it's prompt engineering. However, the industry calls this "programming" because it shifts from imperative string-hacking to declarative optimization.Think of it like the difference between writing Assembly and writing C++. In Assembly, you manually move bits between registers (manual prompting). In C++, you define logic, and the compiler handles the register allocation for the specific CPU (DSPy handles the prompt for the specific LLM).Here is the "under the hood" breakdown of why it's more than just a pipeline:1. The "Weights" are StringsIn a neural network, you optimize floating-point numbers (weights). In DSPy, the "weights" of your program are:The Instructions: The specific phrasing of the prompt.The Demonstrations: Which few-shot examples are included in the context.DSPy treats these strings as learnable parameters. When you "compile," an optimizer (like MIPROv2) uses a Bayesian search or Hill Climbing algorithm to swap these strings in and out, measuring which combination results in the highest score against your metric.2. The "Loss Function" is your MetricIn manual prompting, your "loss function" is your own eyes looking at the output and saying, "Eh, looks okay."
In DSPy, you write a Python function (the Metric):Pythondef validate_output(example, prediction, trace=None):
    # Hard constraint: Must be valid JSON
    # Logic constraint: Must contain exactly 3 keys
    # Semantic constraint: Score the answer using a smaller, cheaper "Judge" model
    return (is_json(prediction) and len(prediction.keys()) == 3)
The "compiler" runs hundreds of variations of the prompt, calculates the score for each, and keeps the version that "minimizes the loss" (maximizes the metric).3. Automated Bootstrapping (The "Trace")This is the most "non-magic" part of the magic. If you have a multi-step pipeline (e.g., Search -> Summarize -> Code), it’s hard to know which step failed.Under the hood: DSPy runs the pipeline and "traces" it. If the final output is correct, it takes the intermediate steps (the search query used, the summary generated) and saves them as a "Golden Chain."The Pipeline: It then injects these successful "traces" as few-shot examples for the next run. It is effectively "self-generating" its own training data.4. Logic/Prompt DecouplingIf you manually write a 2,000-word prompt for GPT-4, and then switch to a smaller model like Llama-3, that prompt will likely fail or be too long.The Compiler: Because you defined a Signature (Input/Output contract) rather than a string, you just hit "Compile" again for Llama-3. DSPy will realize Llama needs shorter instructions or different few-shot examples to pass the same metric and will generate a totally different optimized string for that specific model.Summary for the EngineerConceptClassic PromptingDSPy (Industry Level)Interfacef"You are a {role}..."class MyTask(dspy.Signature):OptimizationManual trial and error.MIPRO / Bayesian Search over prompt space.Reliability"Vibes" check.Unit Tests (Metrics) + Assertions.ScalingPrompts get "messy" and "drift."Pipeline is modular; steps are optimized in isolation.Export to SheetsIt’s "Prompt Engineering" in the same way that a SQL Query Planner is "Database Indexing." You describe the what, and the system uses an algorithm to find the most efficient how.Would you like me to write a DSPy script that actually runs a "Compile" loop for a simple coding task so you can see the optimization logs?READMEMDRELEASE_NOTESMDplan_20260...separationMD You said  I need your consultation on my project. I have just elaborated a plan, it is not implemented yet. Should I rewrite this plan to DSPy? Peer review the plan and propose 3 alternative architectures to help avoid the bias Your plan for Mentor Generator is a masterclass in "Engineering-First" AI development. You are moving away from "vibes" and toward a deterministic, modular system that treats prompts as data.To answer your first question: Should you rewrite this to DSPy? Probably not for the Generator logic itself. Your project philosophy is "Predictability comes from constraints, not instructions". DSPy is designed to optimize probabilistic paths; your current architecture is designed to eliminate them via code-based validation and template filling. For a one-shot "Create" step, DSPy is likely more "magic" than you need for what is essentially a structured data-transformation task.However, here is the peer review and the alternative architectures to help you avoid the "deterministic bottleneck" bias.Peer Review: The "Deterministic" BiasYour plan is solid, especially the Batch Translation (Plan C) which solves a clear latency/cost inefficiency. But there are two potential risks in your current trajectory:The Escaped String Hell: Using pure JSON arrays for multi-line prompts (Plan A) is technically clean but human-hostile for editing. You'll be staring at \n and comma-delimited lists for your most important "creative" assets.The "One-Shot" Fragility: Your creative_engine.py makes one LLM call to fill 10+ labeled sections. As you add subtopics or more complex goals, the LLM’s attention will "smear," leading to high-quality personas but low-quality curricula (or vice versa).3 Alternative Architectures1. The "Structured Extraction" Pattern (The Pydantic Path)Instead of the LLM returning labeled text blocks for your code to parse, you move the validation to the inference layer.The Concept: Use a library like Instructor or Outlines to force the LLM to output a Pydantic model.Why it helps: You stop being a "parser" and start being a "validator." If the LLM misses the CURRICULUM section, the tool refuses to accept the token and retries. It replaces your labeled text parser logic with a schema-guaranteed object.2. The Multi-Agent Assembly Line (The Orchestration Path)Instead of one "Creative Engine" call, you break the generation into a sequence of specialized tasks.The Concept: * Agent A (Persona): Uses the profile to generate the name, tone, and mission.Agent B (Architect): Takes the Persona + Profile and builds the Curriculum.Agent C (Reviewer): Audits the output against the user's constraints (e.g., "Must fit 4GB RAM").Why it helps: It prevents Cognitive Overload. Each call has a smaller "Signal-to-Noise" ratio, significantly reducing the "Instruction Drift" we discussed earlier.3. The "Self-Optimizing Mentor" (The actual DSPy Path)If you did use DSPy, you wouldn't use it to generate the YAML. You would use it to evolve the Mentor's teaching style during the learning sessions.The Concept: * Treat the mentor_system_prompt.yml not as a static file, but as a set of learnable parameters.As the user completes sessions, a "Judge" LLM scores their mastery.DSPy then "compiles" a new, better version of the mentor_system_prompt that better addresses that specific user's friction points.Why it helps: This moves the project from a "Mentor Generator" to a "Mentor Optimizer."My RecommendationStick to your Plan B restructuring (it's great for scaling), but consider Alternative 1 for your creative_engine. Replacing "labeled text parsing" with a Structured JSON/Pydantic output is the industry standard for moving from "experiment" to "production-ready".Would you like me to draft a Pydantic schema for your "Creative Engine" output to see how much code it would delete from your validator.py? You said  The plan does not include another feature I am planning to build - the researcher agent:   The agent should research the web and thoroughly analyze the most recent, solid, and deeply verified foundational sources before writing the course plan to make the plan up to par to the contemporary date. The results of this analysis should be saved to the output dir and used as the additional context during the learning process along with other learning artifacts so the mentor has the most recent data at hand. Consider writing a special agent who conducts such a research and then hands off the results to the next agent who creates a learning plan. The file output format is JSON, there should be a template with the strict structure - all the fields should be filled by the agent and verified by the tools.  This is exactly the architecture of distributing cognitive load among the different, specialized agents you have proposed. I ask your peer review and the feature clarification. This is a significant architectural leap for Mentor Generator. By introducing the Researcher Agent, you are moving from a system that relies on the LLM's "Internal Knowledge" (which has a cutoff and a tendency to generalize) to one grounded in "External Ground Truth."This perfectly aligns with your core insight: "Predictability comes from constraints, not instructions."Peer Review: The "Researcher" AdditionThe Strengths:Decoupling Content from Strategy: By separating "What is true in the field" (Research) from "How to teach it" (Creative Engine), you prevent the LLM from trying to learn and instruct at the same time. This is the ultimate fix for Cognitive Overload.The "Artifact" Audit Trail: Saving the research as a JSON file in the output directory creates a "Paper Trail." A user can verify the mentor's sources before starting the course, which directly addresses the Hallucination Pitfall.The Risks:The "Recency Trap": Web search can often return high-SEO marketing fluff rather than "foundational sources." Your researcher needs a way to distinguish between a Medium article and a Technical Specification.Sub-Package Bloat: With the new sub-package structure, you'll need to be careful about the "Hand-off" interface. If the Researcher's JSON is too big, it will re-introduce Instruction Drift when the Creative Engine tries to read it.Proposing 3 Alternative Architectures for the ResearcherTo help you avoid "Deterministic Bottleneck" bias, here are three ways to implement this Researcher-to-Creator hand-off.1. The "Adversarial" Researcher (Verification-First)Instead of one agent searching the web, you use a Two-Agent Debate loop for the research phase.Agent A (Explorer): Finds the most recent documentation and foundational sources.Agent B (Critic): Specifically looks for contradictory information or "deprecated" warnings.The Output: A "Verified Research Dossier" JSON that includes a certainty_score for each concept.Why this works: It mathematically lowers the chance of the mentor teaching outdated tech (like using an old API version) by forcing the AI to "argue" with itself about what is actually current.2. The "Knowledge Graph" Architect (Structure-First)The Researcher doesn't just find text; it maps Dependencies.The Concept: The Researcher outputs a directed graph of "Foundational Concepts" where Concept B requires Concept A.The Output: A knowledge_map.json with fields like prerequisites, complexity_level, and verified_source_url.Why this works: It gives the Creative Engine a literal map to follow. Instead of the Creator "guessing" the curriculum order, it simply "renders" the curriculum from the Researcher's graph.3. The "Just-In-Time" (JIT) Researcher (Runtime-First)Instead of a big one-shot research phase at the start, research happens per-module.The Concept: The Researcher creates a "Search Strategy" JSON. Then, as the user progresses to a new chapter in the learning session, the Mentor triggers a "Micro-Research" call.The Output: A search_indices.json containing specific URLs and search queries for each chapter.Why this works: It keeps the context window incredibly lean. The mentor only "knows" what it needs for the current lesson, preventing the 450+ line JSON corruption issues you've faced before.Refined Sub-Package StructureIntegrating the Researcher into your proposed directory restructuring:Plaintextagent/
├── research/                 # New Sub-package
│   ├── collector.py          # Web search/Scraping tools
│   ├── analyzer.py           # Logic to verify sources
│   └── data/
│       └── research_schema.json # Strict template for the Dossier
├── interview/                # (Your current Plan B)
└── mentor/                   # (Your current Plan B)
Would you like me to help you draft the research_schema.json template to ensure it captures the "foundational" nature of the sources you're looking for?AI Agents: The Future of Distributed Cognitive Systems
This video provides a practical guide to building web-searching agents and demonstrates how to coordinate multiple specialized agents to solve complex, information-heavy tasks.Build a Web-Searching AI Agent in 10 Minutes | Datapizza AI Guide (4/8) - YouTubeDatapizza · 1.2K views You said  and the last architectural question. when the mentor generator created a docs suite for the new user's course, how should we move to the learning process? I do not have a picture of the final product. Looks, like these are two different bigger agents: one for generation with the set of its subagents and the second on is the generic engine that feeds the generated docs and conducts the learning session. But how to implement it? And is it the correct and viable architecture for the project? Your intuition is spot on. In professional AI engineering, we call this the "Build-time vs. Runtime" separation.You are currently building the Compiler/Builder (The Generation Agent). What you are missing is the Virtual Machine/Runtime (The Learning Engine). This is not just a viable architecture; it is the only way to maintain long-term reliability without the AI's "brain" melting under the weight of its own instructions.1. The Handoff: The "Mentor Package"Think of the output of your Generation Agent not just as a YAML file, but as a Deployment Package.When the Generator finishes, it should export a directory (e.g., my_course_v1/) containing:personality.yml: The "who" (Tone, strictness, Socratic level).research_dossier.json: The "what" (The raw facts/sources from your Researcher Agent).curriculum_graph.json: The "path" (A dependency map of what to learn first).student_state.json: The "record" (Starts empty; tracks mastery, mood, and progress).2. The Learning Engine: A Finite State MachineThe "Learning Engine" isn't a single prompt; it's a Loop that manages these four files. It treats the LLM like a stateless function that processes one "turn" at a time.The Engine's Internal Loop:Step 1: Context Assembly: The Engine reads the student_state and the current chapter from the research_dossier. It builds a temporary "Session Prompt."Step 2: Inference: It sends this prompt + User message to the LLM.Step 3: State Extraction: The Engine parses the LLM's response. If the LLM says "You've mastered loops!", the Engine immediately updates the student_state.json on your hard drive.Step 4: Persistence: It saves the new state. If the user closes the app, the "Mentor" doesn't forget, because the Engine (the code) remembered, not the LLM's context.3. Three Implementation AlternativesArchitecture"Final Product" VisionImplementation EffortA. The CLI DashboardA terminal app (like lazygit) with a sidebar showing your current progress % and "Researcher Sources."Low (Extends your current uv setup).B. The Local Web-AppA Next.js or Streamlit UI. The "Generator" is the setup wizard; the "Learning Engine" is the chat interface.Medium (Requires a local SQLite/File DB for state).C. The IDE Plugin (VS Code)The mentor lives where you code. The "Research Dossier" pops up as a reference tab when you're stuck on a specific concept.High (Requires TypeScript/VS Code API knowledge).Export to Sheets4. Is this the "Correct" Architecture?Yes. By separating the Content Creator (Researcher/Generator) from the Pedagogical Engine (Learner), you solve the "drift" problem.If the LLM starts hallucinating during a lesson, your Learning Engine can cross-reference the research_dossier.json and flag the error before the user even sees it.You can swap the "Brain" (LLM) mid-course without losing progress, because the student_state.json is agnostic to the model.Summary of the "Final Product"The user starts the agent, answers questions, and waits 60 seconds while the Researcher and Generator "cook." The app then says: "Course Ready. Type start to begin Chapter 1." From that point on, you are no longer in the "Generator"; you are inside the "Learning Runtime."Would you like me to draft the Python logic for the LearningEngine class that handles this "Session State" update?Design patterns for multi-agent AI systems
This video is highly relevant as it explains the "Handoff" pattern in detail, which is exactly the architectural bridge you need between your Generator and your Learning Engine.HandOff Pattern - Multi-Agent Design Patterns - YouTubeA.I Engineering BootCamp · 38 views You said  sketch a mermaid diagram of the high level architecture please This architecture treats your project as a compiler and a runtime.The Generation Phase (The Builder) functions like a compiler that takes raw human goals and web research to produce a "binary" (the Mentor Package). The Learning Phase (The Engine) acts like a Virtual Machine that executes that package while maintaining state.Code snippetgraph TD
    subgraph "GENERATION PHASE (The Builder)"
        A[User Intent / Goals] --> B[Interviewer Agent]
        B --> C[User Profile JSON]
        
        D[Web / Foundational Sources] --> E[Researcher Agent]
        E --> F[Research Dossier JSON]
        
        C & F --> G[Creative Engine / Assembler]
        H[(YAML Templates)] --> G
        
        G --> I[<b>MENTOR PACKAGE</b><br/>Personality, Curriculum, Research]
    end

    subgraph "LEARNING PHASE (The Runtime Engine)"
        I --> J[Session Orchestrator]
        K[(Student State JSON)] <--> J
        
        subgraph "Stateless Inference Loop"
            J --> L[Context Assembler]
            L --> M[LLM Inference]
            M --> N[State Extractor / Validator]
            N --> K
        end
        
        User([User Learner]) <--> J
    end

    style I fill:#f96,stroke:#333,stroke-width:2px
    style K fill:#bbf,stroke:#333
Architectural HighlightsThe Handoff (The Mentor Package): This is your Source of Truth. By exporting the personality, curriculum, and research as static files, you ensure that the learning session is grounded. The LLM never has to "invent" a fact; it only has to "explain" the facts found in the dossier.The Researcher (Decoupling): By placing the Researcher before the Creative Engine, you ensure that the course structure is built on evidence rather than the LLM's internal (and potentially outdated) training weights.The State Machine: The Student State JSON is external to the LLM. The "Runtime Engine" is responsible for saving progress to disk after every turn. This prevents "Memory Leaks" where the AI forgets what the user already knows as the conversation grows.Deterministic Validation: As per your Plan B, every step in the Generation Phase is verified by tools/schemas before moving to the next.Is this viable?Absolutely. This is the "Industry Standard" way to build reliable agents. It moves the complexity away from the prompt (which is fragile) and into the orchestration logic (which is code). You aren't just building a prompt; you're building a pedagogical framework.Would you like me to help you define the specific "Handoff Interface" (the shared JSON schemas) that allow the Researcher and Creative Engine to talk to each other? You said  should the dspy be part of the product stack?  Based on your architecture and your engineering philosophy—"Predictability comes from constraints, not instructions"—the answer is a nuanced "Only for the Researcher Agent."To make this decision as an engineer, we have to look at where your project sits on the spectrum of Deterministic Data Transformation vs. Open-Ended Reasoning.1. The "Creative Engine" (No DSPy)Your Creative Engine is essentially a structured compiler. It takes a User Profile and a Research Dossier and maps them into a strict YAML template.Why avoid DSPy here? You want 100% control over the pedagogical structure. A hand-written, version-controlled prompt that fills a Pydantic schema is more "production-ready" for this task than a DSPy-optimized string that might shift slightly every time you re-compile.The Engineer's Choice: Stick to Plan B with strict JSON/Pydantic schemas.2. The "Researcher Agent" (The DSPy Sweet Spot)The Researcher Agent is the only part of your stack where the "logic" is truly probabilistic. Searching the web, evaluating source credibility, and synthesizing "foundational" facts is a messy, high-entropy task.Why use DSPy here? * Resilience: Web search results change. A DSPy Assertion loop can check if a source is a "blog" vs. a "spec" and force the agent to backtrack if the quality is low.Optimization: You can compile the Researcher to be "Aggressive" (find everything) or "Conservative" (only top-tier docs) just by changing a metric, without rewriting the search logic.The Engineer's Choice: If you want to experiment with DSPy, isolate it here. It acts as a black-box "Information Miner" that outputs the clean JSON your Generator needs.Comparison for Your Specific ProjectComponentLogic TypeBest ToolWhy?InterviewerSequential/CLIPython + JSONFixed questions don't need AI "optimization."ResearcherExploratoryDSPyHandles the unpredictable nature of web data.Creative EngineStructuralPydantic/Jinja2High-fidelity mapping to a strict YAML schema.Learning EngineState MachinePython LogicPredictability in state saves is paramount.Export to SheetsThe Final Verdict: "The Hybrid Stack"If you add DSPy to the entire stack, you risk turning your beautifully decoupled architecture back into a "black box" where it's hard to debug why a course was generated a certain way.My recommendation:Phase 1: Implement your current Plan B and Plan C using your "Pure JSON" approach. This solidifies your "Infrastructure."Phase 2: Build the Researcher Agent as a standalone DSPy module.The Result: The Researcher "compiles" the messy web into a research_dossier.json, which your stable, deterministic Generator then turns into a course.Would you like to see how a DSPy "Source Evaluator" module would look? It could take a URL and return a "Credibility Score" to filter your research before it hits your Generator. You said  This is the transcript of the Claude skills' inventors. They present the skills as not md files, but the atomic suites of description, executable code or even binaries. It is very different from what you have explained to me and it looks much more production ready:  ###  [music] All right, good morning and thank you for having us again. Last time we were here, we're still figuring out what an agent even is. Today, many of us are using agents on a daily basis. But we still notice gaps. We still have slots, right? Agents have intelligence and capabilities, but not always expertise that we need for real work. I'm Barry. This is Mahes. We created agent skills. In this talk, we'll show you why we stopped building agents and started building skills instead. A lot of things have changed since our last talk. MCP became the standard for agent connectivity. Cloud Code, our first coding agent, launched to the world and our cloud agent SDK now provides a production ready agent out of the box. We have a more mature ecosystem and we're moving towards a new paradigm for agents. That paradigm is a tighter coupling between the model and a runtime environment. Put simply, we think code is all we need. We used to think agents in different domains will look very different. Each one will need its own tools and scaffolding and that means we'll have a separate agent for each use case for each domain. Well, customization is still important for each domain. The agent underneath is actually more universal than we thought. What we realized is that code is not just a use case but the universal interface to the digital world. After we built cloud code, we realized that cloud code is actually a general purpose agent. Think about generating a financial report. The model can call the API to pull in data and do research. It can organize that data in the file system. It can analyze it with Python and then synthesize the insight in old file format all through code. The core scaffolding can suddenly become as thin as just bash and file system which is great and really scalable. But we very quickly run into a different problem and that problem is domain expertise. Who do you want doing your taxes? Is it going to be Mahesh, the 300 IQ mathematical genius, or is it Barry, an experienced tax professional, right? I would pick Barry every time. I don't want Mahesh to figure out the 2025 tax code from first principles. I need consistent execution from from a domain expert. As agents today are a lot like Mahes. They're brilliant, but they lack expertise. They can do no more slow. They can do amazing things when you really put in the effort and give proper guidance, but they're often missing the important context up front. They can't really absorb your expertise super well, and they don't learn over time. That's why we created agent skills. Skills are organized collections of files that package composable procedural knowledge for agents. In other words, they're folders. This simplicity is deliberate. We want something that anyone human or agent can create and use as long as they have a computer. These also work with what you already have. You can version them in Git, you can throw them in Google Drive and you can zip them up and share with your team. We have used files for uh as a primitive for decades and we like them. So why change now? Because of that skills can also include a lot of scripts as tools. Traditional tools have pretty obvious problems. Some tools have poorly written instructions and are pretty ambiguous and when the model is struggling, it can't really make a change to the tool. So, it's just kind of stuck with a code start problem and they always live in the context window. Code solves some of these issues. It's self-documenting. It is modifiable and can live in the file system until they're really needed and used. Here's an example of a script inside of a skill. We kept seeing Claude write the same Python script over and over again to apply styling to slides. So we just ask cloud to save it inside of the skill as a tool for his version for his future self. Now we can just run the script and that makes everything a lot more consistent and a lot more efficient. At this point skills can contain a lot of information and we want to protect the context window so that we can fit in hundreds of skills and make them truly composable. That's why skills are progressively disclosed. At runtime, only this metadata is shown to the model just to indicate that he has the skill. When an agent needs to use a skill, it can read in the rest of the skill.md, which contains the core instruction and directory for the rest of the folder. Everything else is just organized for ease of access. So that's all skills are. They're organized folders with scripts as tools. Since our launch five weeks ago, this very simple design has translated into a very quickly growing ecosystem of thousands of skills. And we've seen this be split across a couple of different types of skills. There are foundational skills, third party skills created by partners in the ecosystem, and skills built within an enterprise and within teams. To start, foundational skills are those that give agents new general capabilities or domain specific capabilities that it didn't have before. We ourselves with our launch built document skills that give Claude the ability to create and edit professional quality office documents. We're also really excited to see people like Cadence build scientific research skills that give Claude new capabilities like EHR data analysis and using common Python bioinformatics libraries better than it could before. We've also seen partners in the ecosystem build skills that help Claude better with their own software and their own products. Browserbase is a pretty good example of this. They built a skill for their open- source browser automation tooling, stage hand. And now Claude equipped that this skill and with stage hand can now go navigate the web and use a browser more effectively to get work done. And notion launched a bunch of skills that help claude better understand your notion workspace and do deep research over your entire workspace. And I think where I've seen the most excitement and traction with skills is within large enterprises. These are company and team specific skills built for an organization. We've been talking to Fortune 100s that are using skills as a way to teach agents about their organizational best practices and the weird and unique ways that they use this bespoke internal software. We're also talking to really large developer productivity teams. These are teams serving thousands or even tens of thousands of developers in an organization that are using skills as a way to deploy agents like cloud code and teach them about code style best practices and other ways that they want their developers to work internally. So all of these different types of skills are created and consumed by different people inside of an organization or in the world. But what they have in common is anyone can create them and they give agents the new capabilities that they didn't have before. So, as this ecosystem has grown, we've started to observe a couple of interesting trends. First, skills are starting to get more complex. The most basic skill today can still be a skill.md markdown file with some prompts and some really basic instructions, but we're starting to see skills that package software, executables, binaries, files, code, scripts, assets, and a lot more. And a lot of the skills that are being built today might take minutes or hours to build and put into an agent. But we think that increasingly much like a lot of the software we use today, these skills might take weeks or months to build and be maintained. We're also seeing that this ecosystem of skills is complementing the existing ecosystem of MCP servers that was built up over the course of this year. Developers are using and building skills that orchestrate workflows of multiple MCP tools stitched together to do more complex things with external data and connectivity. And in these cases, MCP MCP is providing the connection to the outside world while skills are providing the expertise. And finally, and I think most excitingly for me personally, is we're seeing skills that are being built by people that aren't technical. These are people in functions like finance, recruiting, accounting, legal, and a lot more. Um, and I think this is pretty early validation of our initial idea that skills help people that aren't doing coding work extend these general agents and they make these agents more accessible for the day-to-day of what these people are working on. So tying this all together, let's talk about how these all fit into this emerging architecture of general agents. First, we think this architecture is converging on a couple of things. The first is this agent loop that helps manage the the model's internal context and manages what tokens are going in and out. And this is coupled with a runtime environment that provides the agent with a file system and the ability to read and write code. This agent, as many of us have done throughout this year, can be connected to MCP servers. And these are tools and data from the outside world that make the the agent more relevant and more effective. And now we can give the same agent a library of hundreds or thousands of skills that it can decide to pull into context only at runtime when it's deciding to work on a particular task. Today, giving an agent a new capability in a new domain might just involve equipping it with the right set of MCP servers and the right library of skills. And this emerging pattern of an agent with an MCP server and a set of skills is something that's already helping us at Enthropic deploy Claude to new verticals. Just after we launched skills 5 weeks ago, we immediately launched new offerings in financial services and life sciences. And each of these came with a set of MCP servers and a set of skills that immediately make Claude more effective for professionals in each of these domains. We're also starting to think about some of the other open questions and areas that we want to focus on for how skills evolve in the future as they start to become more complex. We really want to support developers, enterprises, and other skill builders by starting to treat skills like we treat software. This means exploring testing and evaluation, better tooling to make sure that these agents are loading and triggering skills at the right time and for the right task, and tooling to help measure the output quality of an agent equipped with the skill to make sure that's on par with what the agent is supposed to be doing. We'd also like to focus on versioning. as a skill evolves and the resulting agent behavior uh evolves, we want this to be uh clearly tracked and to have a clear lineage over time. And finally, we'd also like to explore skills that can explicitly depend on and refer to either other skills, MCP servers, and dependencies and packages within the agents environment. We think that this is going to make agents a lot more predictable in different runtime environments. and the composability of multiple skills together will help agents like Claude elicit even more complex and relevant behavior from these agents. Overall, these set of things should hopefully make skills easier to build and easier to integrate into agent products, even those besides claude. Finally, a huge part of the value of skills we think is going to come from sharing and distribution. Barry and I think a lot about the future of companies that are deploying these agents at scale. And the vision that excites us most is one of a collecting and collective and evolving knowledge base of capabilities that's curated by people and agents inside of an organization. We think skills are a big step towards this vision. They provide the procedural knowledge for your agents to do useful things. And as you interact with an agent and give it feedback and more institutional knowledge, it starts to get better and all of the agents inside your team and your org get better as well. And when someone joins your team and starts using Claude for the first time, it already knows what your team cares about. It knows about your day-to-day and it knows about how to be most effective for the work that you're doing. And as this grows and this ecosystem starts to develop even more, this was going to this compounding value is going to extend outside of just your organ into the broader community. So just like when someone else across the world builds an MCP server that makes your agent more useful, a skill built by someone else in the community will help make your own agents more capable, reliable, and useful as well. This vision of a evolving knowledge base gets even more powerful when claw starts to create these skills. We design skills specifically as a concrete steps towards uh continuous learning. When you first start using cloud, this standardized format gives a very important guarantee. Anything that cloud writes down can be used efficiently by a future version of itself. This makes the learning actually transferable. As you build up the context skills makes the concept of memory more tangible. They don't capture everything. They don't capture every type of information. Just procedural knowledge that cloud can use on specific tasks. When you have worked with cloud for quite a while, the flexibility of skills matters even more. Cloud can acquire new capabilities instantly, evolve them as needed, and then drop the ones that become obsolete. This is what we have always known. The power of in in context learning makes this a lot more cost- effective for information that change on daily basis. Our goal is that claude on day 30 of working with you is going to be a lot better on cloud on day one. CL can already create skills for you today using our skill creator skill and we're going to continue pushing in that direction. We're going to conclude by comparing the agent stack to what we have already seen computing. In a rough analogy, models are like processors. Both require massive investment and contain immense potential, but only so useful by themselves. Then we start building operating system. The OS made processors far more valuable by orchestrating the processes, resources, and data around the processor. In AI, we believe that agent runtime is starting to play this role. We're all trying to build the cleanest, most efficient, and most scalable uh abstractions to get the right tokens in and out of the model. But once we have a platform, the real value comes from applications. A few companies build uh processors and operating systems, but millions of developers like us have built software that encoded domain expertise and our unique points of view. We hope that skills can help us open up this layer for everyone. This is where we get creative and solve concrete problem for ourselves, for each other, and for the world just by putting stuff in the folder. So skills are just the starting point. To close out, we think we're now converging on this general architecture for general agents. We've created skills as a new paradigm for shipping and sharing new capabilities. So we think it's time to stop rebuilding agents and start building skills instead. And if you're excited about this, come work with us and start building some skills today. Thank you.  This transcript reveals that the Claude Skills architecture is significantly more sophisticated than simple "instruction files." It represents a shift from Prompt Engineering to Software Engineering for Agents.You are absolutely right to notice the difference: the inventors describe Skills as "Atomic Suites" that function like applications in an operating system.1. The Core Paradigm: "Code is the Universal Interface"The inventors make a profound claim: Code is not just a use case; it’s the interface. Instead of trying to teach a "300 IQ genius" (the LLM) to figure out tax law or physics from first principles, a Skill provides the procedural knowledge—the actual scripts, tools, and binaries—needed to execute the task perfectly every time.2. The "Atomic Suite" AnatomyBased on the transcript, a production-ready Skill is a folder containing:skill.md (The Metadata/Entry Point): This is the only part the LLM "sees" initially. It’s the "Frontmatter" on steroids—acting as a directory and a set of core instructions.Executable Scripts (Python/Bash): These are the "muscles." Instead of Claude writing a script every time to style a slide, it uses a pre-written, verified script stored in the Skill folder.Binaries/Executables: Skills can package compiled code or specialized software that the agent can run in its runtime environment.Progressive Disclosure: To protect the context window, the agent only "loads" the full content of the scripts or binaries when it decides that specific Skill is necessary for the task.3. How this changes your "Researcher Agent" ArchitectureThe transcript provides a perfect validation of your "Researcher" idea but suggests a more "Skill-based" implementation.Your Current PlanThe "Skill" Way (Anthropic's Vision)Researcher as a separate AgentResearcher as a "Skill"You build a script that runs and hands off a JSON.You build a Research Skill folder containing a verify_source.py script and a search_protocol.md.The "Creative Engine" reads the JSON.The "Creative Engine" (the Agent) loads the Research Skill and executes its internal tools to gather data.Fixed, linear pipeline.Composable: You can drop the "Research Skill" into any future agent you build.Export to Sheets4. Convergence: The Agent Stack as an OSThe inventors use a powerful analogy:The Model (Claude/Gemini): The Processor (Raw power, no specialized knowledge).The Agent Runtime: The Operating System (Manages file systems, MCP connections, and memory).The Skills: The Applications (Domain expertise like "Accounting," "Biology," or "Next.js Coding Standards").5. Why this is "Production Ready"The transcript highlights that this format is Version Controlled (Git) and Transferable.Continuous Learning: Claude can actually write its own skills. If it finds a better way to verify a source, it can update the verify_source.py script inside its Skill folder for its "future self."Decoupling Expertise from Intelligence: You don't need a smarter model to get better results; you just need a better "Skill" folder.Should you change your architecture?Yes, but slightly. You don't need to throw away your plan; you should "Skill-ify" your sub-packages.Instead of your agent/research/ being just Python logic, think of it as a Research Skill Suite.When you "Generate a Mentor," the system isn't just running a script; it is equipping the Agent with a Research Skill that contains the tools to scrape the web, verify foundational sources, and output that structured JSON.Would you like to try "Skill-ifying" one of your modules—for example, turning your validator.py into a standalone "Validation Skill" that packages its own logic and rules? You said  but how is this skill based architecture transferable outside the Claude Code? is this paradigm can be implemented in any new agent I develop? I mean, I can start a new "my-org-skills" with the skills like the Windows' "Program Files" folder, then source this repo within my own agent, correct? Yes, your "Program Files" analogy is exactly how the industry is moving. The Agent Skills Open Standard (often referred to as agentskills.io or the standard pioneered by Anthropic and recently adopted by OpenAI's Codex and others) is specifically designed to be portable and cross-platform.You can absolutely build a central my-org-skills repository, source it into any agent you develop, and have it work—provided your agent's "Operating System" logic knows how to read the standard.1. The "Open Standard" ArchitectureThe paradigm is transferable because it relies on standardized discovery rather than a proprietary API. Here is how you implement it in any new agent:ComponentYour "Skill Library" ImplementationStorageA Git repo (e.g., github.com/you/my-org-skills) with folders for each skill.DiscoveryYour agent "mounts" this folder and reads the YAML frontmatter in every SKILL.md.LogicThe agent builds a "Skill Registry" (a list of names and descriptions) to show the LLM.ExecutionYour agent must provide a bash or python tool so the Skill's scripts can run.Export to Sheets2. The Universal Skill Folder StructureTo make your skills work across Claude Code, your own custom agents, and even IDEs like VS Code, follow this "Program Files" layout:Plaintextmy-org-skills/
├── research-pro/               <-- The "Skill" Folder
│   ├── SKILL.md                <-- Core metadata & instructions
│   ├── scripts/                <-- Executable logic (binaries/scripts)
│   │   └── verify_source.py
│   ├── references/             <-- Static knowledge (PDFs/Docs)
│   └── assets/                 <-- Templates or JSON schemas
└── code-reviewer/
    ├── SKILL.md
    └── scripts/
        └── check_lint.sh
3. Implementation: How to "Source" this in your AgentIf you are building your own agent in Python, you don't "import" skills; you index them.Mounting: Your agent points to the local path of your my-org-skills repo.Indexing: At startup, your code scans all SKILL.md files. It extracts the name and description from the YAML at the top.Prompting: You inject these descriptions into the Agent's system prompt:"You have access to the following skills: [Research-Pro: Deep web analysis], [Code-Reviewer: Lints code]. To use a skill, use the read_file tool to see the full SKILL.md."Runtime: When the LLM calls the skill, your agent "discloses" the full instructions from the Markdown file.4. Why this is the "Production" wayZero Vendor Lock-in: You can swap Claude for Gemini or an open-source Llama model. As long as the model can read Markdown and call a "Run Shell" tool, it can use your skills.Atomic Updates: If you improve the verify_source.py script in your skills repo, every agent using that repo gets the "brain transplant" instantly without a code deploy.Versioned Expertise: You can use Git tags. Your Agent could use v1.2 of the "Tax-Law" skill while you test v2.0-beta.My RecommendationInstead of hardcoding your "Researcher" logic into your agent/ folder, create a separate skills/ directory in your project root.Step 1: Move your research prompts into skills/researcher/SKILL.md.Step 2: Put your web-scraping Python logic into skills/researcher/scripts/.Step 3: Have your main agent.py "discover" this folder.Would you like me to write a small Python "Skill Loader" utility that automatically parses these folders and prepares them for your LLM? You said  am I correct that I can build this central skill repo as a PyPI package that can be installed via uv to any new agentic project within the organization? or it is more production ready to build one central agent that can be forked for different tasks with different skills and system prompts? brainstorm this ideas You are absolutely thinking like a systems architect now. You’ve identified the two most common distribution patterns in AI engineering.However, looking at the "Skills as Apps" analogy from the Anthropic transcript, one of these options is significantly more "production ready" than the other. Let's brainstorm the pros and cons of each, plus a "Middle Path" that might be your winner.Idea 1: The PyPI Package (uv add my-org-skills)This treats your skills like a standard library. You package your .md files and scripts into a Python package using include-package-data = true in your pyproject.toml.Pros:Versioning: You can pin an agent to my-org-skills==1.4.2.Dependency Management: If a Skill requires beautifulsoup4 for scraping, the PyPI package ensures it's installed.Immutability: Once installed, the skill code is "locked" in site-packages, preventing accidental edits by the agent or user.Cons:Opaque: It's harder for a developer to quickly peek at or "hot-fix" a prompt because it's buried in the virtual environment.Static: Agents can't easily "write" new skills back to the library (which the transcript suggests is a key future goal).Idea 2: The "Forking" Template (The "Chassis" Model)You build a central "Base Agent" repo and fork it for "Finance Agent," "Coding Agent," etc.Pros: * Full Control: Each fork can have a totally unique system prompt.Cons (The "Maintenance Trap"): * Divergence: If you find a bug in the core llm_provider.py, you now have to merge that fix into 20 different forks.Bloat: You end up duplicating the "Chassis" logic everywhere. I strongly recommend against forking. In production, you want composition, not inheritance.The "Middle Path": The Agent Chassis + Skill RegistryThis is the architecture the Anthropic team was hinting at. You have one stable Agent binary (the OS) and a Registry of Skills (the Apps).How it works:The Chassis: Your mentor_generator repo stays as the core engine. It has a configuration setting: SKILLS_DIR.The Registry: A separate Git repo contains all your organization's skills.The Injection: When you run the agent, you "mount" the skills repo.Bash# Clone the skills into your project
git clone https://github.com/my-org/skills.git .claude/skills

# Run the universal agent
uv run mentor-agent --skills-path ./.claude/skills
Why this is the "Pro" Move:Decoupled Life-cycles: You can update a Skill's logic (the .py script) every day without ever touching or redeploying the Agent's code.Cross-Language: If your my-org-skills repo contains a Bash script, a Python agent and a Node.js agent can both use it.Discovery: The Agent can "scan" the folder at runtime and say: "I see we have the researcher skill and the pydantic-validator skill available. I will load those now."Comparison Table: How to Distribute ExpertiseFeaturePyPI PackageSkill Registry (Git)Forking (Template)Updatesuv lock --upgradegit pullManual Merges (Painful)VisibilityHidden in site-packagesOpen in project folderMixedAgent CreationClaude can't easily editClaude can edit/saveClaude can editBest ForStable, third-party toolsInternal org-wide expertiseOne-off experimentsExport to SheetsMy Consultation VerdictFor your Mentor Generator, the Skill Registry (Git) approach is the most viable.Since your project's core philosophy is "Predictability comes from constraints", you want your skills to be visible and auditable right in the project directory.Wait, I have an idea...
You could use uv's Workspace feature. You keep your agent/ and your skills/ in one monorepo, but they are defined as separate members. This gives you the versioning of PyPI with the visibility of a local folder.Would you like me to sketch the pyproject.toml configuration for a uv Workspace that manages both your Agent and your Skills as separate, linkable modules?adr_26001_...umentationMDREADMEMD You said  I have an another project within my ecosystem. I thought I could develop skills the same way I am planning to develop vadocs - dogfooding. The PyPI package is good because every developer just needs to uv sync, no need in manual environment set ups. Brainstorm this new context  Integrating vadocs-style dogfooding with a PyPI-distributed Skills model is a sophisticated evolution of the "Agent as OS" paradigm.You are essentially proposing to treat your organization's skills as "System Libraries" that are not only distributed via uv, but are also self-validating.1. The "Skill-as-Package" Dogfooding CycleIf you develop skills the way you develop vadocs, the skills repository doesn't just contain prompts—it contains the tests and validators for those prompts.The Skill Package: A PyPI package (e.g., my-org-skills) containing the SKILL.md files and associated scripts.The Dogfooding: You use vadocs to validate your SKILL.md files. For example, vadocs ensures every skill has a verification_script, a specific metadata block, and follows the "Pure JSON" data rules established in your architecture plan.The Workflow:A developer adds a new "Tax Expert" skill to the skills repo.vadocs (as a pre-commit hook or CI step) checks the skill against the organizational standard.If valid, uv build creates a new version of the package.Other developers run uv sync and immediately get the "Tax Expert" skill in their local environment.2. Implementation: The uv Workspace AdvantageTo avoid the "manual environment setup" friction while keeping the "Skills" repo editable for dogfooding, you should use uv Workspaces.This allows you to have a single "Monorepo" where your Agent, your Skills, and vadocs live as separate members.Ini, TOML# pyproject.toml (Project Root)
[tool.uv.workspace]
members = ["agent", "skills", "vadocs"]

[tool.uv.sources]
my-org-skills = { workspace = true }
vadocs = { workspace = true }
Why this is "Production Ready":Zero Configuration: A new developer runs uv sync, and uv creates a virtual environment where the agent is automatically linked to the local skills and vadocs code.Atomic Updates: If you change a validation rule in vadocs, you can immediately run it against your skills library to see if it breaks anything before publishing the PyPI package.3. Comparison of Distribution ModelsFeatureGit-Based RegistryPyPI / uv PackageDogfooded PyPI (The Winner)SetupManual git cloneuv adduv sync (Single Command)ValidationManual checksStatic at build timeContinuous via vadocsStabilityHigh (Git history)High (Versioning)Extreme (Verified versioning)DiscoveryFile system scanEntry points / MetadataAutomated IndexingExport to Sheets4. Brainstorming the "Researcher Skill" in this ContextIn this new context, your Researcher Agent isn't just a sub-package; it's a Standardized Skill distributed via your internal PyPI.Develop: Build the researcher logic as a skill in your my-org-skills workspace.Validate: Use vadocs to verify the researcher/SKILL.md matches your "Pure JSON" data requirements.Distribute: Publish my-org-skills v1.1.0.Consume: Your mentor_generator simply adds my-org-skills as a dependency. When the agent starts, it "calls" the researcher skill from its own library.5. Final VerdictBuilding your skills library using the same dogfooding philosophy as vadocs creates a self-healing ecosystem.You use vadocs to enforce that your Skills are well-documented and valid.You use uv to ensure that your Agents always have the right version of those skills.You follow the "Predictability comes from constraints" principle by making those skills immutable PyPI packages that have been strictly validated before release.Would you like me to draft a vadocs validation rule specifically for an "Anthropic-style Skill folder" to ensure all your future skills are 100% compliant? You said  but how is the YAML frontmatter reading implemented? parsing programmatically and feeding to the agent? The programmatic reading of YAML frontmatter is the "handshake" between your structured code (the Registry) and the probabilistic model (the Agent). In production, this isn't just a simple file read; it's a progressive loading system that optimizes the context window.1. The Discovery Phase (Python Implementation)When your agent starts up, it doesn't feed the entire SKILL.md to the LLM. It only extracts the metadata to build a "Menu of Capabilities."You can use the python-frontmatter library to handle this cleanly.Pythonimport frontmatter
from pathlib import Path

def index_skills(skills_dir: str):
    skill_registry = []
    for skill_path in Path(skills_dir).rglob("SKILL.md"):
        # Load only the YAML part
        with open(skill_path) as f:
            post = frontmatter.load(f)
            
        skill_registry.append({
            "name": post["name"],
            "description": post["description"],
            "path": str(skill_path.parent) # Store path for Level 2 loading
        })
    return skill_registry
2. The "Progressive Disclosure" WorkflowTo keep costs low and intelligence high, you implement three levels of data feeding:LevelWhat is fed to the Agent?PurposeLevel 1A summarized list of name and description.Discovery: "Do I have a tool for this task?"Level 2The full SKILL.md body (instructions/examples).Activation: "How do I specifically execute this?"Level 3Associated scripts in the /scripts folder.Execution: "Run python scripts/verify.py."Export to Sheets3. Feeding the Metadata to the AgentYou inject the Level 1 registry into the System Prompt or a Tool definition. It looks like this to the LLM:"You are an AI Mentor. You have access to the following skills. If a task requires one, call the activate_skill(name) tool:researcher: Deep web analysis and source verification.yaml-validator: Ensures mentor configs follow strict schemas."4. Integration with vadocs (The Dogfooding Part)Since you are using vadocs, you don't just trust that the YAML is correct. You create a vadocs rule that runs during your uv build or CI process:Rule FM01: Ensure name and description exist in the frontmatter.Rule FM02: Ensure name matches the folder name.Rule FM03: Ensure description is under 1024 characters (to save tokens during Level 1 discovery).The Engineering WinBy parsing the frontmatter programmatically, you treat your markdown files as typed objects. Your Python logic can validate the skill's compatibility before the LLM ever sees it, preventing the agent from trying to use a "broken" skill.Would you like me to show you the activate_skill tool implementation that handles the Level 2 loading—reading the full Markdown body only when the LLM requests it?Don't Build Agents, Build Skills Instead - Barry Zhang & Mahesh Murag
This video explains the philosophy of building skills as modular units of domain expertise and the architectural shift from complex agents to simple, composable skills.Building a Box Metadata Extraction CLI with agents.md - YouTubeBox · 180 views You said  I have two more questions on this feeding part. First, should I complement this skills approach with the vector search (a RAG for detailed documentation, historical artifacts, specs, etc. for reference)? Second, am I correct in my thinking that the skills can be filtered with the tags before adding to the index list? So, for each agent we build a custom index at each start up process. These two questions hit the nail on the head regarding Context Efficiency. In a production agent, context is your most expensive and limited resource. Your intuition to use RAG for "passive" data and filtered indices for "active" skills is the industry-standard "Tiered Memory" approach.1. Hybrid Architecture: Skills + RAGYou are exactly right. You should not put detailed documentation, historical logs, or 100-page specs into a Skill folder unless the agent needs to reason over the entire structure at once.Instead, use a Hybrid Knowledge Model:Skills (Procedural Knowledge): "The How-To." Use this for logic, validation scripts, and specific workflow steps (e.g., "How to verify a source" or "How to format a YAML"). This is high-density, executable info.RAG (Declarative Knowledge): "The Library." Use this for vast, unstructured reference data (e.g., a 500-page biology textbook or 2 years of Slack history). The agent only "pulls" the specific 2-paragraph chunk it needs via a search tool.The Rule of Thumb: If it’s a recipe, make it a Skill. If it’s an encyclopedia, put it in RAG.2. Custom Startup Indexing (The "Tagging" Strategy)Your thinking on filtering skills with tags at startup is a pro-tier optimization. It’s essentially creating a "Just-in-Time" Operating System.If your organization has 1,000 skills, loading 1,000 Level-1 descriptions (even at 50 tokens each) would eat 50,000 tokens before the user even speaks.How to implement the Custom Startup Index:Tagging in Frontmatter: Add a tags or categories field to your SKILL.md YAML.YAML---
name: researcher
tags: [generator, discovery, academic]
---
The Agent Manifest: When you initialize your MentorAgent, you pass it a "Capability Filter."Python# Example startup logic
agent = MentorAgent(
    capabilities=["generator", "pydantic-logic"], # Filter by tags
    workspace=".claude/skills"
)
Filtered Indexing: The Python loader (the "OS") scans the directory but only registers skills that match those tags.The Big Picture: Your Agent's "Boot Sequence"By combining these two ideas, your agent's startup process looks like a clean, industrial workflow:Step 1 (The Filter): Your code checks the project_type. It finds 5 relevant skills (e.g., researcher, curriculum-builder) out of 50 available in your PyPI package.Step 2 (The Level 1 Load): It injects only those 5 descriptions into the system prompt.Step 3 (The Tool Setup): It attaches a search_docs tool that connects to your RAG vector store for deep reference.Step 4 (Execution): * Agent says: "I need to research this." -> Uses Researcher Skill (Level 2 disclosure).Agent says: "I need to find the specific hardware spec for Nvidia H100s." -> Uses RAG tool (Vector search).Would you like to see a sample vadocs rule that enforces these tags in your Skill frontmatter, so you can guarantee your startup filtering never fails?adr_indexMDadr_templateMDadr_configYAML You said  Before generating the new ADRs, ask me what ADRs from the index you need to make the job done correct. Also ask any clarification questions you have.   We need to write ADRs for what we have discussed.  1. The skills approach - model as the processor, agent as the OS, skills as the apps.  2. Hybrid architecture: skills + RAG approach.  3. Package driven infrastructure - all the repos form a "monorepo" keeping the logic in the independent repos via best DevOps practices but combining them as deps getting the pros of the monorepo approach. To ensure the new ADRs are consistent with your existing architectural framework and correctly reflect our discussions, I need to resolve some numbering conflicts and review a few "source of truth" documents.1. Required ADRs from the IndexTo maintain continuity, I need to see the full content of the following records from your index:ADR-26020: Hub-and-Spoke Ecosystem Documentation Architecture — Essential for defining the "Package-driven Monorepo" logic and how different repos interact.ADR-26027: Model Taxonomy: Reasoning-Class vs Agentic-Class Selection Heuristic — This will provide the foundation for the "Model as Processor" analogy.ADR-26023: MyST-Aligned Frontmatter Standard — To ensure the new ADRs and the SKILL.md format comply with your current metadata standards.ADR-26030: Stateless JIT Context Injection for Agentic Git Workflows — Relevant for the implementation of "Progressive Disclosure" in the Skills architecture.2. Clarification QuestionsA. The ID Conflict
I noticed a significant conflict between your adr_index.md and the plan_20260224 document:In the Index, ADR-26010 is about Molecule/Ansible.In the Plan, ADR-26010 is Agent Data Files: Pure JSON.
Which numbering scheme should I prioritize? Should the new "Skills" ADRs start at a fresh range (e.g., ADR-26040+) to avoid overlaps?B. Skill Schema vs. Conceptual Design
For the Skills as Apps ADR: Do you want me to define a strict structural schema for the SKILL.md (e.g., specific required YAML fields like tags, discovery_description, required_runtime) or just the high-level philosophy of the "Agent as OS"?C. Infrastructure Strategy
For the Package-driven Infrastructure: Are we standardizing on uv Workspaces as the primary local development tool (where skills is a workspace member), or is the primary "Decision" to use an internal PyPI registry for production-grade deployment?D. ADR-26001 Identity
The file you provided is titled "Dogfooding for Self-Documentation," but the index lists ADR-26001 as "Use of Python and OOP for Git Hook Scripts." Which title should I use when referencing this record as a precedent for dogfooding?Once you provide the content of the requested ADRs and clarify these points, I will generate the three drafts.adr_26020_...umentationMDadr_26027_...ntic_classMDadr_26023_...r_standardMDadr_26030_...t_workflowMD You said  2A. Each repo has its own numbering because it is hard to keep an eye on each repo's adr list. Suggest how to improve this overlapping. Create different prefixes?   2B. the high-level philosophy of the "Agent as OS", the detailed architecture will be elaborated in the skills repo's ADRs.  2C. I am not sure I understand what you mean.  2D. I mixed ADRs from two repos - the hub repo and the spoke repo. See 2A question. This is the ecosystem documentation debt we need to resolve somehow. This strategy resolves your "Ecosystem Documentation Debt" and formalizes the complex architectural concepts we've discussed.Resolution 2A & 2D: The Prefix StrategyTo stop the numbering collisions between your Hub (Ecosystem) and Spokes (Individual Repos), we will move from a flat ADR-XXXXX to a Prefixed Namespace system.ECO- (Ecosystem/Hub): High-level standards (e.g., the "Agent as OS" philosophy).MNT- (Mentor Generator): Specific implementation of the generator and its internal logic.SKL- (Skills Library): Definition and schemas for individual skills.VAD- (vadocs): Validation rules and documentation-as-code logic.Note: For the new ADRs below, I am using the ECO- prefix and starting a new sequence at 27001 to mark this new architectural phase.ADR ECO-27001: The Agentic OS Paradigm (Skills as Apps)Markdown---
id: 27001
title: "Agentic OS Paradigm: Skills as Composable Applications"
date: 2026-02-25
status: proposed
superseded_by: null
tags: [architecture, skills, abstraction]
---

# ADR-ECO-27001: Agentic OS Paradigm: Skills as Composable Applications

## Date
2026-02-25

## Status
proposed

## Context
As LLM capabilities move toward autonomous agency, our internal mental models for building them must shift from "Chat-based instruction" to "System-based execution." Current agents often suffer from "Role Confusion" and "Cognitive Overload" because they are expected to be both the processor (intelligence) and the database (expertise).

## Decision
We adopt the **Agentic OS Paradigm**, which decouples intelligence from execution through three distinct layers:

1.  **The Processor (The Model):** LLMs are treated as stateless CPUs. We use the taxonomy from {term}`ADR-26027` to select the right "core" (Reasoning-class vs Agentic-class) for the task.
2.  **The Operating System (The Runtime):** A stable, code-based environment (e.g., Mentor Generator's Chassis) that manages the file system, network connections (MCP), and the context window.
3.  **The Applications (The Skills):** Skills are "Atomic Suites" stored as folders containing `SKILL.md` (instructions), `/scripts` (executable logic), and `/assets` (data/templates).

## Consequences

### Positive
- **Transferability:** Skills can be developed in one repo (e.g., `my-org-skills`) and "installed" into any agentic runtime.
- **Continuous Learning:** Agents can update their own "Apps" (scripts) to improve performance over time without changing the OS code.
- **Context Protection:** High-level metadata is disclosed first; implementation details are loaded only when a skill is "activated."

### Negative / Risks
- **Runtime Dependency:** A skill written in Python requires the OS to have a Python interpreter. **Mitigation**: Skills must declare environment requirements in their metadata.

## References
- [Claude Skills Transcript: Code is all we need]
- {term}`ADR-26027`: Model Taxonomy
ADR ECO-27002: Tiered Cognitive Memory (Skills + RAG)Markdown---
id: 27002
title: "Tiered Cognitive Memory: Procedural Skills vs. Declarative RAG"
date: 2026-02-25
status: proposed
superseded_by: null
tags: [architecture, context_management, rag]
---

# ADR-ECO-27002: Tiered Cognitive Memory: Procedural Skills vs. Declarative RAG

## Date
2026-02-25

## Status
proposed

## Context
Feeding an agent too much information leads to "Context Tax" and "Hallucination Drift." We need a deterministic way to decide which information belongs in the LLM's direct instructions (Skills) and which belongs in an external searchable library (RAG).

## Decision
We implement a **Tiered Memory Architecture** that distinguishes between *how* to do something and *what* facts exist.

1.  **Level 1: Procedural Skills (The "How"):** - Information: Logic, validation rules, workflow steps, and executable scripts.
    - Format: Skill folders with YAML frontmatter tags.
    - Delivery: Filtered by tags at startup and progressively disclosed into the context window.
2.  **Level 2: Declarative RAG (The "What"):** - Information: Documentation, specs, research papers, and historical logs.
    - Format: Vector store or semantic index.
    - Delivery: Just-In-Time (JIT) retrieval via tool-calling only when relevant.

## Consequences

### Positive
- **Determinism:** Core logic (Skills) is never "searched" for; it is always present or explicitly loaded, reducing execution errors.
- **Scalability:** The RAG library can grow to millions of documents without affecting the agent's base performance.

### Negative / Risks
- **Complexity:** The agent must manage two different retrieval paths. **Mitigation**: Use standardized tool-naming (e.g., `use_skill` vs `search_docs`).
ADR ECO-27003: Package-Driven "Virtual Monorepo" InfrastructureMarkdown---
id: 27003
title: "Virtual Monorepo via Package-Driven Dependency Management"
date: 2026-02-25
status: proposed
superseded_by: null
tags: [infrastructure, devops, uv]
---

# ADR-ECO-27003: Virtual Monorepo via Package-Driven Dependency Management

## Date
2026-02-25

## Status
proposed

## Context
We need to combine the benefits of a **Monorepo** (easy cross-project testing, unified environment) with the benefits of **Polyrepos** (independent release cycles, clear ownership). Per {term}`ADR-26020`, we use a Hub-and-Spoke model, but the local development friction remains high.

## Decision
We standardize on **`uv` Workspace-linked Packages** to create a "Virtual Monorepo." 

1.  **Architecture:** The `skills`, `vadocs`, and `mentor_generator` remain independent Git repositories.
2.  **Local Dev (Dogfooding):** In a central "Development" folder, we use a `pyproject.toml` with `tool.uv.workspace`. This links the local folders so changes in `vadocs` are immediately visible to `mentor_generator`.
3.  **Production Distribution:** Every repo is built as a standard Python package.
4.  **The Handshake:** All cross-repo expertise is delivered via PyPI-style installs. An agent "activates" its skills by looking into its own `site-packages` or a mounted directory.

## Consequences

### Positive
- **No Manual Setup:** A single `uv sync` prepares the entire ecosystem for a new developer.
- **Versioned Trust:** We can pin specific versions of `my-org-skills` to ensure an agent's behavior doesn't change unexpectedly.
- **Dogfooding:** Validation rules developed in `vadocs` can be run against `skills` and `agent` logic in a single unified test suite.

### Negative / Risks
- **Workspace Clutter:** Managing many linked repos can be confusing. **Mitigation**: Use a standard `dev-chassis` repo that orchestrates the `git clone` and `uv sync` process.

## References
- {term}`ADR-26020`: Hub-and-Spoke Ecosystem
- {term}`ADR-26001`: Dogfooding for Self-Documentation
Would you like me to create the corresponding vadocs rules to enforce the ECO-, MNT-, and SKL- prefixes in your ADR index automatically?adr_26025_...malizationMD You said  write the ADR on the new prefixes policy. consider refining the ADRs you have written to align to the 26025 policy To address the "Ecosystem Documentation Debt" and stop the numbering collisions between repositories, we will first formalize the Prefixed Namespace System. Following this, the three core architectural decisions discussed (Agentic OS, Hybrid Memory, and Virtual Monorepo) are refined into "Fat ADRs" to comply with the promotion gates and analytical requirements established in ADR-26025.ADR ECO-27000: Prefixed Namespace System for Architectural RecordsMarkdown---
id: ECO-27000
title: "Prefixed Namespace System for Architectural Records"
date: 2026-02-25
status: accepted
superseded_by: null
tags: [governance, documentation, workflow]
---

# ADR-ECO-27000: Prefixed Namespace System for Architectural Records

## Date
2026-02-25

## Status
accepted

## Context
As the ecosystem expands into a hub-and-spoke model, maintaining a single global incrementing ID for ADRs (e.g., ADR-26001) has become impossible. Multiple repositories are generating overlapping IDs, leading to "documentation debt" where it is unclear which decision governs which repository.

## Decision
We adopt a **Prefixed Namespace System** for all architectural records to ensure unique identification across the ecosystem.

1. **Namespace Registry:**
   - `ECO-`: Ecosystem/Hub standards (High-level architecture).
   - `MNT-`: Mentor Generator specific implementation.
   - `SKL-`: Skills Library definitions and schemas.
   - `VAD-`: vadocs validation rules and engine logic.

2. **Formatting:** IDs must follow the pattern `[PREFIX]-[YY][NNN]` (e.g., `ECO-27001`).
3. **Canonical Source:** The Hub repository (`adr_index.md`) remains the registry for `ECO-` records, while spoke repositories maintain their own local indices using their respective prefixes.

## Consequences

### Positive
- **Collision Avoidance:** Developers can number ADRs within their repo without checking other spoke repos.
- **Contextual Clarity:** The ID itself signals the scope of the decision.

### Negative / Risks
- **Index Fragmentation:** Finding an ADR requires knowing its prefix. **Mitigation:** The Hub `adr_index.md` will serve as a "Super-Index" linking to spoke indices.

## Alternatives
- **Universal Global Index:** A single file in the Hub that must be updated for every ADR in every spoke. **Rejection Reason:** High friction; blocks spoke development on Hub PR merges.
- **Random UUIDs:** Using strings like `ADR-8f3a1...`. **Rejection Reason:** Violates human-readability and MyST term reference simplicity.

## References
- {term}`ADR-26020`: Hub-and-Spoke Ecosystem Documentation Architecture
- {term}`ADR-26025`: RFC→ADR Workflow Formalization

## Participants
1. Vadim Rudakov
2. Gemini (Principal Systems Architect)
ADR ECO-27001: Agentic OS Paradigm (Skills as Apps)Markdown---
id: ECO-27001
title: "Agentic OS Paradigm: Skills as Composable Applications"
date: 2026-02-25
status: proposed
superseded_by: null
tags: [architecture, skills, abstraction]
---

# ADR-ECO-27001: Agentic OS Paradigm: Skills as Composable Applications

## Date
2026-02-25

## Status
proposed

## Context
We need a model for agency that moves beyond simple prompts. As discussed with the inventors of Claude Skills, "Code is the universal interface". We seek a way to make expertise portable, versioned, and executable across different agents.

## Decision
Adopt the **Agentic OS Paradigm**:
1. **The Processor:** The LLM (Reasoning or Agentic class).
2. **The OS:** The Agent Runtime (file system, Python interpreter, MCP).
3. **The Skills:** Atomic folders containing `SKILL.md` and `/scripts`.

## Consequences

### Positive
- **Transferability:** Skills can be installed via `uv` packages across the org.
- **Validation:** `vadocs` can dogfood these folders to ensure compliance.

## Alternatives
- **Instruction-Only Agents (The Status Quo):** Relying on massive system prompts to explain domain logic. **Rejection Reason:** Non-deterministic. As logic becomes complex (e.g., tax law), prompts fail where code succeeds. Leads to "Cognitive Overload" noted in v0.31.0 release notes.
- **Tool-Calling Without Procedural Code:** Using only external APIs (MCP) for all logic. **Rejection Reason:** Requires an active server for every small task. Local scripts in a "Skill" folder are more resilient and faster for deterministic data transformation.

## References
- {term}`ADR-26027`: Model Taxonomy
- Claude Skills Inventor Transcript

## Participants
1. Vadim Rudakov
2. Claude (Implementation Partner)
ADR ECO-27002: Tiered Cognitive Memory (Skills + RAG)Markdown---
id: ECO-27002
title: "Tiered Cognitive Memory: Procedural Skills vs. Declarative RAG"
date: 2026-02-25
status: proposed
superseded_by: null
tags: [architecture, context_management, rag]
---

# ADR-ECO-27002: Tiered Cognitive Memory: Procedural Skills vs. Declarative RAG

## Context
To achieve "Predictability from constraints", we must solve the "Context Tax" problem where the agent is overloaded with reference data that obscures its core instructions.

## Decision
Implement a **Tiered Memory Architecture**:
- **Skills:** High-density procedural logic (Level 1/2 Disclosure).
- **RAG:** Low-density declarative knowledge (JIT Retrieval).

## Alternatives
- **Pure RAG for Everything:** Store logic/instructions in the vector store. **Rejection Reason:** Vector search is probabilistic; the agent might retrieve the wrong "how-to" step, leading to catastrophic failure in structured tasks like YAML generation.
- **Pure Skills (No RAG):** Try to fit all domain knowledge into the context window. **Rejection Reason:** Violates the 90k-token "performance cliff" for many models. Historical logs and large specs must remain external to keep the "OS" responsive.

## References
- {term}`ADR-26030`: Stateless JIT Context Injection
- {term}`ADR-26013`: Just-in-Time Prompt Transformation

## Participants
1. Vadim Rudakov
2. Gemini (Principal Systems Architect)
ADR ECO-27003: Virtual Monorepo via Package-Driven InfrastructureMarkdown---
id: ECO-27003
title: "Virtual Monorepo via Package-Driven Dependency Management"
date: 2026-02-25
status: proposed
superseded_by: null
tags: [infrastructure, devops, uv]
---

# ADR-ECO-27003: Virtual Monorepo via Package-Driven Dependency Management

## Context
Managing `vadocs`, `skills`, and `mentor_generator` as separate repos creates friction for developers who need to "dogfood" changes across the stack.

## Decision
Standardize on **`uv` Workspace-linked Packages** to create a "Virtual Monorepo" during development. Use internal PyPI/Git-based installs for production.

## Alternatives
- **Git Submodules:** Linking repos via `.gitmodules`. **Rejection Reason:** High "Maintenance Tax." Submodules are notoriously difficult to keep in sync and often lead to "Detached HEAD" states for developers.
- **Traditional Monorepo:** Merging all code into one giant repository. **Rejection Reason:** Violates the Hub-and-Spoke model. Prevents independent versioning of the `vadocs` engine and the `skills` library, which have different release cycles.

## References
- {term}`ADR-26020`: Hub-and-Spoke Ecosystem
- {term}`ADR-26001`: Dogfooding for Self-Documentation

## Participants
1. Vadim Rudakov
2. Claude (DevOps Consultant)
 You said  Let's redefine a context of Agentic OS context. You say:  We need a model for agency that moves beyond simple prompts. As [discussed with the inventors of Claude Skills](https://www.youtube.com/watch?v=CEvIs9y1uog), "Code is the universal interface". We seek a way to make expertise portable, versioned, and executable across different agents.  But this is very narrow and does not reflect our previous discussions. Start with this:   In the search for a viable, composable, and production level architecture for a spoke repo "Mentor Generator" ... continue In the search for a viable, composable, and production-level architecture for a spoke repo "Mentor Generator," we must recognize that the paradigm has shifted from monolithic prompting to a layered "Agentic OS" strategy.Our previous discussions established that a production-grade agent is not just an LLM with a long system message; it is a system of portable, versioned, and executable expertise. In this context, the "Mentor Generator" serves as the factory that assembles these layers into a cohesive persona.1. The 3-Tier Architecture of ExpertiseInstead of a single JSON blob, we now treat the agent's brain as a stack of three distinct, decoupled layers. This separation prevents the "role confusion" and "state drift" identified in our v0.31.0 overhaul.Skills (The "How"): Procedural, versioned knowledge. These are the "apps" of the Agentic OS. For a mentor, this includes pedagogical strategies, Socratic questioning protocols, and feedback loops. By moving these to a code-accessible skills/ directory, we ensure they are executable and testable independently of the mentor's specific subject matter.Projects/Context (The "What"): Grounding data. This includes the specific curriculum, student records, and knowledge base. This is the state that the OS manages, kept distinct from the logic to avoid cognitive overload.Orchestration (The "Universal Interface"): The "Code is the universal interface" principle means the mentor interacts with its environment—and its own memory—through typed, versioned tool calls (MCP, filesystem scripts, etc.) rather than loose narrative instructions.2. From Instructions to ConstraintsOur core insight from ADR-26023 and ADR-26030 is that predictability comes from constraints, not instructions.The Template as Jail: In "Mentor Generator," we moved from telling the LLM to "be a good teacher" to forcing it to fill a strictly validated 30-line YAML template. The template acts as the hardware spec of the Agentic OS.Stateless JIT Injection: To solve "Computational Debt," the architecture now treats the mentor as a Stateless Observer. We inject only the necessary context for the current turn (Just-In-Time), ensuring the model doesn't hallucinate based on stale, historical "narrative" turns.3. The "Hub-and-Spoke" EcosystemAs established in ADR-26020, "Mentor Generator" is a spoke—a specialized implementation of high-level ecosystem standards.The Hub: Defines the standards (e.g., how an ADR should be formatted, how a Skill folder is structured).The Spoke (Mentor): Implements these standards for a specific vertical (Education). This allows us to "dogfood" our validation engine (vadocs) to ensure that the mentor's own documentation and skills remain compliant with the broader architecture.4. Portability via "Code as Interface"The goal of making expertise "portable" means a Mentor Skill created in this repository should be executable by any agentic system (Claude Code, a local Ollama instance, or a production RAG pipeline) because it adheres to a standard folder structure:skill.md: The procedural logic.tools/: The executable scripts (Python/Bash).tests/: Golden-file fixtures that verify the skill's logic with zero API calls.This architecture ensures that "Mentor Generator" is not just a chatbot, but a system for generating verifiable, domain-specific intelligence that can be deployed across the evolving Agentic OS landscape. Google Privacy Policy  Opens in a new window  Google Terms of Service  Opens in a new window  Your privacy & Gemini Apps  Opens in a new window Gemini may display inaccurate info, including about people, so double-check its responses. Continue this chat Google AccountVadim Rudakovlefthand67@gmail.comCopy public linkReport