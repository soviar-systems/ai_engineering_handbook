üöÄ GEMM: –ò–Ω–∂–µ–Ω–µ—Ä–Ω—ã–π —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç –ò–ò –∏ High-Performance Computing

–í—ã—à–ª–∞ —Å—Ç–∞—Ç—å—è-–æ–±–∑–æ—Ä https://book.soviar.ru/algebra-gemm-engineering-standard/, –≥–¥–µ –∞–ª–≥–µ–±—Ä–∞–∏—á–µ—Å–∫–∞—è –æ–ø–µ—Ä–∞—Ü–∏—è **GEMM** (General Matrix-Matrix Multiplication) —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –Ω–µ –ø—Ä–æ—Å—Ç–æ –∫–∞–∫ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Ñ–æ—Ä–º—É–ª–∞, –∞ –∫–∞–∫ –∫–ª—é—á–µ–≤–æ–π –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç, –Ω–∞ –∫–æ—Ç–æ—Ä–æ–º –¥–µ—Ä–∂–∏—Ç—Å—è –≤–µ—Å—å —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π Deep Learning.

üå≥ **–ì–ª–∞–≤–Ω—ã–µ –∏–Ω—Å–∞–π—Ç—ã —Å—Ç–∞—Ç—å–∏:**

‚úÖ **–ü–æ—á–µ–º—É GEMM ‚Äî —ç—Ç–æ –Ω–µ GEMV?**

–°—Ç–∞—Ç—å—è –æ–±—ä—è—Å–Ω—è–µ—Ç ¬´–∏–µ—Ä–∞—Ä—Ö–∏—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏¬ª –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞ BLAS (–±–∞–∑–æ–≤—ã–µ —Å—É–±–ø—Ä–æ–≥—Ä–∞–º–º—ã –ª–∏–Ω–µ–π–Ω–æ–π –∞–ª–≥–µ–±—Ä—ã). 

–í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —É–º–Ω–æ–∂–µ–Ω–∏—è –º–∞—Ç—Ä–∏—Ü—ã –Ω–∞ –≤–µ–∫—Ç–æ—Ä (GEMV), –∫–æ—Ç–æ—Ä–æ–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–æ –ø—Ä–æ–ø—É—Å–∫–Ω–æ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å—é –ø–∞–º—è—Ç–∏, GEMM –æ–±–ª–∞–¥–∞–µ—Ç –≤—ã—Å–æ–∫–æ–π **–∞—Ä–∏—Ñ–º–µ—Ç–∏—á–µ—Å–∫–æ–π –∏–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ—Å—Ç—å—é**. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç GPU A100/H100 –¥–æ—Å—Ç–∏–≥–∞—Ç—å >90% –ø–∏–∫–æ–≤–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, –≤—ã–ø–æ–ª–Ω—è—è –∫–æ–ª–æ—Å—Å–∞–ª—å–Ω—ã–π –æ–±—ä–µ–º —Ä–∞–±–æ—Ç—ã –Ω–∞ –∫–∞–∂–¥—ã–π –ø–µ—Ä–µ–º–µ—â–µ–Ω–Ω—ã–π –±–∞–π—Ç –¥–∞–Ω–Ω—ã—Ö.

‚úÖ **–ê–Ω–∞—Ç–æ–º–∏—è GPU: –¢–∞–π–ª–∏–Ω–≥ –∏ Tensor Cores**

–†–∞–∑–±–∏—Ä–∞–µ–º, –∫–∞–∫ –æ–≥—Ä–æ–º–Ω—ã–µ –º–∞—Ç—Ä–∏—Ü—ã –Ω–∞—Ä–µ–∑–∞—é—Ç—Å—è –Ω–∞ ¬´—Ç–∞–π–ª—ã¬ª (–±–ª–æ–∫–∏), —á—Ç–æ–±—ã –ø–æ–º–µ—Å—Ç–∏—Ç—å—Å—è –≤ —Å–∫–æ—Ä–æ—Å—Ç–Ω—É—é **SRAM** (Shared Memory). –í—ã —É–∑–Ω–∞–µ—Ç–µ, –∫–∞–∫ **Tensor Cores** –≤—ã–ø–æ–ª–Ω—è—é—Ç –æ–ø–µ—Ä–∞—Ü–∏–∏  –∑–∞ –æ–¥–∏–Ω —Ç–∞–∫—Ç –∏ –∑–∞—á–µ–º –Ω—É–∂–Ω–∞ —Å–º–µ—à–∞–Ω–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å (**BF16/FP32**) –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è LLM.

‚úÖ **Interface vs. API vs. ABI: –£—Ä–æ–∫–∏ –∏—Å—Ç–æ—Ä–∏–∏**

–í–∞–∂–Ω–µ–π—à–∏–π —Ä–∞–∑–¥–µ–ª –æ –ø–µ—Ä–µ–Ω–æ—Å–∏–º–æ—Å—Ç–∏ —Å–æ—Ñ—Ç–∞. –°—Ç–∞—Ç—å—è –ø—Ä–∏–≤–æ–¥–∏—Ç –≤ –ø—Ä–∏–º–µ—Ä **—É—Ä–æ–∫ –°–°–°–† 70-—Ö –≥–æ–¥–æ–≤**: –∫–∞–∫ –∂–µ—Å—Ç–∫–∞—è –ø—Ä–∏–≤—è–∑–∫–∞ –∫–æ–¥–∞ –∫ ¬´–∂–µ–ª–µ–∑—É¬ª –ë–≠–°–ú-6 —Å–æ–∑–¥–∞–ª–∞ –æ–≥—Ä–æ–º–Ω—ã–π —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –¥–æ–ª–≥, –∏ –ø–æ—á–µ–º—É –∑–∞–ø–∞–¥–Ω—ã–π –ø–æ–¥—Ö–æ–¥ —Å –∞–±—Å—Ç—Ä–∞–∫—Ç–Ω—ã–º–∏ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞–º–∏ (BLAS) –ø–æ–∑–≤–æ–ª–∏–ª –∫–æ–¥—É –ø–µ—Ä–µ–∂–∏—Ç—å –¥–µ—Å—è—Ç–∏–ª–µ—Ç–∏—è —Å–º–µ–Ω—ã –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä.

‚úÖ **–ü—Ä–∞–∫—Ç–∏–∫–∞: cuBLAS vs. CUTLASS**

–ß—Ç–æ –≤—ã–±—Ä–∞—Ç—å?

* **cuBLAS**: –≥–æ—Ç–æ–≤—ã–π ¬´—á–µ—Ä–Ω—ã–π —è—â–∏–∫¬ª –¥–ª—è –ø—Ä–æ–¥–∞–∫—à–µ–Ω–∞.
* **CUTLASS**: –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä –Ω–∞ C++ –¥–ª—è —Ç–µ—Ö, –∫–æ–º—É –Ω—É–∂–µ–Ω –ø–æ–ª–Ω—ã–π –∫–æ–Ω—Ç—Ä–æ–ª—å –Ω–∞–¥ —Ç–∞–π–ª–∏–Ω–≥–æ–º –∏ —Å–ª–∏—è–Ω–∏–µ–º —è–¥–µ—Ä (kernel fusion).

‚úÖ **–ì–¥–µ —ç—Ç–æ –≤ –∫–æ–¥–µ?**

–í—Å—ë, —á—Ç–æ –º—ã –¥–µ–ª–∞–µ–º –≤ **PyTorch** (`nn.Linear`, Self-Attention –≤ –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞—Ö), –ø–æ–¥ –∫–∞–ø–æ—Ç–æ–º –ø—Ä–µ–≤—Ä–∞—â–∞–µ—Ç—Å—è –≤ —Å–µ—Ä–∏—é –≤—ã–∑–æ–≤–æ–≤ GEMM. –°—Ç–∞—Ç—å—è –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, –ø–æ—á–µ–º—É 60-80% –≤—Ä–µ–º–µ–Ω–∏ –æ–±—É—á–µ–Ω–∏—è –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π —Ç—Ä–∞—Ç–∏—Ç—Å—è –∏–º–µ–Ω–Ω–æ –∑–¥–µ—Å—å –∏ –∫–∞–∫ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —ç—Ç–∏—Ö –≤—ã–∑–æ–≤–æ–≤ —ç–∫–æ–Ω–æ–º–∏—Ç –º–∏–ª–ª–∏–æ–Ω—ã –¥–æ–ª–ª–∞—Ä–æ–≤ –Ω–∞ VRAM-—Ç—Ä–∞—Ñ–∏–∫–µ.

‚úçÔ∏è **–ò—Ç–æ–≥**

–ü–æ–Ω–∏–º–∞–Ω–∏–µ GEMM ‚Äî —ç—Ç–æ —Ä–∞–∑–Ω–∏—Ü–∞ –º–µ–∂–¥—É ¬´–ø—Ä–æ—Å—Ç–æ –∑–∞–ø—É—Å–∫–æ–º –∫–æ–¥–∞¬ª –∏ —Å–æ–∑–¥–∞–Ω–∏–µ–º —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö –ò–ò-–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ —É–ø–∏—Ä–∞—é—Ç—Å—è –≤ ¬´–±—É—Ç—ã–ª–æ—á–Ω–æ–µ –≥–æ—Ä–ª—ã—à–∫–æ¬ª –ø–∞–º—è—Ç–∏.

#HPC #DeepLearning #CUDA #GPU #GEMM #Architecture #Programming #AI
