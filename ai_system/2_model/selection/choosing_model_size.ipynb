{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00ef6b1b-3a9e-4c1f-9d5b-853c978f16ea",
   "metadata": {},
   "source": [
    "# Choosing Model Size for Chats, Workflows, and Agents: Model Size, Reasoning, and Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc7d0f7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Owner: Vadim Rudakov, lefthand67@gmail.com  \n",
    "Version: 0.2.2  \n",
    "Birth: 2025-10-19  \n",
    "Last Modified: 2026-01-10\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7286e7bc",
   "metadata": {},
   "source": [
    "The size of the Large Language Model (LLM) affects its performance, resource requirements, and suitability for specific applications. Larger models offer deeper understanding, better reasoning, and more nuanced outputs—at the cost of increased inference compute, memory, and latency.\n",
    "\n",
    "\n",
    "See also: \n",
    "- [LLM Usage Patterns: Chats, Workflows, and Agents in AI](/ai_system/4_orchestration/patterns/llm_usage_patterns.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2610895",
   "metadata": {},
   "source": [
    "## 1. The Decision Matrix: Tiers & Use Cases\n",
    "\n",
    "| Pattern | Best Model Size | Why? |\n",
    "| --- | --- | --- |\n",
    "| **Chats** | **3B – 8B** | **Low Latency.** Users want responses in <200ms. Small models are \"snappy.\" |\n",
    "| **Workflows** | **8B – 14B** | **Consistency.** Good at following \"if/then\" steps without needing massive compute. |\n",
    "| **Agents** | **14B – 70B+** | **Planning.** High-parameter counts are needed for tool selection and self-correction. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab86ede4",
   "metadata": {},
   "source": [
    "## 2. The \"Short-Term Memory\" Tax (KV Cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab89978",
   "metadata": {},
   "source": [
    "When you choose a model size, you aren't just buying space for the \"brain\" (Weights). You are also buying space for its \"short-term memory\" (**KV Cache**).\n",
    "\n",
    "* **The Problem:** If a model fits on your GPU but the conversation gets long, the model will run out of memory and crash (**OOM - Out of Memory**).\n",
    "* **The Rule of Thumb:** For an 8B model, budget **~1.5GB of VRAM** for every 8,000 words (tokens) of conversation history."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde32eda",
   "metadata": {},
   "source": [
    "## 3. Schema-Guided Reasoning (SGR): The \"Safety Harness\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4db6e7",
   "metadata": {},
   "source": [
    "SGR is the technique of forcing a model to output a specific format (like JSON). This allows a **Small model** to behave as reliably as a **Large model**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97f1da6",
   "metadata": {},
   "source": [
    "### **The \"Scratchpad\" Requirement**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504ead7d",
   "metadata": {},
   "source": [
    "To make SGR work, your JSON schema **must** start with a \"thought\" field.\n",
    "\n",
    "> **Example:** `{\"thought\": \"User wants a summary, I will identify key points...\", \"summary\": \"...\"}`\n",
    "\n",
    "* **Why?** It forces the model to think before it acts. Without this, small models often \"hallucinate\" or skip logic to finish the task faster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14465af1",
   "metadata": {},
   "source": [
    "## 4. Advanced Production Patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73288656",
   "metadata": {},
   "source": [
    "### **Pattern A: The Verifier Cascade**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13ea245",
   "metadata": {},
   "source": [
    "Instead of one slow, expensive 70B model, use two fast ones:\n",
    "\n",
    "1. **The Drafter (3B):** Writes a quick, \"rough\" answer.\n",
    "2. **The Verifier (8B):** Checks the answer against your rules/schema.\n",
    "\n",
    "* **Benefit:** You get 70B quality at 3B speed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4439919",
   "metadata": {},
   "source": [
    "### **Pattern B: Hybrid Routing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac5c871",
   "metadata": {},
   "source": [
    "Use a tiny model (1B) as a \"gatekeeper\" to sort incoming user requests.\n",
    "\n",
    "* **Simple requests** (e.g., \"Hello\") go to your **Small local model**.\n",
    "* **Complex requests** (e.g., \"Write a legal brief\") are routed to a **Large Cloud API**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f11dee",
   "metadata": {},
   "source": [
    "## 5. Implementation Summary (Actionable Checklist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836c40d0-3f96-4adc-ad59-32c90f9ce854",
   "metadata": {},
   "source": [
    "1. **Select Tier:** 8B for most tasks, 14B+ for Agents.\n",
    "2. **Quantize:** Use **4-bit (Q4_K_M)** for most deployments. It reduces size by 50% with only a ~1% loss in logic.\n",
    "3. **Calculate VRAM:** `Model Size + 2GB (Headroom)`. If you have 12GB of VRAM, stick to 8B models.\n",
    "4. **Enforce SGR:** Use libraries like *Pydantic* or *Outlines* to ensure the model doesn't output broken text.\n",
    "5. **Audit:** Test with the longest possible user input to ensure the \"KV Cache Tax\" doesn't crash your server."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
