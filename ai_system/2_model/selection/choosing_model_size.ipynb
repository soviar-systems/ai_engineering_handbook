{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00ef6b1b-3a9e-4c1f-9d5b-853c978f16ea",
   "metadata": {},
   "source": [
    "# Model Sizing, Quantization, and VRAM Budgeting for Local Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc7d0f7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Owner: Vadim Rudakov, rudakow.wadim@gmail.com\n",
    "Version: 1.0.0\n",
    "Birth: 2025-10-19\n",
    "Last Modified: 2026-02-05\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7286e7bc",
   "metadata": {},
   "source": [
    "Deploying language models locally — whether as the **Editor** in the [`aidx` pipeline](/ai_system/4_orchestration/workflows/aidx_industrial_ai_orchestration_framework.ipynb) or as a standalone inference endpoint — requires engineering three interrelated budgets: **model weights**, **KV cache memory**, and **quantization loss**. This article provides the sizing rationale that complements the [model classification](/ai_system/2_model/selection/general_purpose_vs_agentic_models.ipynb) (Agentic / General Purpose / Thinking tiers) with concrete VRAM arithmetic.\n",
    "\n",
    ":::{seealso}\n",
    "> 1. {term}`ADR-26005`: Formalization of Aider as the Primary Agentic Orchestrator\n",
    "> 2. {term}`ADR-26006`: Requirement for Agentic-Class Models for the Architect Phase\n",
    "> 3. {term}`ADR-26021`: Content Lifecycle Policy for RAG-Consumed Repositories\n",
    "> 4. [Hybrid Execution and KV Cache Offloading](/ai_system/1_execution/hybrid_execution_and_kv_cache_offloading.ipynb) — deep dive on Host/Device memory architecture\n",
    ":::"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c1173300-bcf9-45d8-9e05-2c331899a31f",
   "metadata": {},
   "source": [
    "## 1. Model Size Tiers and the `aidx` Role Map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4e47a3-ac1b-4748-9a81-ed6bc49f1a19",
   "metadata": {},
   "source": [
    "The `aidx` framework assigns models to specific pipeline phases based on their capability tier. Size alone does not determine role — **instruction adherence** and **reasoning depth** matter more (see [General Purpose vs Agentic Models](/ai_system/2_model/selection/general_purpose_vs_agentic_models.ipynb)).\n",
    "\n",
    "| Tier | Parameter Range | `aidx` Role | Representative Models | Hardware Target |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| **Micro** | 125M – 3B | Researcher (RAG retrieval), classifier, router | `ministral`, `phi-3-mini` | CPU / mobile / edge |\n",
    "| **Editor** | 7B – 14B | **Editor** (Phase 3: Execution) | `qwen2.5-coder:14b-instruct-q4_K_M` | Consumer GPU (8–16 GB VRAM) |\n",
    "| **Architect** | 70B+ / Cloud API | **Architect** (Phase 2: Planning) | **Claude 4.0 Sonnet**, **Gemini 3 Flash**, **DeepSeek-V3** | Cloud API |\n",
    "| **Thinking** | Cloud API | Pre-flight verification | **OpenAI o2**, **Gemini 3 (DeepThink)**, **DeepSeek-R1** | Cloud API |\n",
    "\n",
    "> **Key insight:** The local GPU budget is reserved for the **Editor** tier. Architect and Thinking models run via cloud API, so their parameter count is irrelevant to your VRAM planning. Plan your hardware around the Editor model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab86ede4",
   "metadata": {},
   "source": [
    "## 2. The \"Short-Term Memory\" Tax (KV Cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab89978",
   "metadata": {},
   "source": [
    "When you load a model, you are not just paying for the **weights** (the \"brain\"). Every token of conversation history allocates **KV cache** (the \"short-term memory\") on the GPU.\n",
    "\n",
    "* **The Problem:** A model that fits in VRAM at initialization can **OOM** (Out of Memory) mid-conversation as the KV cache grows with each turn.\n",
    "* **The Rule of Thumb:** For a 14B model at Q4 quantization, budget **~2 GB of VRAM per 8,192 tokens** of active context.\n",
    "\n",
    "This is why the `aidx` framework enforces a **Hard Reset** at the Architect→Editor transition ({term}`ADR-26005`):\n",
    "\n",
    "> The Editor instance is launched without the Architect's message history. It receives only `artifacts/plan.md` as input, keeping KV cache usage below 4 GB and leaving maximum headroom for model weights.\n",
    "\n",
    "The `max-chat-history-tokens: 2048` setting in the `aidx` configuration is a **Context Gate** — it caps the Editor's KV cache growth to prevent the OOM crash that long aider sessions would otherwise produce.\n",
    "\n",
    ":::{seealso}\n",
    "> [Hybrid Execution and KV Cache Offloading](/ai_system/1_execution/hybrid_execution_and_kv_cache_offloading.ipynb) — for detailed Host/Device memory split, KV cache offloading strategies, and Mermaid diagrams of the memory lifecycle.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde32eda",
   "metadata": {},
   "source": [
    "## 3. Quantization: Trading Precision for Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4db6e7",
   "metadata": {},
   "source": [
    "Quantization reduces model weights from 16-bit floats to lower-bit integers, shrinking VRAM requirements at the cost of minor accuracy loss.\n",
    "\n",
    "| Format | Size Reduction | Accuracy Impact | When to Use |\n",
    "| --- | --- | --- | --- |\n",
    "| **FP16** (baseline) | — | — | Benchmarking, maximum quality |\n",
    "| **Q8_0** | ~50% | Negligible | When VRAM is available but you want a safety margin |\n",
    "| **Q4_K_M** | ~75% | ~1% logic degradation | **Default for local Editor deployment.** Best balance of size and quality. |\n",
    "| **Q4_0** | ~75% | ~2–3% degradation | Budget hardware; test thoroughly before production use |\n",
    "\n",
    "**Practical example:** `qwen2.5-coder:14b` at FP16 requires ~28 GB VRAM. At Q4_K_M, it fits in ~8 GB, leaving headroom for KV cache on a 12 GB consumer GPU.\n",
    "\n",
    ":::{important}\n",
    "The `aidx` Editor configuration uses Q4_K_M explicitly: `ollama_chat/qwen2.5-coder:14b-instruct-q4_K_M`. This is not arbitrary — it's the quantization level validated for code editing tasks with acceptable logic retention.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97f1da6",
   "metadata": {},
   "source": [
    "## 4. Production Patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504ead7d",
   "metadata": {},
   "source": [
    "### Pattern A: The Verifier Cascade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14465af1",
   "metadata": {},
   "source": [
    "Instead of routing everything to a large cloud model, use a two-stage local pipeline:\n",
    "\n",
    "1. **The Drafter (7B):** Produces a fast, rough answer.\n",
    "2. **The Verifier (14B):** Checks the draft against your rules/schema.\n",
    "\n",
    "This maps naturally to the `aidx` Architect→Editor flow: the Architect drafts the plan (cloud), and the Editor executes against the codebase (local). The Verifier Cascade extends this to fully local pipelines where cloud access is unavailable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73288656",
   "metadata": {},
   "source": [
    "### Pattern B: Hybrid Routing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13ea245",
   "metadata": {},
   "source": [
    "Use a micro model (≤3B) as a **gatekeeper** to classify incoming requests:\n",
    "\n",
    "* **Simple requests** (greetings, FAQ lookups) → local Editor model.\n",
    "* **Complex requests** (multi-file refactors, architectural decisions) → cloud Architect API.\n",
    "\n",
    "In the `aidx` context, this is the **Researcher** role (Phase 1): `ministral` performs lightweight RAG retrieval to determine what context the Architect needs, avoiding expensive cloud API calls for work that can be handled locally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4439919",
   "metadata": {},
   "source": [
    "## 5. VRAM Budget Checklist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac5c871",
   "metadata": {},
   "source": [
    "For a target local deployment (e.g., 12 GB consumer GPU):\n",
    "\n",
    "1. **Select the Editor model:** `qwen2.5-coder:14b` or equivalent in the 7B–14B range.\n",
    "2. **Quantize to Q4_K_M:** Reduces ~28 GB → ~8 GB for a 14B model.\n",
    "3. **Reserve KV cache headroom:** 2–4 GB depending on `max-chat-history-tokens` setting.\n",
    "4. **Verify total:** Model weights + KV cache + OS overhead (~500 MB) must fit within VRAM.\n",
    "5. **Enforce structured output:** If the Editor produces structured output, enforce schema compliance with Pydantic or Outlines to prevent broken responses.\n",
    "6. **Stress test:** Run the longest expected input through the pipeline to verify no OOM under peak KV cache load.\n",
    "\n",
    "| Component | Budget (12 GB GPU) |\n",
    "| --- | --- |\n",
    "| Model weights (14B Q4_K_M) | ~8 GB |\n",
    "| KV cache (2048 tokens gate) | ~1 GB |\n",
    "| OS / framework overhead | ~0.5 GB |\n",
    "| **Available headroom** | **~2.5 GB** |"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "md:myst,ipynb"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
