{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "148fc498",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Multi-Phase AI Pipeline: Decoupling Research from Code Generation\"\n",
    "author: Vadim Rudakov, rudakow.wadim@gmail.com\n",
    "date: 2026-02-16\n",
    "options:\n",
    "  version: 1.0.0\n",
    "  birth: 2026-01-14\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c773d2cf",
   "metadata": {},
   "source": [
    "This article documents a tool-agnostic **Research-Apply pipeline** for AI-assisted engineering. The core principle: decouple \"finding truth\" (knowledge retrieval) from \"writing code\" (code generation) to ensure local hardware stability and architectural alignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2d2ab2",
   "metadata": {},
   "source": [
    "## **1. The Research-Apply Pipeline**\n",
    "\n",
    "> ISO 29148: Appropriateness\n",
    "\n",
    "The pipeline moves away from **passive** AI chat toward an **Active (Agentic) Retrieval** model. We decouple \"finding truth\" from \"writing code\" so that local models remain focused, accurate, and stable.\n",
    "\n",
    "| Phase | Role | Action |\n",
    "| --- | --- | --- |\n",
    "| **1. Research** | **Researcher** | A lightweight local agent identifies relevant context from a large Knowledge Base via vector DB or retrieval mechanism. |\n",
    "| **2. Planning** | **Architect** | A high-reasoning model (cloud or large local) processes the research results to generate a precise execution plan. |\n",
    "| **3. Execution** | **Editor** | A local SLM applies the plan to the codebase in a clean context state, preventing GPU OOM. |\n",
    "| **4. Validation** | **CI/CD Gates** | Automated hooks verify code integrity, architectural tags, and commit standards. |\n",
    "| **5. Review** | **Forensic** | Human-led verification of changes. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a05c141",
   "metadata": {},
   "source": [
    "### Why Five Phases?\n",
    "\n",
    "Each phase exists for a specific reason:\n",
    "\n",
    "- **Research** grounds the subsequent reasoning in actual project knowledge, not model hallucination\n",
    "- **Planning** leverages high-capability models for structural decisions (architecture, design)\n",
    "- **Execution** uses efficient local models for the mechanical act of code modification\n",
    "- **Validation** provides automated safety nets that catch regressions immediately\n",
    "- **Review** ensures human oversight on all changes\n",
    "\n",
    "The key insight is the **hard boundary between Planning and Execution**: the Editor must start with a **clean context**, receiving only the plan and target files from the Architect. This prevents the unbounded [VRAM/context growth](/ai_system/1_execution/hybrid_execution_and_kv_cache_offloading.ipynb) typical of long interactive sessions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb144f9",
   "metadata": {},
   "source": [
    "## **2. Namespace Partitioning for RAG**\n",
    "\n",
    "**The Problem:**\n",
    "Standard RAG faces \"Context Overload\" when a Knowledge Base exceeds the functional context window of local models. Retrieving from a flat, undifferentiated collection introduces noise and reduces precision.\n",
    "\n",
    "**The Solution:**\n",
    "Following the same separation-of-concerns principle as the [Hub-and-Spoke Ecosystem Architecture](/architecture/adr/adr_26020_hub_spoke_ecosystem_documentation.md), split retrieval into **namespaced collections** to maintain high precision:\n",
    "\n",
    "| Namespace | Contents | Use Case |\n",
    "| --- | --- | --- |\n",
    "| `Global_Workflows` | Organization-wide standards, ADRs, engineering policies | Ensuring compliance with cross-project conventions |\n",
    "| `Project_Specific` | Project documentation, API specs, domain knowledge | Grounding in the specific codebase being modified |\n",
    "\n",
    "This partitioning ensures that:\n",
    "\n",
    "- **Precision stays high**: Queries hit the relevant namespace rather than searching all documents\n",
    "- **Context budget is preserved**: Only the most relevant chunks from each namespace are injected into the Architect's prompt\n",
    "- **Traceability is maintained**: Every automated code change can cite the documentation chunk that drove it (e.g., `REF: [Workflow-Standard-04]`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d44f6c",
   "metadata": {},
   "source": [
    "### Stage Injection Pattern\n",
    "\n",
    "Retrieved snippets are injected into the Architect context during the Planning phase, before plan generation begins:\n",
    "\n",
    "1. **Query both namespaces** with the task description\n",
    "2. **Rank and filter** chunks by relevance score\n",
    "3. **Inject** the top-k chunks into the Architect's system prompt or context window\n",
    "4. **Cite** sources in the generated plan for downstream traceability\n",
    "\n",
    "This \"pre-flight\" retrieval ensures the plan is grounded in current organizational standards rather than stale model knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c25275",
   "metadata": {},
   "source": [
    "## **3. Pipeline Visualization**\n",
    "\n",
    "```mermaid\n",
    "---\n",
    "config:\n",
    "  look: handDrawn\n",
    "  theme: redux\n",
    "---\n",
    "graph TD\n",
    "    subgraph \"Phase 1: Research (Local Context)\"\n",
    "        A[Knowledge Base / RAG] -->|Query| B(Researcher: Local Agent)\n",
    "        B -->|Retrieve Chunks| C{Context Window Check}\n",
    "    end\n",
    "\n",
    "    subgraph \"Phase 2: Planning (High-Capability Model)\"\n",
    "        C -->|Grounded Prompt| D[Architect: Reasoning Model]\n",
    "        D -->|Draft Strategy| E[Execution Plan]\n",
    "    end\n",
    "\n",
    "    subgraph \"Phase 3: Execution (Local SLM)\"\n",
    "        E -->|Clean State Input| F[Editor: Local SLM]\n",
    "        F -->|Atomic Write| G[Staged Code Changes]\n",
    "    end\n",
    "\n",
    "    subgraph \"Phase 4: Validation (CI/CD Gates)\"\n",
    "        G -->|Pre-commit| H[Automated Hooks]\n",
    "        H -->|Checks| I[Standards Verification]\n",
    "    end\n",
    "\n",
    "    subgraph \"Phase 5: Review (Human)\"\n",
    "        I -->|Success| J[Final Commit/Merge]\n",
    "        I -->|Fail| B\n",
    "    end\n",
    "\n",
    "    style D fill:#f9f,stroke:#333,stroke-width:2px\n",
    "    style F fill:#bbf,stroke:#333,stroke-width:2px\n",
    "    style E stroke-dasharray: 5 5\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c99563",
   "metadata": {},
   "source": [
    "## **4. Implementation Considerations**\n",
    "\n",
    "- **Model selection is independent of the pipeline**: Any combination of cloud/local models can fill the Researcher, Architect, and Editor roles — see [\"General Purpose vs Agentic Models\"](/ai_system/2_model/selection/general_purpose_vs_agentic_models.ipynb) for selection criteria. The pipeline pattern remains the same.\n",
    "- **The hard reset between Planning and Execution is non-negotiable**: The Editor must start with a clean context to maintain VRAM stability on constrained hardware.\n",
    "- **Namespace partitioning scales**: Additional namespaces (e.g., `Security_Policies`, `API_Contracts`) can be added as the Knowledge Base grows.\n",
    "\n",
    ":::{seealso}\n",
    "- [\"Hybrid Execution and KV Cache Offloading\"](/ai_system/1_execution/hybrid_execution_and_kv_cache_offloading.ipynb) — VRAM mechanics and KV cache growth during inference\n",
    "- [\"Choosing Model Size\"](/ai_system/2_model/selection/choosing_model_size.ipynb) — VRAM budgeting and quantization trade-offs\n",
    ":::"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
