{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22570a60-86ff-41c6-8d29-a0d6dec71c33",
   "metadata": {},
   "source": [
    "# Small LM-Backed Release Documentation Pipeline: Architecture\n",
    "\n",
    "-----\n",
    "\n",
    "Owner: Vadim Rudakov, lefthand67@gmail.com  \n",
    "Version: 0.1.2  \n",
    "Birth: 2025-12-01  \n",
    "Last Modified: 2025-12-31\n",
    "\n",
    "-----\n",
    "\n",
    "> INFO: *The handbook is optimized for environments supporting Mermaid.js diagrams. For static export, rasterized versions are available in Appendix A.*\n",
    "\n",
    "## 1. Introduction: Architecture Driven by Resource Constraint\n",
    "\n",
    "This handbook defines the **specific technical architecture** we use to generate official project documentation (Changelogs, Release Notes) leveraging the local, resource-constrained **Small Language Models (SLMs)** (1B to 14B parameters).\n",
    "\n",
    "The key distinction from larger, cloud-hosted LLMs (like GPT-4 or Gemini) is the **resource scarcity** typical of local, commodity AI engineering stacks (e.g., VRAM pools of **12GiB to 48GiB**). This necessitates a highly optimized, modular approach.\n",
    "\n",
    "### The Core Principle: Decoupling $\\to$ Efficiency\n",
    "\n",
    "We implement a **Decoupled Documentation Pipeline**. This is the architectural standard. We explicitly avoid the common anti-pattern of passing massive, raw data (like a complete `git diff/log -p` output) to the SLM in one monolithic prompt. This anti-pattern causes VRAM saturation, OOM errors, and non-deterministic latency.\n",
    "\n",
    "Instead, we shift the heavy lifting (parsing, filtering, aggregation) to lightweight, non-LLM tools and use the SLM *only* for the final, low-context, high-value step of **transformation and summarization**.\n",
    "\n",
    "### Target Audience\n",
    "\n",
    "This document is mandatory for all MLOps Engineers, Architects, and Senior Developers responsible for managing release automation and maintaining the local AI tooling stack (`aider`, `ollama`, HuggingFace).\n",
    "\n",
    "## 2. The Staged Documentation Flow and Dependency\n",
    "\n",
    "Our process is defined by **three distinct, independent stages**. The architecture's robustness is entirely predicated on the strict enforcement of the **Conventional Commit Standard** (defined in the *[Production Git Workflow: Standards](../../../mlops/git_workflows/production_git_workflow_standards.md)* handbook).\n",
    "\n",
    "### 2.1 The Architectural Flow Diagram\n",
    "\n",
    "The following diagram illustrates how the enforced commit standard is processed through the stages to produce the final release artifacts. Note that **Stage 1** is primarily a **Developer** task, but we are aimed to make it **SLM-assisted** stage also - generatin commit title and body using the small `git diff` file (**REMEMBER**: commits should be atomic and self-contained).\n",
    "\n",
    "```mermaid\n",
    "---\n",
    "config:\n",
    "  layout: dagre\n",
    "  theme: default\n",
    "---\n",
    "flowchart TB\n",
    " subgraph MLOps_Doc_Pipeline[\"**MLOps Documentation Pipeline**\"]\n",
    "    direction LR\n",
    "    \n",
    "    B[\"**Stage 1: Commit and Format Enforcement**\n",
    "    - *Input*: Code Diff\n",
    "    - *Actor*: **Developer** (with optional SLM assistance and `gitlint`)\n",
    "    - *Output*: Structured Commit (Conventional Commits, Tier 2/3)\n",
    "    - *Gate*: `pre-commit` hook + `gitlint` (enforces type/scope/subject)\"]\n",
    "    \n",
    "    D[\"**Stage 2: Changelog Aggregation**\n",
    "    - *Input*: Git history of structured commits\n",
    "    - *Processor*: **tool** `git-chglog` or equivalent\n",
    "    - *Output*: `CHANGELOG.md` (standard Markdown, version-scoped entries)\n",
    "    - *Gate*: CI validation + Engineer review\"]\n",
    "    \n",
    "    E[\"**Stage 3: Release Notes Transformation**\n",
    "    - *Input*: Version-scoped section from `CHANGELOG.md` (e.g., v1.2.0 → v1.3.0)\n",
    "    - *Processor*: **SLM** (7B–14B) with **system prompt**:\n",
    "      • Only summarize listed changes\n",
    "      • Never invent features\n",
    "      • Adapt tone per audience (user/internal)\n",
    "    - *Output*: Audience-specific Release Notes\n",
    "    - *Gate*: Architect or Peer Review (factual + compliance check)\"]\n",
    "  end\n",
    "    A[\"Create Topic Branch\"] --> MLOps_Doc_Pipeline\n",
    "    B --> D\n",
    "    D --> E\n",
    "    MLOps_Doc_Pipeline --> F[\"Publish Final Release Notes\"]\n",
    "\n",
    "     A:::ioStage\n",
    "     B:::devStage\n",
    "     D:::toolStage\n",
    "     E:::sllmStage\n",
    "     F:::ioStage\n",
    "    classDef devStage fill:#e0f7fa,stroke:#0097a7,stroke-width:2px\n",
    "    classDef toolStage fill:#ffe0b2,stroke:#ff9800,stroke-width:2px\n",
    "    classDef sllmStage fill:#c8e6c9,stroke:#388e3c,stroke-width:2px\n",
    "    classDef ioStage fill:#f0f9ff,stroke:#0077b6,stroke-width:2px\n",
    "```\n",
    "\n",
    "**Diagram Legend: MLOps Documentation Pipeline**\n",
    "\n",
    "This diagram illustrates the end-to-end workflow for generating auditable, audience-appropriate release documentation in resource-constrained SLM environments. It enforces separation of concerns by assigning each stage to the most appropriate actor:\n",
    "\n",
    "- **Stage 1 (Blue – Developer)**: Developers structure commits using Conventional Commits (Tier 2/3), optionally assisted by an SLMs or tools like `gitlint`. Enforcement occurs via pre-commit hooks.\n",
    "- **Stage 2 (Orange – Deterministic Tooling)**: A standards-compliant, non-LLM tool (`git-chglog`) aggregates commits into a machine-readable `CHANGELOG.md`. Validated by CI and engineers.\n",
    "- **Stage 3 (Green – SLM)**: A capable SLM (7B–14B), guided by engineered prompts, transforms raw changelog entries into tailored release notes (e.g., for users vs. internal teams). Requires architect or peer approval.\n",
    "- **I/O Stages (Light Blue)**: Represent workflow boundaries—branch creation (input trigger) and final publication (output artifact).\n",
    "\n",
    "**Design Principle**: Use the smallest effective component for each task\n",
    "- human judgment for intent, \n",
    "- deterministic tools for structure, and \n",
    "- SLMs only where natural language adaptation is required. \n",
    "\n",
    "> **Avoid LLM overuse and ensures reproducibility.**\n",
    "\n",
    "## 3. SLM Allocation and Resource Strategy\n",
    "\n",
    "The core design choice is assigning the **smallest viable model** to the most frequent tasks. This ensures optimal **latency** and predictable, minimal **VRAM consumption** across  heterogeneous local stack.\n",
    "\n",
    "### 3.1 Stage 1: Commit Message Generation\n",
    "\n",
    "| Parameter | Specification | Rationale |\n",
    "| :--- | :--- | :--- |\n",
    "| **SLM Size** | **1 Billion to 3 Billion** | Task is highly constrained: adherence to a format and summarization of a small, local diff. **Minimal VRAM is required** for near-instant inference, supporting high-frequency use. |\n",
    "| **Input Context** | Low. Single commit diff + the concise prompt template. | **Maximally low token count.** Prevents VRAM pressure during high-frequency development use. |\n",
    "| **Tooling** | `aider` (CLI agent) in `/ask` mode and with `--no-git` and `--read-only` flags to prevent implicit context loading from the codebase. | The tool is used **only for drafting text output**, preventing code mutation and ensuring process idempotency. |\n",
    "\n",
    "### 3.2 Stage 2: Changelog Aggregation (The Non-LLM Gate)\n",
    "\n",
    "This stage is primarily handled by standard, non-LLM command-line tools. This is the **Critical Architectural Choice** that removes the complex, high-cost, and non-deterministic **context-saturation risk** entirely.\n",
    "\n",
    "| Parameter | Specification | Rationale |\n",
    "| :--- | :--- | :--- |\n",
    "| **SLM Size** | **Not Used.** | **Computational Efficiency & Cost Avoidance.** Using an SLM for deterministic parsing is computationally inefficient. Lightweight, non-LLM tools execute this process near-instantly on the CPU, avoiding the VRAM load, latency, and token costs associated with even a short LLM prompt. |\n",
    "| **Tooling** | `git-chglog` (or similar). | These tools use simple regex and parsing logic to reliably aggregate the structured commit history into the standard `CHANGELOG.md` format. |\n",
    "\n",
    "### 3.3 Stage 3: Release Notes Transformation\n",
    "\n",
    "This is the final, high-value task where the largest SLM is justified. The goal is to transform the *developer-focused* Markdown into an *audience-focused* narrative (e.g., executive summary, customer email).\n",
    "\n",
    "| Parameter | Specification | Rationale |\n",
    "| :--- | :--- | :--- |\n",
    "| **SLM Size** | **7 Billion to 14 Billion** | Requires complex linguistic capabilities (tone, style, summarization, abstraction). The higher parameter count is necessary for quality, as the task is **less frequent** (only at release). |\n",
    "| **Input Context** | **Low-to-Medium.** **Only** the relevant, clean section (e.g., the `## vX.Y.Z` block) from `CHANGELOG.md` is provided. | Context is short, clean, and pre-categorized. The prompt asks the SLM to act as a **text transformer**, not a complex parser. |\n",
    "| **Prompt Focus** | **Instructional/Transformative Prompt.** | The prompt explicitly details the target audience (e.g., \"Write this for a non-technical manager\") and the required tone. |\n",
    "\n",
    "## 4. Resource Pitfalls and Technical Debt\n",
    "\n",
    "New engineers must understand the specific constraints that mandate this architecture. Deviating from these constraints *will* result in production failures.\n",
    "\n",
    "### 4.1 Hidden Debt: Context Window Inflation\n",
    "\n",
    "The most significant risk is a developer attempting to increase the SLM's context size by modifying the system prompt or adding unnecessary input data at **Stage 3**.\n",
    "\n",
    "* **Risk:** Even a short, clean changelog list, if followed by an unnecessarily large or verbose system prompt, can push a mid-sized SLM over the edge, causing latency spikes or Out-of-Memory (OOM) errors.\n",
    "* **Mitigation:** System prompts for SLMs **MUST** be highly compact, using JSON or constrained Markdown templates instead of verbose prose. All prompts must be **version-controlled alongside model configs** to prevent drift between prompt behavior and model quantization performance. \n",
    "  \n",
    "> **Every token is a resource cost.**\n",
    "\n",
    "### 4.2 The Anti-Pattern: Monolithic Processing\n",
    "\n",
    "Avoid any attempt to use a single SLM (even the 14B model) for both raw code analysis (Stage 1) and final summarization (Stage 2/3) simultaneously.\n",
    "\n",
    "**Why it Fails**: Combining raw diff analysis and release note generation in one SLM pass forces the model to juggle low-level syntactic noise and high-level semantic summarization. This leads to:\n",
    "- **Loss of composability**: Violates the Unix philosophy—each stage should do one thing well.\n",
    "- **Prompt instability**: Long, heterogeneous prompts (diffs + templates + instructions) cause attention dilution and hallucination.\n",
    "- **Operational fragility**: Context lengths grow unpredictably with PR size; even 14B models with 4-bit quantization fail on >8K tokens under concurrent load.\n",
    "- **Non-reproducibility**: Small diff changes cause large output variance due to attention reallocation.\n",
    "\n",
    "**Empirical evidence**: In SLM pipelines, decoupling deterministic preprocessing (e.g., `git diff --name-only`, AST extraction) from LLM summarization reduces failure rate by 3–5× and improves cache hit ratio in CI.\n",
    "\n",
    "### 4.3 Mandatory Human Audit Gates\n",
    "\n",
    "The \"Manual Review & Edit\" steps are **MANDATORY AUDIT GATES** against SLM errors and parsing failures. They are not optional:\n",
    "\n",
    "1.  **Stage 2 Audit:** Verify the non-LLM tool correctly grouped and categorized **all** relevant commits from the specified range.\n",
    "2.  **Stage 3 Audit:** Verify the SLM-generated release notes accurately reflect the facts in the changelog and use the correct professional tone. **Do not trust the SLM's stylistic choices without review.**\n",
    "\n",
    "### 4.4 Prompt injection via commit messages\n",
    "\n",
    "A malicious or malformed commit body (e.g., containing` }} {{`\") could break JSON-formatted system prompts in Stage 1 or 3, leading to prompt leakage or role confusion.\n",
    "\n",
    "**Mitigation:** All commit message fields used as LLM input must be sanitized: (1) escape double quotes and braces, (2) truncate to max 200 chars per field, (3) validate against allow-list of Conventional Commit types. Never interpolate raw git log output directly into prompts.\n",
    "\n",
    "## 5. Architectural Rationale: Monolithic vs. Decoupled\n",
    "\n",
    "The choice of the **Decoupled Documentation Pipeline** is an architectural imperative driven by the **resource constraints** of the SLM stack. This decision ensures high performance, reliability, and low operational cost, directly contrasting the brittle and resource-heavy \"Monolithic\" anti-pattern.\n",
    "\n",
    "### The Monolithic Anti-Pattern\n",
    "\n",
    "The monolithic approach attempts to use a single large prompt to feed raw, unparsed data (e.g., hundreds of lines of `git diff -p`) directly into the SLM for processing and summarization.\n",
    "\n",
    "### The Decoupled Solution (Our Standard)\n",
    "\n",
    "We use lightweight tools to parse the structured **Conventional Commits** first, and only feed the clean, minimal context (i.e., the changelog entries) to the SLM for the final transformation step.\n",
    "\n",
    "| Architectural Dimension | Monolithic Anti-Pattern (Raw Diff $\\to$ LLM) | Decoupled Solution (Commit $\\to$ Tool $\\to$ LLM) |\n",
    "| :--- | :--- | :--- |\n",
    "| **SLM Role** | **Parser and Summarizer** (High-Cost Data Processing) | **Contextual Transformer** (Low-Cost Stylistic Adaptation) |\n",
    "| **Input Context** | **Extremely High** (Raw diffs: hundreds of lines of code). | **Very Low** (Structured commit summaries: tens of lines of text). |\n",
    "| **VRAM/RAM Load** | **Critical:** High risk of **Out-of-Memory (OOM)** error. $O(N^2)$ complexity on large $N$. | **Minimal:** Predictable, low-peak memory usage. |\n",
    "| **Latency/Speed** | **High and Unpredictable** (Slow token generation). | **Fast and Predictable** (Rapid generation on short context). |\n",
    "| **Failure Point** | **Brittle:** Single failure (OOM or prompt misinterpretation) breaks the entire release notes generation. | **Robust:** Failure in one stage is localized; the LLM stage is highly reliable due to clean input. |\n",
    "| **Operational Cost** | **High** (GPU hours, retries, development complexity) | **Low** (CPU work + brief, predictable GPU burst) |\n",
    "| **Applicability** | Only viable for large, cloud-hosted LLMs with massive memory pools. | **Mandatory** for resource-constrained local SLMs (1B-14B). |\n",
    "\n",
    "## Appendix A. AI-Assisted Documentation Flow\n",
    "\n",
    "![AI-Assisted Documentation Flow](../images/ai_assisted_doc_flow_1.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
